"""
ğŸ”¥ MoE Gateæ¢¯åº¦è¯Šæ–­è„šæœ¬
ç”¨äºæ£€æŸ¥ä¸ºä»€ä¹ˆexpert_entropy_normalizedä¸ä¸‹é™

ä½¿ç”¨æ–¹æ³•ï¼š
python debug_gate_gradient.py

è¯Šæ–­å†…å®¹ï¼š
1. Gateæ¢¯åº¦æ˜¯å¦ä¸º0
2. Aux_lossæ˜¯å¦æ­£ç¡®è®¡ç®—
3. Aux_lossæ˜¯å¦åŠ å…¥æ€»loss
4. Gateæƒé‡æ˜¯å¦æ›´æ–°
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from model.gate.MoEgate import MoEGate, SparseMoeBlock


def test_gate_gradient_flow():
    """æµ‹è¯•gateæ¢¯åº¦æ˜¯å¦æ­£å¸¸æµåŠ¨"""
    print("="*80)
    print("ğŸ” MoE Gateæ¢¯åº¦è¯Šæ–­æµ‹è¯•")
    print("="*80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºMoEæ¨¡å—
    embed_dim = 768
    num_experts = 3
    num_experts_per_tok = 2
    aux_loss_alpha = 1.0
    
    print(f"\nåˆ›å»ºSparseMoeBlock:")
    print(f"  - embed_dim: {embed_dim}")
    print(f"  - num_experts: {num_experts}")
    print(f"  - num_experts_per_tok: {num_experts_per_tok}")
    print(f"  - aux_loss_alpha: {aux_loss_alpha}")
    
    moe_block = SparseMoeBlock(
        embed_dim=embed_dim,
        mlp_ratio=4,
        num_experts=num_experts,
        num_experts_per_tok=num_experts_per_tok,
        n_shared_experts=0,
        aux_loss_alpha=aux_loss_alpha,
        enable_grad_accumulation=False
    ).to(device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(moe_block.parameters(), lr=1e-4)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    batch_size = 16
    seq_len = 12
    hidden_states = torch.randn(batch_size, seq_len, embed_dim, device=device)
    
    print(f"\nè¾“å…¥æ•°æ®å½¢çŠ¶: {hidden_states.shape}")
    
    # å‰å‘ä¼ æ’­
    moe_block.train()
    output = moe_block(hidden_states)
    
    print(f"è¾“å‡ºæ•°æ®å½¢çŠ¶: {output.shape}")
    
    # è·å–MoEç»Ÿè®¡ä¿¡æ¯
    if hasattr(moe_block, 'moe_stats'):
        stats = moe_block.moe_stats
        print(f"\nMoEç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - aux_loss: {stats['aux_loss']:.6f}")
        print(f"  - expert_usage: {stats['expert_usage'].cpu().numpy()}")
        print(f"  - topk_weights_mean: {stats['topk_weights_mean']:.4f}")
        
        # è®¡ç®—ç†µå€¼
        expert_usage = stats['expert_usage']
        entropy = -(expert_usage * torch.log(expert_usage + 1e-10)).sum()
        max_entropy = torch.log(torch.tensor(num_experts, dtype=torch.float32))
        entropy_normalized = entropy / max_entropy
        print(f"  - expert_entropy_normalized: {entropy_normalized:.4f}")
    
    # ğŸ”¥ å…³é”®æµ‹è¯•ï¼šæ£€æŸ¥gateæ¢¯åº¦
    print(f"\n" + "="*80)
    print("ğŸ”¥ å…³é”®æµ‹è¯•ï¼šGateæ¢¯åº¦æµåŠ¨æ£€æŸ¥")
    print("="*80)
    
    # æ£€æŸ¥gateæƒé‡çš„åˆå§‹å€¼
    gate_weight_before = moe_block.gate.weight.clone().detach()
    print(f"\nGateæƒé‡åˆå§‹ç»Ÿè®¡:")
    print(f"  - å‡å€¼: {gate_weight_before.mean():.6f}")
    print(f"  - æ ‡å‡†å·®: {gate_weight_before.std():.6f}")
    print(f"  - èŒƒå›´: [{gate_weight_before.min():.4f}, {gate_weight_before.max():.4f}]")
    
    # è®¡ç®—æŸå¤±ï¼ˆæ¨¡æ‹Ÿå®é™…è®­ç»ƒï¼‰
    target = torch.randn_like(output)
    main_loss = nn.functional.mse_loss(output, target)
    
    # è·å–aux_loss
    if hasattr(moe_block, 'moe_stats'):
        # æ³¨æ„ï¼šaux_losså·²ç»é€šè¿‡AddAuxiliaryLossåŠ å…¥åˆ°outputçš„è®¡ç®—å›¾ä¸­
        # ä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿å®ƒçœŸçš„è¢«åŠ å…¥äº†
        aux_loss = stats['aux_loss']
        print(f"\nLossç»Ÿè®¡:")
        print(f"  - main_loss: {main_loss.item():.6f}")
        print(f"  - aux_loss (detached): {aux_loss:.6f}")
        print(f"  âš ï¸  æ³¨æ„ï¼šaux_lossåº”è¯¥å·²é€šè¿‡AddAuxiliaryLossåŠ å…¥è®¡ç®—å›¾")
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    main_loss.backward()
    
    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒæ£€æŸ¥ï¼šgateæ¢¯åº¦æ˜¯å¦å­˜åœ¨ä¸”éé›¶
    print(f"\n" + "="*80)
    print("ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒæ£€æŸ¥ï¼šGateæ¢¯åº¦")
    print("="*80)
    
    if moe_block.gate.weight.grad is not None:
        gate_grad = moe_block.gate.weight.grad
        gate_grad_norm = gate_grad.norm().item()
        gate_grad_mean = gate_grad.mean().item()
        gate_grad_std = gate_grad.std().item()
        
        print(f"âœ… Gateæ¢¯åº¦å­˜åœ¨!")
        print(f"  - æ¢¯åº¦èŒƒæ•°: {gate_grad_norm:.6f}")
        print(f"  - æ¢¯åº¦å‡å€¼: {gate_grad_mean:.6f}")
        print(f"  - æ¢¯åº¦æ ‡å‡†å·®: {gate_grad_std:.6f}")
        
        if gate_grad_norm < 1e-6:
            print(f"âŒ é”™è¯¯ï¼šGateæ¢¯åº¦å‡ ä¹ä¸º0ï¼è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆä¸“å®¶ä¸åˆ†åŒ–ï¼")
            print(f"  å¯èƒ½åŸå› ï¼š")
            print(f"  1. aux_lossæ²¡æœ‰æ­£ç¡®åŠ å…¥è®¡ç®—å›¾")
            print(f"  2. AddAuxiliaryLosså®ç°æœ‰è¯¯")
            print(f"  3. æ¢¯åº¦è¢«è£å‰ªåˆ°0")
        else:
            print(f"âœ… Gateæ¢¯åº¦æ­£å¸¸æµåŠ¨")
    else:
        print(f"âŒ è‡´å‘½é”™è¯¯ï¼šGateæ¢¯åº¦ä¸ºNoneï¼")
        print(f"  è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸“å®¶ä¸åˆ†åŒ–çš„æ ¹æœ¬åŸå› ï¼")
        print(f"  Gateæƒé‡æ²¡æœ‰requires_gradæˆ–æ²¡æœ‰å‚ä¸è®¡ç®—å›¾")
        return False
    
    # æ›´æ–°å‚æ•°
    optimizer.step()
    
    # æ£€æŸ¥gateæƒé‡æ˜¯å¦æ›´æ–°
    gate_weight_after = moe_block.gate.weight.detach()
    weight_change = (gate_weight_after - gate_weight_before).norm().item()
    
    print(f"\næƒé‡æ›´æ–°æ£€æŸ¥:")
    print(f"  - æƒé‡å˜åŒ–èŒƒæ•°: {weight_change:.6f}")
    
    if weight_change < 1e-6:
        print(f"âŒ é”™è¯¯ï¼šGateæƒé‡å‡ ä¹æ²¡æœ‰æ›´æ–°ï¼")
        return False
    else:
        print(f"âœ… Gateæƒé‡æ­£å¸¸æ›´æ–°")
    
    # å¤šæ¬¡è¿­ä»£æµ‹è¯•
    print(f"\n" + "="*80)
    print("ğŸ“Š å¤šæ¬¡è¿­ä»£æµ‹è¯•ï¼ˆ10æ¬¡ï¼‰")
    print("="*80)
    
    entropy_history = []
    for step in range(10):
        hidden_states = torch.randn(batch_size, seq_len, embed_dim, device=device)
        output = moe_block(hidden_states)
        target = torch.randn_like(output)
        loss = nn.functional.mse_loss(output, target)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if hasattr(moe_block, 'moe_stats'):
            stats = moe_block.moe_stats
            expert_usage = stats['expert_usage']
            entropy = -(expert_usage * torch.log(expert_usage + 1e-10)).sum()
            max_entropy = torch.log(torch.tensor(num_experts, dtype=torch.float32))
            entropy_normalized = (entropy / max_entropy).item()
            entropy_history.append(entropy_normalized)
            
            if step % 2 == 0:
                print(f"Step {step}: entropy={entropy_normalized:.4f}, usage={expert_usage.cpu().numpy()}")
    
    print(f"\nç†µå€¼å˜åŒ–è¶‹åŠ¿:")
    print(f"  åˆå§‹: {entropy_history[0]:.4f}")
    print(f"  æœ€ç»ˆ: {entropy_history[-1]:.4f}")
    print(f"  å˜åŒ–: {entropy_history[-1] - entropy_history[0]:.4f}")
    
    if entropy_history[-1] < entropy_history[0]:
        print(f"âœ… ç†µå€¼ä¸‹é™ï¼Œä¸“å®¶æ­£åœ¨åˆ†åŒ–ï¼")
        return True
    else:
        print(f"âš ï¸  ç†µå€¼æœªä¸‹é™ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè¿­ä»£æˆ–è°ƒæ•´aux_loss_alpha")
        return False


if __name__ == "__main__":
    success = test_gate_gradient_flow()
    
    print(f"\n" + "="*80)
    if success:
        print("âœ… è¯Šæ–­ç»“æœï¼šMoE Gateæ¢¯åº¦æµåŠ¨æ­£å¸¸ï¼Œä¸“å®¶åº”è¯¥èƒ½åˆ†åŒ–")
        print("   å¦‚æœå®é™…è®­ç»ƒä¸­ä¸“å®¶ä»ä¸åˆ†åŒ–ï¼Œè¯·æ£€æŸ¥ï¼š")
        print("   1. aux_loss_alphaæ˜¯å¦è¶³å¤Ÿå¤§ï¼ˆå»ºè®®>=0.5ï¼‰")
        print("   2. è§†è§‰ç¼–ç å™¨æ˜¯å¦å†»ç»“ï¼ˆå‡å°‘æ¢¯åº¦å¹²æ‰°ï¼‰")
        print("   3. batch_sizeå’Œå­¦ä¹ ç‡æ˜¯å¦åˆé€‚")
    else:
        print("âŒ è¯Šæ–­ç»“æœï¼šå‘ç°é—®é¢˜ï¼Gateæ¢¯åº¦æµåŠ¨å¼‚å¸¸")
        print("   å»ºè®®ç«‹å³æ£€æŸ¥ï¼š")
        print("   1. AddAuxiliaryLosså®ç°æ˜¯å¦æ­£ç¡®")
        print("   2. aux_lossæ˜¯å¦çœŸçš„åŠ å…¥äº†è®¡ç®—å›¾")
        print("   3. æ˜¯å¦æœ‰æ¢¯åº¦è£å‰ªè¿‡äºæ¿€è¿›")
    print("="*80)

