"""
ç»Ÿä¸€çš„å¤šæ¨¡æ€è§‚æµ‹ç¼–ç å™¨ï¼Œæ”¯æŒRGBå›¾åƒã€è§¦è§‰ä¼ æ„Ÿå™¨å’Œä½ç»´çŠ¶æ€
å¤ç”¨TimmEncoderçš„RGBå¤„ç†é€»è¾‘å’ŒTimmTactileEncoderçš„è§¦è§‰å¤„ç†é€»è¾‘

åŠŸèƒ½è¯´æ˜:
    - RGBç›¸æœº: ä½¿ç”¨timmé¢„è®­ç»ƒæ¨¡å‹(ResNet/ViT/SigLIPç­‰)æå–è§†è§‰ç‰¹å¾
    - è§¦è§‰ä¼ æ„Ÿå™¨: ä½¿ç”¨TimmTactileEncoderæå–è§¦è§‰ç‰¹å¾
    - ä½ç»´çŠ¶æ€: ç›´æ¥å±•å¹³æ‹¼æ¥
    - æœ€ç»ˆè¾“å‡º: æ‰€æœ‰æ¨¡æ€ç‰¹å¾æŒ‰é¡ºåºæ‹¼æ¥ (RGB -> è§¦è§‰ -> ä½ç»´)
"""
import copy
import timm
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import logging
from termcolor import cprint

from maniflow.model.common.module_attr_mixin import ModuleAttrMixin
from maniflow.common.pytorch_util import replace_submodules
from maniflow.model.tactile.tactile_encoder import TimmTactileEncoder

logger = logging.getLogger(__name__)


class AttentionPool2d(nn.Module):
    """æ³¨æ„åŠ›æ± åŒ–å±‚ï¼Œç”¨äºRGBç‰¹å¾èšåˆ"""
    
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)
        x, _ = F.multi_head_attention_forward(
            query=x[:1], key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)
class CrossAttentionBlock(nn.Module):
    """
    å›¾åƒ-è§¦è§‰äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œæ”¯æŒå¤šç§æ³¨æ„åŠ›æ¨¡å¼
    
    Args:
        embed_dim: åµŒå…¥ç»´åº¦
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        dropout: dropoutæ¯”ç‡
        attention_type: æ³¨æ„åŠ›ç±»å‹ï¼Œæ”¯æŒï¼š
            - 'cls': åªä½¿ç”¨CLS tokenè¿›è¡Œäº¤å‰æ³¨æ„åŠ›ï¼ˆé€‚åˆViTç­‰æœ‰CLS tokençš„æ¨¡å‹ï¼‰
            - 'avg': ä½¿ç”¨å¹³å‡æ± åŒ–çš„image tokenè¿›è¡Œäº¤å‰æ³¨æ„åŠ›ï¼ˆè®¡ç®—é«˜æ•ˆï¼‰
            - 'all_patch': ä½¿ç”¨æ‰€æœ‰patch tokensè¿›è¡Œäº¤å‰æ³¨æ„åŠ›ï¼ˆæœ€ç»†ç²’åº¦ï¼Œè®¡ç®—é‡å¤§ï¼‰
            - 'hybrid': CLS tokenæ›´æ–° + éƒ¨åˆ†patch tokensä¸è§¦è§‰äº¤äº’
    """
    def __init__(self, embed_dim=768, num_heads=8, dropout=0.0, attention_type='cls'):
        super().__init__()
        self.attention_type = attention_type
        cprint(f"âœ“ åˆå§‹åŒ–CrossAttentionBlock: attention_type={attention_type}, "
               f"embed_dim={embed_dim}, num_heads={num_heads}", 'cyan')
        
        # è§¦è§‰â†’å›¾åƒçš„æ³¨æ„åŠ›
        self.attn_t2i = nn.MultiheadAttention(embed_dim, num_heads, 
                                              dropout=dropout, batch_first=True)
        # å›¾åƒâ†’è§¦è§‰çš„æ³¨æ„åŠ›
        self.attn_i2t = nn.MultiheadAttention(embed_dim, num_heads,
                                              dropout=dropout, batch_first=True)
        
        # LayerNorm
        self.ln_tact = nn.LayerNorm(embed_dim)
        self.ln_img = nn.LayerNorm(embed_dim)
        
        # å¦‚æœæ˜¯avgæ¨¡å¼ï¼Œéœ€è¦ä¸€ä¸ªé¢å¤–çš„æŠ•å½±å±‚æ¥å¤„ç†èšåˆåçš„token
        if attention_type == 'avg':
            self.img_token_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, image_tokens, tactile_tokens):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            image_tokens: (B, 1+P, D) - å›¾åƒtokensï¼Œç¬¬ä¸€ä¸ªæ˜¯CLS tokenï¼Œå…¶ä½™æ˜¯patch tokens
            tactile_tokens: (B, Q, D) - è§¦è§‰tokens
            
        Returns:
            image_tokens: (B, 1+P, D) - æ›´æ–°åçš„å›¾åƒtokens
            tactile_tokens: (B, Q, D) - æ›´æ–°åçš„è§¦è§‰tokens
            attn_weights: æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        """
        B, N, D = image_tokens.shape  # N = 1(CLS) + P(patches)
        cls_tok = image_tokens[:, :1, :]   # (B, 1, D)
        patch_tok = image_tokens[:, 1:, :]  # (B, P, D)
        
        if self.attention_type == "cls":
            # ========== CLSæ¨¡å¼ï¼šåªä½¿ç”¨CLS tokenè¿›è¡Œäº¤å‰æ³¨æ„åŠ› ==========
            # 1) è§¦è§‰ â†’ CLSï¼šè®©è§¦è§‰ä¿¡æ¯å…³æ³¨å…¨å±€è§†è§‰ç‰¹å¾
            tact_out, attn_weights = self.attn_t2i(
                query=tactile_tokens,   # (B, Q, D)
                key=cls_tok,            # (B, 1, D)
                value=cls_tok,          # (B, 1, D)
                need_weights=True,
                average_attn_weights=False
            )
            tactile_tokens = self.ln_tact(tactile_tokens + tact_out)

            # 2) CLS â†’ è§¦è§‰ï¼šè®©å…¨å±€è§†è§‰ç‰¹å¾èåˆè§¦è§‰ä¿¡æ¯
            img_out, _ = self.attn_i2t(
                query=cls_tok,          # (B, 1, D)
                key=tactile_tokens,     # (B, Q, D)
                value=tactile_tokens,   # (B, Q, D)
            )
            cls_tok = self.ln_img(cls_tok + img_out)

            # 3) é‡ç»„ï¼špatch tokensä¿æŒä¸å˜
            image_tokens = torch.cat([cls_tok, patch_tok], dim=1)
            
        elif self.attention_type == "avg":
            # ========== AVGæ¨¡å¼ï¼šä½¿ç”¨å¹³å‡æ± åŒ–çš„image token ==========
            # 1) å¹³å‡æ± åŒ–æ‰€æœ‰tokensï¼ˆåŒ…æ‹¬CLSå’Œpatchesï¼‰
            avg_img_tok = torch.mean(image_tokens, dim=1, keepdim=True)  # (B, 1, D)
            avg_img_tok = self.img_token_proj(avg_img_tok)  # å¯å­¦ä¹ çš„æŠ•å½±
            
            # 2) è§¦è§‰ â†’ å¹³å‡å›¾åƒtoken
            tact_out, attn_weights = self.attn_t2i(
                query=tactile_tokens,   # (B, Q, D)
                key=avg_img_tok,        # (B, 1, D)
                value=avg_img_tok,      # (B, 1, D)
                need_weights=True,
                average_attn_weights=False
            )
            tactile_tokens = self.ln_tact(tactile_tokens + tact_out)
            
            # 3) å¹³å‡å›¾åƒtoken â†’ è§¦è§‰
            img_out, _ = self.attn_i2t(
                query=avg_img_tok,      # (B, 1, D)
                key=tactile_tokens,     # (B, Q, D)
                value=tactile_tokens,   # (B, Q, D)
            )
            avg_img_tok = self.ln_img(avg_img_tok + img_out)
            
            # 4) å°†æ›´æ–°åçš„ä¿¡æ¯å¹¿æ’­å›æ‰€æœ‰tokensï¼ˆç®€å•ç›¸åŠ ï¼‰
            image_tokens = image_tokens + avg_img_tok
            
        elif self.attention_type == "all_patch":
            # ========== ALL_PATCHæ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰patch tokensï¼ˆä¸å«CLSï¼‰ ==========
            # 1) è§¦è§‰ â†’ æ‰€æœ‰patchesï¼šç»†ç²’åº¦çš„ç©ºé—´äº¤äº’
            tact_out, attn_weights = self.attn_t2i(
                query=tactile_tokens,   # (B, Q, D)
                key=patch_tok,          # (B, P, D)
                value=patch_tok,        # (B, P, D)
                need_weights=True,
                average_attn_weights=False,
            )
            tactile_tokens = self.ln_tact(tactile_tokens + tact_out)

            # 2) æ‰€æœ‰patches â†’ è§¦è§‰ï¼šè®©æ¯ä¸ªpatchéƒ½èƒ½æ„ŸçŸ¥è§¦è§‰ä¿¡æ¯
            patch_out, _ = self.attn_i2t(
                query=patch_tok,        # (B, P, D)
                key=tactile_tokens,     # (B, Q, D)
                value=tactile_tokens,   # (B, Q, D)
            )
            patch_tok = self.ln_img(patch_tok + patch_out)

            # 3) é‡ç»„ï¼šCLSä¿æŒä¸å˜ï¼Œæ›´æ–°åçš„patchesæ‹¼æ¥å›å»
            image_tokens = torch.cat([cls_tok, patch_tok], dim=1)
            
        elif self.attention_type == "hybrid":
            # ========== HYBRIDæ¨¡å¼ï¼šCLS + Patcheséƒ½å‚ä¸äº¤å‰æ³¨æ„åŠ› ==========
            # 1) è§¦è§‰ â†’ æ‰€æœ‰å›¾åƒtokensï¼ˆCLS + patchesï¼‰
            tact_out, attn_weights = self.attn_t2i(
                query=tactile_tokens,   # (B, Q, D)
                key=image_tokens,       # (B, 1+P, D)
                value=image_tokens,     # (B, 1+P, D)
                need_weights=True,
                average_attn_weights=False,
            )
            tactile_tokens = self.ln_tact(tactile_tokens + tact_out)

            # 2) æ‰€æœ‰å›¾åƒtokens â†’ è§¦è§‰
            img_out, _ = self.attn_i2t(
                query=image_tokens,     # (B, 1+P, D)
                key=tactile_tokens,     # (B, Q, D)
                value=tactile_tokens,   # (B, Q, D)
            )
            image_tokens = self.ln_img(image_tokens + img_out)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„attention_type: {self.attention_type}. "
                           f"æ”¯æŒçš„ç±»å‹: ['cls', 'avg', 'all_patch', 'hybrid']")

        return image_tokens, tactile_tokens, attn_weights

class TimmMultimodalEncoder(ModuleAttrMixin):
    """
    å¤šæ¨¡æ€è§‚æµ‹ç¼–ç å™¨ï¼Œç»Ÿä¸€å¤„ç†RGBã€è§¦è§‰å’Œä½ç»´çŠ¶æ€
    - RGBå›¾åƒ: ä½¿ç”¨TimmEncoderçš„é€»è¾‘
    - è§¦è§‰ä¼ æ„Ÿå™¨: ä½¿ç”¨ResNet18 + SpatialSoftmax
    - ä½ç»´çŠ¶æ€: ç›´æ¥å±•å¹³
    """
    
    def __init__(self,
            shape_meta: dict,
            model_name: str,
            pretrained: bool,
            frozen: bool,
            global_pool: str,
            transforms: list,
            # RGBç›¸å…³å‚æ•°
            use_group_norm: bool=False,
            share_rgb_model: bool=False,
            imagenet_norm: bool=False,
            feature_aggregation: str='spatial_embedding',
            downsample_ratio: int=32,
            position_encording: str='learnable',
            # è§¦è§‰ç›¸å…³å‚æ•°
            tactile_model_name: str='resnet18',
            tactile_pretrained: bool=False,
            tactile_frozen: bool=False,
            tactile_feature_dim: int=512,
            share_tactile_model: bool=False,
            # äº¤å‰æ³¨æ„åŠ›å‚æ•°
            use_cross_attention: bool=True,
            cross_attention_type: str='cls',
            cross_attention_num_heads: int=8,
            cross_attention_dropout: float=0.0,
            # ğŸ†• æ¨¡æ€çº§MoEæ”¯æŒ
            output_token_sequence: bool=False,
        ):
        """
        Args:
            shape_meta: æ•°æ®å½¢çŠ¶å…ƒä¿¡æ¯
            model_name: RGBç¼–ç å™¨çš„æ¨¡å‹åç§°
            pretrained: RGBæ¨¡å‹æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            frozen: RGBæ¨¡å‹æ˜¯å¦å†»ç»“
            global_pool: å…¨å±€æ± åŒ–ç±»å‹
            transforms: æ•°æ®å¢å¼ºå˜æ¢åˆ—è¡¨
            feature_aggregation: RGBç‰¹å¾èšåˆæ–¹å¼
            downsample_ratio: RGBä¸‹é‡‡æ ·æ¯”ç‡
            tactile_model_name: è§¦è§‰ç¼–ç å™¨æ¨¡å‹åç§°
            tactile_feature_dim: è§¦è§‰ç‰¹å¾è¾“å‡ºç»´åº¦
            share_tactile_model: æ˜¯å¦åœ¨å¤šä¸ªè§¦è§‰ä¼ æ„Ÿå™¨é—´å…±äº«æƒé‡
            use_cross_attention: æ˜¯å¦ä½¿ç”¨å›¾åƒ-è§¦è§‰äº¤å‰æ³¨æ„åŠ›
            cross_attention_type: äº¤å‰æ³¨æ„åŠ›ç±»å‹ ('cls', 'avg', 'all_patch', 'hybrid')
            cross_attention_num_heads: äº¤å‰æ³¨æ„åŠ›çš„å¤´æ•°
            cross_attention_dropout: äº¤å‰æ³¨æ„åŠ›çš„dropoutæ¯”ç‡
        """
        super().__init__()
        
        # åˆ†ç±»è§‚æµ‹key
        rgb_keys = []
        tactile_keys = []
        low_dim_keys = []
        key_shape_map = {}
        
        obs_shape_meta = shape_meta['obs']
        for key, attr in obs_shape_meta.items():
            shape = tuple(attr['shape'])
            obs_type = attr.get('type', 'low_dim')
            key_shape_map[key] = shape
            
            if obs_type == 'rgb':
                # åŒºåˆ†RGBå›¾åƒå’Œè§¦è§‰ä¼ æ„Ÿå™¨
                if 'tactile' in key.lower():
                    tactile_keys.append(key)
                else:
                    rgb_keys.append(key)
            elif obs_type == 'low_dim':
                if not attr.get('ignore_by_policy', False):
                    low_dim_keys.append(key)
        
        rgb_keys = sorted(rgb_keys)
        tactile_keys = sorted(tactile_keys)
        low_dim_keys = sorted(low_dim_keys)
        
        cprint(f"RGBç›¸æœº: {rgb_keys}", 'cyan')
        cprint(f"è§¦è§‰ä¼ æ„Ÿå™¨: {tactile_keys}", 'yellow')
        cprint(f"ä½ç»´çŠ¶æ€: {low_dim_keys}", 'green')
        
        # ============ RGBç¼–ç å™¨åˆå§‹åŒ– ============
        rgb_model_map = nn.ModuleDict()
        rgb_transform_map = nn.ModuleDict()
        rgb_feature_dim = None
        
        if len(rgb_keys) > 0:
            assert global_pool == ''
            
            # åˆ›å»ºRGBæ¨¡å‹
            if model_name == "r3m":
                from r3m import load_r3m
                rgb_base_model = load_r3m("resnet18", pretrained=pretrained)
                rgb_base_model.eval()
                cprint(f"ä½¿ç”¨R3Mæ¨¡å‹: {model_name}, pretrained={pretrained}", 'green')
            else:
                rgb_base_model = timm.create_model(
                    model_name=model_name,
                    pretrained=pretrained,
                    global_pool=global_pool,
                    num_classes=0
                )
            
            if frozen:
                assert pretrained
                for param in rgb_base_model.parameters():
                    param.requires_grad = False
            
            # ç¡®å®šRGBç‰¹å¾ç»´åº¦
            if model_name.startswith('resnet'):
                if downsample_ratio == 32:
                    modules = list(rgb_base_model.children())[:-2]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 512
                elif downsample_ratio == 16:
                    modules = list(rgb_base_model.children())[:-3]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 256
                else:
                    raise NotImplementedError(f"ä¸æ”¯æŒçš„ä¸‹é‡‡æ ·ç‡: {downsample_ratio}")
            elif model_name.startswith('convnext'):
                if downsample_ratio == 32:
                    modules = list(rgb_base_model.children())[:-2]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 1024
                else:
                    raise NotImplementedError(f"ä¸æ”¯æŒçš„ä¸‹é‡‡æ ·ç‡: {downsample_ratio}")
            elif model_name.startswith('r3m'):
                rgb_feature_dim = 512
            elif 'siglip' in model_name.lower():
                if 'base' in model_name:
                    rgb_feature_dim = 768
                elif 'large' in model_name:
                    rgb_feature_dim = 1024
                elif 'so400m' in model_name:
                    rgb_feature_dim = 1152
                else:
                    rgb_feature_dim = 768
                cprint(f"ä½¿ç”¨SigLIPæ¨¡å‹: {model_name}, feature_dim={rgb_feature_dim}", 'green')
            
            # GroupNormæ›¿æ¢
            if use_group_norm and not pretrained:
                rgb_base_model = replace_submodules(
                    root_module=rgb_base_model,
                    predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                    func=lambda x: nn.GroupNorm(
                        num_groups=(x.num_features // 16) if (x.num_features % 16 == 0) else (x.num_features // 8),
                        num_channels=x.num_features)
                )
            
            # è·å–RGBå›¾åƒå°ºå¯¸å¹¶åˆ›å»ºæ•°æ®å¢å¼º
            image_shape = key_shape_map[rgb_keys[0]][1:]  # (H, W)
            if transforms is not None and not isinstance(transforms[0], torch.nn.Module):
                if hasattr(transforms[0], 'type') and transforms[0].type == 'RandomCrop':
                    ratio = transforms[0].ratio
                    transforms = [
                        torchvision.transforms.RandomCrop(size=int(image_shape[0] * ratio)),
                        torchvision.transforms.Resize(size=image_shape[0], antialias=True)
                    ] + transforms[1:]
            transform = nn.Identity() if transforms is None else nn.Sequential(*transforms)
            
            # ä¸ºæ¯ä¸ªRGBç›¸æœºåˆ†é…æ¨¡å‹å’Œå˜æ¢
            for key in rgb_keys:
                this_model = rgb_base_model if share_rgb_model else copy.deepcopy(rgb_base_model)
                rgb_model_map[key] = this_model
                rgb_transform_map[key] = transform
            
            # åˆå§‹åŒ–ç‰¹å¾èšåˆæ¨¡å—
            feature_map_shape = [x // downsample_ratio for x in image_shape]
            self._init_rgb_aggregation(feature_aggregation, rgb_feature_dim, feature_map_shape, 
                                      position_encording, model_name)
        
        # ============ è§¦è§‰ç¼–ç å™¨åˆå§‹åŒ– ============
        tactile_encoder = None
        
        if len(tactile_keys) > 0:
            # æ„é€ è§¦è§‰ä¸“ç”¨çš„shape_metaï¼ˆåªéœ€è¦obsï¼Œä¸éœ€è¦actionï¼‰
            tactile_shape_meta = {
                'obs': {k: v for k, v in obs_shape_meta.items() if k in tactile_keys}
            }
            
            tactile_encoder = TimmTactileEncoder(
                shape_meta=tactile_shape_meta,
                model_name=tactile_model_name,
                pretrained=tactile_pretrained,
                frozen=tactile_frozen,
                use_group_norm=use_group_norm,
                share_tactile_model=share_tactile_model,
                feature_dim=tactile_feature_dim
            )
            
            cprint(f"âœ“ è§¦è§‰ç¼–ç å™¨: {tactile_encoder.tactile_keys}, "
                   f"ç‰¹å¾ç»´åº¦={tactile_feature_dim}, å…±äº«æƒé‡={share_tactile_model}", 'green')
        
        # ============ äº¤å‰æ³¨æ„åŠ›åˆå§‹åŒ– ============
        self.use_cross_attention = use_cross_attention
        self.cross_attention_left = None
        self.cross_attention_right = None
        self.left_rgb_keys = []
        self.right_rgb_keys = []
        self.left_tactile_keys = []
        self.right_tactile_keys = []
        
        if use_cross_attention and len(rgb_keys) > 0 and len(tactile_keys) > 0:
            # åŒºåˆ†å·¦å³æ‰‹çš„RGBç›¸æœºå’Œè§¦è§‰ä¼ æ„Ÿå™¨
            for key in rgb_keys:
                if 'left' in key.lower():
                    self.left_rgb_keys.append(key)
                elif 'right' in key.lower():
                    self.right_rgb_keys.append(key)
            
            for key in tactile_keys:
                if 'left' in key.lower():
                    self.left_tactile_keys.append(key)
                elif 'right' in key.lower():
                    self.right_tactile_keys.append(key)
            
            cprint(f"å·¦æ‰‹RGBç›¸æœº: {self.left_rgb_keys}", 'magenta')
            cprint(f"å³æ‰‹RGBç›¸æœº: {self.right_rgb_keys}", 'magenta')
            cprint(f"å·¦æ‰‹è§¦è§‰ä¼ æ„Ÿå™¨: {self.left_tactile_keys}", 'magenta')
            cprint(f"å³æ‰‹è§¦è§‰ä¼ æ„Ÿå™¨: {self.right_tactile_keys}", 'magenta')
            
            # ç¡®å®šç‰¹å¾ç»´åº¦ï¼ˆéœ€è¦ç»Ÿä¸€RGBå’Œè§¦è§‰çš„ç‰¹å¾ç»´åº¦ï¼‰
            # å¦‚æœç»´åº¦ä¸åŒï¼Œéœ€è¦æ·»åŠ æŠ•å½±å±‚
            assert rgb_feature_dim is not None and tactile_feature_dim is not None
            
            # åˆ›å»ºå·¦æ‰‹äº¤å‰æ³¨æ„åŠ›æ¨¡å—
            if len(self.left_rgb_keys) > 0 and len(self.left_tactile_keys) > 0:
                # å¦‚æœç»´åº¦ä¸åŒï¼Œæ·»åŠ æŠ•å½±å±‚
                if rgb_feature_dim != tactile_feature_dim:
                    self.left_tactile_proj = nn.Linear(tactile_feature_dim, rgb_feature_dim)
                    cprint(f"âœ“ å·¦æ‰‹è§¦è§‰æŠ•å½±å±‚: {tactile_feature_dim} -> {rgb_feature_dim}", 'yellow')
                else:
                    self.left_tactile_proj = nn.Identity()
                
                self.cross_attention_left = CrossAttentionBlock(
                    embed_dim=rgb_feature_dim,
                    num_heads=cross_attention_num_heads,
                    dropout=cross_attention_dropout,
                    attention_type=cross_attention_type
                )
                cprint(f"âœ“ å·¦æ‰‹äº¤å‰æ³¨æ„åŠ›å·²åˆ›å»º: {cross_attention_type} æ¨¡å¼", 'green')
            
            # åˆ›å»ºå³æ‰‹äº¤å‰æ³¨æ„åŠ›æ¨¡å—
            if len(self.right_rgb_keys) > 0 and len(self.right_tactile_keys) > 0:
                # å¦‚æœç»´åº¦ä¸åŒï¼Œæ·»åŠ æŠ•å½±å±‚
                if rgb_feature_dim != tactile_feature_dim:
                    self.right_tactile_proj = nn.Linear(tactile_feature_dim, rgb_feature_dim)
                    cprint(f"âœ“ å³æ‰‹è§¦è§‰æŠ•å½±å±‚: {tactile_feature_dim} -> {rgb_feature_dim}", 'yellow')
                else:
                    self.right_tactile_proj = nn.Identity()
                
                self.cross_attention_right = CrossAttentionBlock(
                    embed_dim=rgb_feature_dim,
                    num_heads=cross_attention_num_heads,
                    dropout=cross_attention_dropout,
                    attention_type=cross_attention_type
                )
                cprint(f"âœ“ å³æ‰‹äº¤å‰æ³¨æ„åŠ›å·²åˆ›å»º: {cross_attention_type} æ¨¡å¼", 'green')
        
        # ä¿å­˜æ‰€æœ‰å±æ€§
        self.model_name = model_name
        self.shape_meta = shape_meta
        self.rgb_keys = rgb_keys
        self.tactile_keys = tactile_keys
        self.low_dim_keys = low_dim_keys
        self.key_shape_map = key_shape_map
        
        self.rgb_model_map = rgb_model_map
        self.rgb_transform_map = rgb_transform_map
        self.rgb_feature_dim = rgb_feature_dim
        self.feature_aggregation = feature_aggregation
        
        self.tactile_encoder = tactile_encoder
        self.tactile_feature_dim = tactile_feature_dim
        
        self.share_rgb_model = share_rgb_model
        self.share_tactile_model = share_tactile_model
        
        # ğŸ†• æ¨¡æ€çº§MoEæ”¯æŒ
        self.output_token_sequence = output_token_sequence
        if output_token_sequence:
            cprint(f"âœ“ å¯ç”¨tokenåºåˆ—è¾“å‡ºæ¨¡å¼ (ç”¨äºæ¨¡æ€çº§MoE)", 'cyan')
            # åˆ›å»ºæœ¬ä½“æ„ŸçŸ¥æŠ•å½±å±‚ï¼ˆå°†ä½ç»´çŠ¶æ€æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦ï¼‰
            if len(low_dim_keys) > 0:
                total_low_dim = sum(key_shape_map[k][0] if len(key_shape_map[k]) == 1 
                                  else key_shape_map[k][-1] for k in low_dim_keys)
                self.proprio_proj = nn.Linear(total_low_dim, rgb_feature_dim)
                nn.init.xavier_uniform_(self.proprio_proj.weight)
                nn.init.zeros_(self.proprio_proj.bias)
                cprint(f"  âœ“ æœ¬ä½“æ„ŸçŸ¥æŠ•å½±å±‚: {total_low_dim} -> {rgb_feature_dim}", 'green')
        
        logger.info(f"å¤šæ¨¡æ€ç¼–ç å™¨å‚æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def _init_rgb_aggregation(self, feature_aggregation, feature_dim, feature_map_shape, 
                              position_encording, model_name):
        """åˆå§‹åŒ–RGBç‰¹å¾èšåˆæ¨¡å—"""
        # ViTæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
        if model_name.startswith('vit'):
            if feature_aggregation == 'all_tokens':
                pass
            elif feature_aggregation is not None:
                logger.warn(f'ViTä½¿ç”¨CLS tokenï¼Œfeature_aggregation ({feature_aggregation})è¢«å¿½ç•¥')
                self.feature_aggregation = None
        
        # åˆ›å»ºèšåˆæ¨¡å—
        if feature_aggregation == 'soft_attention':
            self.rgb_attention = nn.Sequential(
                nn.Linear(feature_dim, 1, bias=False),
                nn.Softmax(dim=1)
            )
        elif feature_aggregation == 'spatial_embedding':
            self.rgb_spatial_embedding = nn.Parameter(
                torch.randn(feature_map_shape[0] * feature_map_shape[1], feature_dim))
        elif feature_aggregation == 'transformer':
            if position_encording == 'learnable':
                self.rgb_position_embedding = nn.Parameter(
                    torch.randn(feature_map_shape[0] * feature_map_shape[1] + 1, feature_dim))
            elif position_encording == 'sinusoidal':
                num_features = feature_map_shape[0] * feature_map_shape[1] + 1
                pos_embed = torch.zeros(num_features, feature_dim)
                position = torch.arange(0, num_features, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, feature_dim, 2).float() * 
                                    (-math.log(2 * num_features) / feature_dim))
                pos_embed[:, 0::2] = torch.sin(position * div_term)
                pos_embed[:, 1::2] = torch.cos(position * div_term)
                self.rgb_position_embedding = pos_embed
            self.rgb_aggregation_transformer = nn.TransformerEncoder(
                encoder_layer=nn.TransformerEncoderLayer(d_model=feature_dim, nhead=4),
                num_layers=4)
        elif feature_aggregation == 'attention_pool_2d':
            self.rgb_attention_pool_2d = AttentionPool2d(
                spacial_dim=feature_map_shape[0],
                embed_dim=feature_dim,
                num_heads=feature_dim // 64,
                output_dim=feature_dim
            )
    
    def aggregate_rgb_feature(self, feature):
        """èšåˆRGBç‰¹å¾"""
        if self.model_name == 'r3m':
            return feature
        
        # SigLIPæ¨¡å‹å¤„ç†
        if 'siglip' in self.model_name.lower():
            if self.feature_aggregation == 'avg' or self.feature_aggregation is None:
                return torch.mean(feature, dim=1)
            elif self.feature_aggregation == 'all_tokens':
                return feature
            else:
                logger.warn(f'SigLIPä½¿ç”¨mean pooling')
                return torch.mean(feature, dim=1)
        
        # ViTæ¨¡å‹å¤„ç†
        if self.model_name.startswith('vit'):
            if self.feature_aggregation is None or self.feature_aggregation == 'cls_token':
                return feature[:, 0, :]
            elif self.feature_aggregation == 'all_tokens':
                return feature
            else:
                logger.warn(f'ViTä½¿ç”¨CLS token')
                return feature[:, 0, :]
        
        # ResNetå¤„ç†
        assert len(feature.shape) == 4
        if self.feature_aggregation == 'attention_pool_2d':
            return self.rgb_attention_pool_2d(feature)
        
        feature = torch.flatten(feature, start_dim=-2)  # B, C, H*W
        feature = torch.transpose(feature, 1, 2)  # B, H*W, C
        
        if self.feature_aggregation == 'avg':
            return torch.mean(feature, dim=[1])
        elif self.feature_aggregation == 'max':
            return torch.amax(feature, dim=[1])
        elif self.feature_aggregation == 'soft_attention':
            weight = self.rgb_attention(feature)
            return torch.sum(feature * weight, dim=1)
        elif self.feature_aggregation == 'spatial_embedding':
            return torch.mean(feature * self.rgb_spatial_embedding, dim=1)
        elif self.feature_aggregation == 'transformer':
            zero_feature = torch.zeros(feature.shape[0], 1, feature.shape[-1], device=feature.device)
            if self.rgb_position_embedding.device != feature.device:
                self.rgb_position_embedding = self.rgb_position_embedding.to(feature.device)
            feature_with_pos = torch.cat([zero_feature, feature], dim=1) + self.rgb_position_embedding
            feature_output = self.rgb_aggregation_transformer(feature_with_pos)
            return feature_output[:, 0]
        else:
            # feature_aggregationä¸ºNoneæ—¶ï¼Œå±•å¹³æ‰€æœ‰ç©ºé—´ç‰¹å¾
            assert self.feature_aggregation is None
            return feature.reshape(feature.shape[0], -1)  # B, H*W*C
    
    def _extract_rgb_tokens(self, obs_dict, key):
        """
        æå–RGBå›¾åƒçš„tokenè¡¨ç¤ºï¼ˆç”¨äºäº¤å‰æ³¨æ„åŠ›ï¼‰
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸
            key: RGBç›¸æœºçš„key
            
        Returns:
            tokens: (B*T, N, D) - Nä¸ºtokenæ•°é‡ï¼ˆ1+P for ViT, H*W for CNNï¼‰
            batch_size: B
            time_steps: T
        """
        img = obs_dict[key]
        
        # å½’ä¸€åŒ–
        if img.max() > 1.0:
            img = img / 255.0
        
        # è°ƒæ•´ç»´åº¦é¡ºåº: (B,T,H,W,C) -> (B,T,C,H,W)
        if img.shape[-1] == 3:
            if len(img.shape) == 5:
                img = img.permute(0, 1, 4, 2, 3)
            elif len(img.shape) == 4:
                img = img.permute(0, 3, 1, 2)
        
        B, T = img.shape[:2]
        img = img.reshape(B*T, *img.shape[2:])
        
        # Resizeåˆ°æœŸæœ›å°ºå¯¸
        if img.shape[1:] != self.key_shape_map[key]:
            target_H, target_W = self.key_shape_map[key][1], self.key_shape_map[key][2]
            img = F.interpolate(img, size=(target_H, target_W), 
                               mode='bilinear', align_corners=False)
        
        # å‰å‘ä¼ æ’­è·å–åŸå§‹ç‰¹å¾ï¼ˆä¸èšåˆï¼‰
        img = self.rgb_transform_map[key](img).to(self.device)
        img = img.float()
        raw_feature = self.rgb_model_map[key](img).to(self.device)
        
        # è½¬æ¢ä¸ºtokenæ ¼å¼
        if self.model_name.startswith('vit') or 'siglip' in self.model_name.lower():
            # ViT/SigLIP: å·²ç»æ˜¯tokenæ ¼å¼ (B*T, N, D)
            tokens = raw_feature
        else:
            # CNN: éœ€è¦è½¬æ¢ (B*T, C, H, W) -> (B*T, H*W, C)
            # æ·»åŠ ä¸€ä¸ªè™šæ‹Ÿçš„CLS token
            tokens = torch.flatten(raw_feature, start_dim=-2)  # (B*T, C, H*W)
            tokens = torch.transpose(tokens, 1, 2)  # (B*T, H*W, C)
            # æ·»åŠ CLS token (å‡å€¼æ± åŒ–)
            cls_token = torch.mean(tokens, dim=1, keepdim=True)  # (B*T, 1, C)
            tokens = torch.cat([cls_token, tokens], dim=1)  # (B*T, 1+H*W, C)
        
        return tokens, B, T
    
    def forward(self, obs_dict):
        """
        å‰å‘ä¼ æ’­ï¼Œç»Ÿä¸€å¤„ç†æ‰€æœ‰æ¨¡æ€
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸ï¼Œæ¯ä¸ªkeyå¯¹åº”(B, T, ...)çš„å¼ é‡
            
        Returns:
            features: æ‹¼æ¥åçš„ç‰¹å¾å‘é‡ (B, D_total) æˆ– tokenåºåˆ— (B, L_tokens, D)
        """
        batch_size = next(iter(obs_dict.values())).shape[0]
        
        # ğŸ†• Tokenåºåˆ—æ¨¡å¼ï¼ˆç”¨äºæ¨¡æ€çº§MoEï¼‰
        if self.output_token_sequence:
            return self._forward_token_sequence(obs_dict, batch_size)
        
        # åŸå§‹æ¨¡å¼ï¼šæ‹¼æ¥æ‰€æœ‰ç‰¹å¾ä¸ºä¸€ä¸ªå‘é‡
        features = []
        
        # ============ å¤„ç†RGBå›¾åƒå’Œè§¦è§‰ä¼ æ„Ÿå™¨ï¼ˆæ”¯æŒäº¤å‰æ³¨æ„åŠ›ï¼‰ ============
        if self.use_cross_attention and self.tactile_encoder is not None:
            # å…ˆæå–æ‰€æœ‰è§¦è§‰ç‰¹å¾çš„tokenè¡¨ç¤º
            tactile_obs = {k: obs_dict[k] for k in self.tactile_keys if k in obs_dict}
            tactile_features_dict = self.tactile_encoder.forward_tokens(tactile_obs) if hasattr(self.tactile_encoder, 'forward_tokens') else {}
            
            # å¦‚æœè§¦è§‰ç¼–ç å™¨æ²¡æœ‰forward_tokensæ–¹æ³•ï¼Œä½¿ç”¨æ™®é€šforward
            if not tactile_features_dict:
                tactile_features_dict = self.tactile_encoder(tactile_obs)  # Dict[key, (B, 1, D)]
                # è½¬æ¢ä¸ºtokenæ ¼å¼
                for k, v in tactile_features_dict.items():
                    if len(v.shape) == 2:
                        v = v.unsqueeze(1)  # (B, D) -> (B, 1, D)
                    tactile_features_dict[k] = v
            
            # ========== å·¦æ‰‹äº¤å‰æ³¨æ„åŠ› ==========
            if self.cross_attention_left is not None and len(self.left_rgb_keys) > 0 and len(self.left_tactile_keys) > 0:
                # æå–å·¦æ‰‹RGB tokens
                left_rgb_tokens_list = []
                for key in self.left_rgb_keys:
                    tokens, B, T = self._extract_rgb_tokens(obs_dict, key)
                    left_rgb_tokens_list.append(tokens)
                
                # åˆå¹¶å·¦æ‰‹RGB tokensï¼ˆç®€å•æ‹¼æ¥ï¼‰
                left_rgb_tokens = torch.cat(left_rgb_tokens_list, dim=1)  # (B*T, N_total, D)
                
                # æå–å·¦æ‰‹è§¦è§‰tokens
                left_tactile_tokens_list = []
                for key in self.left_tactile_keys:
                    if key in tactile_features_dict:
                        tact_tok = tactile_features_dict[key]  # (B, Q, D)
                        # æ‰©å±•åˆ°æ—¶é—´ç»´åº¦
                        tact_tok = tact_tok.unsqueeze(1).expand(-1, T, -1, -1)  # (B, T, Q, D)
                        tact_tok = tact_tok.reshape(B*T, -1, tact_tok.shape[-1])  # (B*T, Q, D)
                        # æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦
                        tact_tok = self.left_tactile_proj(tact_tok)
                        left_tactile_tokens_list.append(tact_tok)
                
                if len(left_tactile_tokens_list) > 0:
                    left_tactile_tokens = torch.cat(left_tactile_tokens_list, dim=1)  # (B*T, Q_total, D)
                    
                    # åº”ç”¨äº¤å‰æ³¨æ„åŠ›
                    left_rgb_tokens, left_tactile_tokens, _ = self.cross_attention_left(
                        left_rgb_tokens, left_tactile_tokens
                    )
                    
                    # èšåˆå·¦æ‰‹RGBç‰¹å¾
                    left_rgb_feature = self.aggregate_rgb_feature(left_rgb_tokens)
                    if len(left_rgb_feature.shape) == 2:
                        features.append(left_rgb_feature.reshape(B, -1))
                    else:
                        features.append(left_rgb_feature.reshape(B, -1))
                    
                    # èšåˆå·¦æ‰‹è§¦è§‰ç‰¹å¾
                    left_tactile_feature = torch.mean(left_tactile_tokens, dim=1)  # (B*T, D)
                    features.append(left_tactile_feature.reshape(B, -1))
            
            # ========== å³æ‰‹äº¤å‰æ³¨æ„åŠ› ==========
            if self.cross_attention_right is not None and len(self.right_rgb_keys) > 0 and len(self.right_tactile_keys) > 0:
                # æå–å³æ‰‹RGB tokens
                right_rgb_tokens_list = []
                for key in self.right_rgb_keys:
                    tokens, B, T = self._extract_rgb_tokens(obs_dict, key)
                    right_rgb_tokens_list.append(tokens)
                
                # åˆå¹¶å³æ‰‹RGB tokens
                right_rgb_tokens = torch.cat(right_rgb_tokens_list, dim=1)  # (B*T, N_total, D)
                
                # æå–å³æ‰‹è§¦è§‰tokens
                right_tactile_tokens_list = []
                for key in self.right_tactile_keys:
                    if key in tactile_features_dict:
                        tact_tok = tactile_features_dict[key]  # (B, Q, D)
                        # æ‰©å±•åˆ°æ—¶é—´ç»´åº¦
                        tact_tok = tact_tok.unsqueeze(1).expand(-1, T, -1, -1)  # (B, T, Q, D)
                        tact_tok = tact_tok.reshape(B*T, -1, tact_tok.shape[-1])  # (B*T, Q, D)
                        # æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦
                        tact_tok = self.right_tactile_proj(tact_tok)
                        right_tactile_tokens_list.append(tact_tok)
                
                if len(right_tactile_tokens_list) > 0:
                    right_tactile_tokens = torch.cat(right_tactile_tokens_list, dim=1)  # (B*T, Q_total, D)
                    
                    # åº”ç”¨äº¤å‰æ³¨æ„åŠ›
                    right_rgb_tokens, right_tactile_tokens, _ = self.cross_attention_right(
                        right_rgb_tokens, right_tactile_tokens
                    )
                    
                    # èšåˆå³æ‰‹RGBç‰¹å¾
                    right_rgb_feature = self.aggregate_rgb_feature(right_rgb_tokens)
                    if len(right_rgb_feature.shape) == 2:
                        features.append(right_rgb_feature.reshape(B, -1))
                    else:
                        features.append(right_rgb_feature.reshape(B, -1))
                    
                    # èšåˆå³æ‰‹è§¦è§‰ç‰¹å¾
                    right_tactile_feature = torch.mean(right_tactile_tokens, dim=1)  # (B*T, D)
                    features.append(right_tactile_feature.reshape(B, -1))
            
            # ========== å¤„ç†å…¶ä»–RGBç›¸æœºï¼ˆæ²¡æœ‰é…å¯¹è§¦è§‰çš„ï¼‰ ==========
            other_rgb_keys = [k for k in self.rgb_keys 
                            if k not in self.left_rgb_keys and k not in self.right_rgb_keys]
            
            for key in other_rgb_keys:
                img = obs_dict[key]
                
                # å½’ä¸€åŒ–
                if img.max() > 1.0:
                    img = img / 255.0
                
                # è°ƒæ•´ç»´åº¦é¡ºåº
                if img.shape[-1] == 3:
                    if len(img.shape) == 5:
                        img = img.permute(0, 1, 4, 2, 3)
                    elif len(img.shape) == 4:
                        img = img.permute(0, 3, 1, 2)
                
                B, T = img.shape[:2]
                img = img.reshape(B*T, *img.shape[2:])
                
                # Resize
                if img.shape[1:] != self.key_shape_map[key]:
                    target_H, target_W = self.key_shape_map[key][1], self.key_shape_map[key][2]
                    img = F.interpolate(img, size=(target_H, target_W), 
                                       mode='bilinear', align_corners=False)
                
                # å‰å‘ä¼ æ’­
                img = self.rgb_transform_map[key](img).to(self.device)
                img = img.float()
                raw_feature = self.rgb_model_map[key](img).to(self.device)
                feature = self.aggregate_rgb_feature(raw_feature)
                
                features.append(feature.reshape(B, -1))
            
            # ========== å¤„ç†å…¶ä»–è§¦è§‰ä¼ æ„Ÿå™¨ï¼ˆæ²¡æœ‰é…å¯¹RGBçš„ï¼‰ ==========
            other_tactile_keys = [k for k in self.tactile_keys 
                                 if k not in self.left_tactile_keys and k not in self.right_tactile_keys]
            
            for key in other_tactile_keys:
                if key in tactile_features_dict:
                    feat = tactile_features_dict[key]  # (B, Q, D)
                    feat = torch.mean(feat, dim=1)  # (B, D)
                    features.append(feat.reshape(batch_size, -1))
        
        else:
            # ============ ä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›çš„æ ‡å‡†å¤„ç† ============
            # å¤„ç†RGBå›¾åƒ
            for key in self.rgb_keys:
                img = obs_dict[key]
                
                # å½’ä¸€åŒ–
                if img.max() > 1.0:
                    img = img / 255.0
                
                # è°ƒæ•´ç»´åº¦é¡ºåº: (B,T,H,W,C) -> (B,T,C,H,W)
                if img.shape[-1] == 3:
                    if len(img.shape) == 5:
                        img = img.permute(0, 1, 4, 2, 3)
                    elif len(img.shape) == 4:
                        img = img.permute(0, 3, 1, 2)
                
                B, T = img.shape[:2]
                assert B == batch_size
                img = img.reshape(B*T, *img.shape[2:])
                
                # Resizeåˆ°æœŸæœ›å°ºå¯¸
                if img.shape[1:] != self.key_shape_map[key]:
                    target_H, target_W = self.key_shape_map[key][1], self.key_shape_map[key][2]
                    img = F.interpolate(img, size=(target_H, target_W), 
                                       mode='bilinear', align_corners=False)
                
                # å‰å‘ä¼ æ’­
                img = self.rgb_transform_map[key](img).to(self.device)
                img = img.float()
                raw_feature = self.rgb_model_map[key](img).to(self.device)
                feature = self.aggregate_rgb_feature(raw_feature)
                
                assert len(feature.shape) == 2 and feature.shape[0] == B * T
                features.append(feature.reshape(B, -1))
            
            # å¤„ç†è§¦è§‰ä¼ æ„Ÿå™¨
            if self.tactile_encoder is not None and len(self.tactile_keys) > 0:
                tactile_obs = {k: obs_dict[k] for k in self.tactile_keys if k in obs_dict}
                tactile_features = self.tactile_encoder(tactile_obs)  # Dict[key, (B, 1, D)]
                
                for key in self.tactile_keys:
                    if key in tactile_features:
                        feat = tactile_features[key]  # (B, 1, D)
                        features.append(feat.reshape(batch_size, -1))  # (B, D)
        
        # ============ å¤„ç†ä½ç»´çŠ¶æ€ ============
        for key in self.low_dim_keys:
            data = obs_dict[key].to(self.device)
            B, T = data.shape[:2]
            assert B == batch_size
            assert data.shape[2:] == self.key_shape_map[key]
            features.append(data.reshape(B, -1))
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        result = torch.cat(features, dim=-1)
        
        return result
    
    def _forward_token_sequence(self, obs_dict, batch_size):
        """
        ğŸ†• è¾“å‡ºtokenåºåˆ—æ ¼å¼: (B, L_tokens, D)
        
        æ¨¡æ€ç»„ç»‡ç­–ç•¥ï¼ˆè§¦è§‰èå…¥è…•éƒ¨ï¼‰:
        - head: head_cam tokens (å¦‚æœæœ‰)
        - wrist: left_wrist_cam + right_wrist_cam + å¯¹åº”è§¦è§‰ä¼ æ„Ÿå™¨çš„tokens
                (é€šè¿‡äº¤å‰æ³¨æ„åŠ›å·²èåˆï¼Œä½“ç°è…•éƒ¨è§†è§‰+è§¦è§‰çš„å®Œæ•´æ„ŸçŸ¥)
        - proprio: agent_pos tokens (æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦)
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸
            batch_size: batchå¤§å°
            
        Returns:
            result: (B, L_tokens, D) tokenåºåˆ—
        """
        head_tokens_list = []
        wrist_tokens_list = []
        proprio_features_list = []
        
        # è·å–æ—¶é—´æ­¥æ•°ï¼ˆä»ä»»æ„è§‚æµ‹ä¸­è·å–ï¼‰
        time_steps = next(iter(obs_dict.values())).shape[1]
        
        # ============ å¤„ç†RGBå›¾åƒï¼ˆä½¿ç”¨äº¤å‰æ³¨æ„åŠ›èåˆè§¦è§‰ï¼‰ ============
        if self.use_cross_attention and self.tactile_encoder is not None and len(self.tactile_keys) > 0:
            # å…ˆæå–æ‰€æœ‰è§¦è§‰ç‰¹å¾çš„tokenè¡¨ç¤º
            tactile_obs = {k: obs_dict[k] for k in self.tactile_keys if k in obs_dict}
            tactile_features_dict = self.tactile_encoder.forward_tokens(tactile_obs) if hasattr(self.tactile_encoder, 'forward_tokens') else {}
            
            # å¦‚æœæ²¡æœ‰forward_tokensæ–¹æ³•ï¼Œä½¿ç”¨æ™®é€šforward
            if not tactile_features_dict:
                tactile_features_dict = self.tactile_encoder(tactile_obs)
                for k, v in tactile_features_dict.items():
                    if len(v.shape) == 2:
                        v = v.unsqueeze(1)  # (B, D) -> (B, 1, D)
                    tactile_features_dict[k] = v
            
            # ========== å·¦æ‰‹: è…•éƒ¨ç›¸æœº + è§¦è§‰ï¼ˆäº¤å‰æ³¨æ„åŠ›èåˆï¼‰ ==========
            if self.cross_attention_left is not None and len(self.left_rgb_keys) > 0 and len(self.left_tactile_keys) > 0:
                # æå–å·¦æ‰‹RGB tokens
                left_rgb_tokens_list = []
                for key in self.left_rgb_keys:
                    tokens, B, T = self._extract_rgb_tokens(obs_dict, key)
                    left_rgb_tokens_list.append(tokens)
                
                left_rgb_tokens = torch.cat(left_rgb_tokens_list, dim=1)  # (B*T, N, D)
                
                # æå–å·¦æ‰‹è§¦è§‰tokens
                left_tactile_tokens_list = []
                for key in self.left_tactile_keys:
                    if key in tactile_features_dict:
                        tact_tok = tactile_features_dict[key]  # (B, Q, D)
                        tact_tok = tact_tok.unsqueeze(1).expand(-1, T, -1, -1)  # (B, T, Q, D)
                        tact_tok = tact_tok.reshape(B*T, -1, tact_tok.shape[-1])  # (B*T, Q, D)
                        tact_tok = self.left_tactile_proj(tact_tok)
                        left_tactile_tokens_list.append(tact_tok)
                
                if len(left_tactile_tokens_list) > 0:
                    left_tactile_tokens = torch.cat(left_tactile_tokens_list, dim=1)  # (B*T, Q, D)
                    
                    # ğŸ”¥ äº¤å‰æ³¨æ„åŠ›ï¼šè…•éƒ¨è§†è§‰ â†” è§¦è§‰
                    left_rgb_tokens, left_tactile_tokens, _ = self.cross_attention_left(
                        left_rgb_tokens, left_tactile_tokens
                    )
                    
                    # èšåˆä¸ºæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªtoken (mean pooling)
                    left_rgb_token_agg = torch.mean(left_rgb_tokens, dim=1)  # (B*T, D)
                    left_tactile_token_agg = torch.mean(left_tactile_tokens, dim=1)  # (B*T, D)
                    
                    # åˆå¹¶è…•éƒ¨+è§¦è§‰ï¼šæ‹¼æ¥åå†æŠ•å½±ï¼Œæˆ–ç›´æ¥ç›¸åŠ ï¼ˆç›¸åŠ æ›´ç®€æ´ï¼‰
                    left_wrist_fused = (left_rgb_token_agg + left_tactile_token_agg) / 2  # (B*T, D)
                    left_wrist_fused = left_wrist_fused.reshape(B, T, -1)  # (B, T, D)
                    
                    wrist_tokens_list.append(left_wrist_fused)
            
            # ========== å³æ‰‹: è…•éƒ¨ç›¸æœº + è§¦è§‰ï¼ˆäº¤å‰æ³¨æ„åŠ›èåˆï¼‰ ==========
            if self.cross_attention_right is not None and len(self.right_rgb_keys) > 0 and len(self.right_tactile_keys) > 0:
                # æå–å³æ‰‹RGB tokens
                right_rgb_tokens_list = []
                for key in self.right_rgb_keys:
                    tokens, B, T = self._extract_rgb_tokens(obs_dict, key)
                    right_rgb_tokens_list.append(tokens)
                
                right_rgb_tokens = torch.cat(right_rgb_tokens_list, dim=1)  # (B*T, N, D)
                
                # æå–å³æ‰‹è§¦è§‰tokens
                right_tactile_tokens_list = []
                for key in self.right_tactile_keys:
                    if key in tactile_features_dict:
                        tact_tok = tactile_features_dict[key]  # (B, Q, D)
                        tact_tok = tact_tok.unsqueeze(1).expand(-1, T, -1, -1)  # (B, T, Q, D)
                        tact_tok = tact_tok.reshape(B*T, -1, tact_tok.shape[-1])  # (B*T, Q, D)
                        tact_tok = self.right_tactile_proj(tact_tok)
                        right_tactile_tokens_list.append(tact_tok)
                
                if len(right_tactile_tokens_list) > 0:
                    right_tactile_tokens = torch.cat(right_tactile_tokens_list, dim=1)  # (B*T, Q, D)
                    
                    # ğŸ”¥ äº¤å‰æ³¨æ„åŠ›ï¼šè…•éƒ¨è§†è§‰ â†” è§¦è§‰
                    right_rgb_tokens, right_tactile_tokens, _ = self.cross_attention_right(
                        right_rgb_tokens, right_tactile_tokens
                    )
                    
                    # èšåˆä¸ºæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªtoken
                    right_rgb_token_agg = torch.mean(right_rgb_tokens, dim=1)  # (B*T, D)
                    right_tactile_token_agg = torch.mean(right_tactile_tokens, dim=1)  # (B*T, D)
                    
                    # åˆå¹¶è…•éƒ¨+è§¦è§‰
                    right_wrist_fused = (right_rgb_token_agg + right_tactile_token_agg) / 2  # (B*T, D)
                    right_wrist_fused = right_wrist_fused.reshape(B, T, -1)  # (B, T, D)
                    
                    wrist_tokens_list.append(right_wrist_fused)
            
            # ========== å¤„ç†å…¶ä»–RGBç›¸æœºï¼ˆæ²¡æœ‰é…å¯¹è§¦è§‰çš„ï¼Œå¦‚head_camï¼‰ ==========
            other_rgb_keys = [k for k in self.rgb_keys 
                            if k not in self.left_rgb_keys and k not in self.right_rgb_keys]
            
            for key in other_rgb_keys:
                # åˆ¤æ–­æ˜¯å¦ä¸ºå¤´éƒ¨ç›¸æœº
                is_head_cam = 'head' in key.lower() or 'front' in key.lower()
                
                tokens, B, T = self._extract_rgb_tokens(obs_dict, key)
                # èšåˆä¸ºæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªtoken
                token_agg = torch.mean(tokens, dim=1)  # (B*T, D)
                token_agg = token_agg.reshape(B, T, -1)  # (B, T, D)
                
                if is_head_cam:
                    head_tokens_list.append(token_agg)
                else:
                    wrist_tokens_list.append(token_agg)
        
        else:
            # ============ ä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›çš„æ ‡å‡†å¤„ç† ============
            for key in self.rgb_keys:
                is_head_cam = 'head' in key.lower() or 'front' in key.lower()
                
                tokens, B, T = self._extract_rgb_tokens(obs_dict, key)
                token_agg = torch.mean(tokens, dim=1)  # (B*T, D)
                token_agg = token_agg.reshape(B, T, -1)  # (B, T, D)
                
                if is_head_cam:
                    head_tokens_list.append(token_agg)
                else:
                    wrist_tokens_list.append(token_agg)
        
        # ============ å¤„ç†ä½ç»´çŠ¶æ€ï¼ˆæœ¬ä½“æ„ŸçŸ¥ï¼‰ ============
        for key in self.low_dim_keys:
            data = obs_dict[key].to(self.device)
            B, T = data.shape[:2]
            assert B == batch_size
            proprio_features_list.append(data)  # (B, T, low_dim)
        
        # ============ ç»„è£…æœ€ç»ˆçš„tokenåºåˆ— ============
        all_tokens = []
        modality_info = {'head': 0, 'wrist': 0, 'proprio': 0}
        
        # Head tokens
        if head_tokens_list:
            head_tokens = torch.cat(head_tokens_list, dim=1)  # (B, n_head_cams*T, D)
            all_tokens.append(head_tokens)
            modality_info['head'] = head_tokens.shape[1]
        
        # Wrist tokens (åŒ…å«èåˆåçš„è§¦è§‰ä¿¡æ¯)
        if wrist_tokens_list:
            wrist_tokens = torch.cat(wrist_tokens_list, dim=1)  # (B, n_wrist_cams*T, D)
            all_tokens.append(wrist_tokens)
            modality_info['wrist'] = wrist_tokens.shape[1]
        
        # Proprio tokens (æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦)
        if proprio_features_list:
            proprio_concat = torch.cat(proprio_features_list, dim=-1)  # (B, T, total_low_dim)
            proprio_tokens = self.proprio_proj(proprio_concat.float())  # (B, T, D)
            all_tokens.append(proprio_tokens)
            modality_info['proprio'] = proprio_tokens.shape[1]
        
        result = torch.cat(all_tokens, dim=1)  # (B, L_total, D)
        
        # ä¿å­˜æ¨¡æ€ä¿¡æ¯ä¾›å¤–éƒ¨ä½¿ç”¨
        self._last_modality_info = modality_info
        
        return result
    
    def get_modality_info(self):
        """
        ğŸ†• è·å–æœ€è¿‘ä¸€æ¬¡forwardçš„æ¨¡æ€é•¿åº¦ä¿¡æ¯
        
        Returns:
            modality_info: dict {'head': L_head, 'wrist': L_wrist, 'proprio': L_proprio}
        """
        return getattr(self, '_last_modality_info', None)
    
    @torch.no_grad()
    def output_shape(self):
        """è®¡ç®—è¾“å‡ºç‰¹å¾çš„å½¢çŠ¶"""
        example_obs_dict = {}
        obs_shape_meta = self.shape_meta['obs']
        for key, attr in obs_shape_meta.items():
            shape = tuple(attr['shape'])
            this_obs = torch.zeros(
                (1, attr['horizon']) + shape,
                dtype=self.dtype,
                device=self.device)
            example_obs_dict[key] = this_obs
        
        example_output = self.forward(example_obs_dict)
        
        # ğŸ†• æ”¯æŒtokenåºåˆ—æ¨¡å¼
        if self.output_token_sequence:
            assert len(example_output.shape) == 3  # (B, L, D)
            assert example_output.shape[0] == 1
        else:
            assert len(example_output.shape) == 2  # (B, total_dim)
            assert example_output.shape[0] == 1
        
        return example_output.shape


if __name__ == '__main__':
    print("\n" + "="*80)
    cprint("TimmMultimodalEncoder æµ‹è¯• (ä½¿ç”¨TimmTactileEncoder)", "cyan", attrs=["bold"])
    print("="*80 + "\n")
    
    # æ„é€ shape_meta (RoboTwin2.0ç¯å¢ƒ)
    shape_meta = {
        'obs': {
            'head_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'left_wrist_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'right_wrist_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'left_tactile': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
            'right_tactile': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
            'agent_pos': {'shape': [14], 'type': 'low_dim', 'horizon': 2},
        },
        'action': {'shape': [14], 'horizon': 16}
    }
    
    # åˆ›å»ºç¼–ç å™¨ï¼ˆä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼‰
    encoder = TimmMultimodalEncoder(
        shape_meta=shape_meta,
        model_name='resnet18',
        pretrained=False,
        frozen=False,
        global_pool='',
        transforms=None,
        use_group_norm=True,
        share_rgb_model=False,
        feature_aggregation=None,
        downsample_ratio=32,
        tactile_model_name='resnet18',
        tactile_pretrained=False,
        tactile_feature_dim=512,
        share_tactile_model=True,
        use_cross_attention=False,
    )
    
    # åˆ›å»ºä½¿ç”¨äº¤å‰æ³¨æ„åŠ›çš„ç¼–ç å™¨
    cprint("\n" + "="*80, "cyan")
    cprint("æµ‹è¯•äº¤å‰æ³¨æ„åŠ›ç‰ˆæœ¬", "cyan", attrs=["bold"])
    cprint("="*80, "cyan")
    
    encoder_with_attn = TimmMultimodalEncoder(
        shape_meta=shape_meta,
        model_name='resnet18',
        pretrained=False,
        frozen=False,
        global_pool='',
        transforms=None,
        use_group_norm=True,
        share_rgb_model=False,
        feature_aggregation=None,
        downsample_ratio=32,
        tactile_model_name='resnet18',
        tactile_pretrained=False,
        tactile_feature_dim=512,
        share_tactile_model=True,
        # äº¤å‰æ³¨æ„åŠ›å‚æ•°
        use_cross_attention=True,
        cross_attention_type='cls',
        cross_attention_num_heads=8,
        cross_attention_dropout=0.0,
    )
    
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  - RGBç›¸æœº: {encoder.rgb_keys}")
    print(f"  - è§¦è§‰ä¼ æ„Ÿå™¨: {encoder.tactile_keys}")
    print(f"  - ä½ç»´çŠ¶æ€: {encoder.low_dim_keys}")
    print(f"  - RGBç‰¹å¾ç»´åº¦: {encoder.rgb_feature_dim}")
    print(f"  - è§¦è§‰ç‰¹å¾ç»´åº¦: {encoder.tactile_feature_dim}")
    print(f"  - æ€»å‚æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size, time_steps = 2, 2
    obs = {
        'head_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'left_wrist_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'right_wrist_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'left_tactile': torch.randn(batch_size, time_steps, 1, 16, 32),
        'right_tactile': torch.randn(batch_size, time_steps, 1, 16, 32),
        'agent_pos': torch.randn(batch_size, time_steps, 14),
    }
    
    cprint("\nå‰å‘ä¼ æ’­æµ‹è¯•:", "yellow")
    with torch.no_grad():
        output = encoder(obs)
    
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # ç»´åº¦éªŒè¯ (æ³¨æ„: TimmTactileEncoderå¯¹æ—¶åºç»´åº¦æ±‚å¹³å‡)
    rgb_dim = 3 * 512 * 7 * 7 * time_steps  # 3ç›¸æœº Ã— 512ç‰¹å¾ Ã— 7Ã—7 Ã— 2T
    tactile_dim = 2 * 512  # 2ä¼ æ„Ÿå™¨ Ã— 512ç‰¹å¾ (TimmTactileEncoderå·²å¯¹æ—¶åºæ±‚å¹³å‡)
    lowdim_dim = 14 * time_steps  # 14ç»´ Ã— 2T
    expected_dim = rgb_dim + tactile_dim + lowdim_dim
    
    print(f"  é¢„æœŸç»´åº¦: {expected_dim} (RGB:{rgb_dim} + è§¦è§‰:{tactile_dim} + ä½ç»´:{lowdim_dim})")
    assert output.shape == (batch_size, expected_dim), f"ç»´åº¦ä¸åŒ¹é…! {output.shape} != ({batch_size}, {expected_dim})"
    
    # æ¢¯åº¦æµ‹è¯•
    cprint("\næ¢¯åº¦åå‘ä¼ æ’­æµ‹è¯•:", "yellow")
    obs_grad = {k: v.clone().requires_grad_(True) for k, v in obs.items()}
    output = encoder(obs_grad)
    loss = output.sum()
    loss.backward()
    
    print(f"  left_tactile æ¢¯åº¦èŒƒæ•°: {obs_grad['left_tactile'].grad.norm().item():.6f}")
    print(f"  head_cam æ¢¯åº¦èŒƒæ•°: {obs_grad['head_cam'].grad.norm().item():.6f}")
    
    print("\n" + "="*80)
    cprint("âœ… æ ‡å‡†ç‰ˆæœ¬æµ‹è¯•é€šè¿‡!", "green", attrs=["bold"])
    print("="*80 + "\n")
    
    # æµ‹è¯•äº¤å‰æ³¨æ„åŠ›ç‰ˆæœ¬
    cprint("\nå‰å‘ä¼ æ’­æµ‹è¯•ï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰:", "yellow")
    with torch.no_grad():
        output_with_attn = encoder_with_attn(obs)
    
    print(f"  è¾“å‡ºå½¢çŠ¶: {output_with_attn.shape}")
    print(f"  å·¦æ‰‹é…å¯¹: {encoder_with_attn.left_rgb_keys} <-> {encoder_with_attn.left_tactile_keys}")
    print(f"  å³æ‰‹é…å¯¹: {encoder_with_attn.right_rgb_keys} <-> {encoder_with_attn.right_tactile_keys}")
    
    # æ¢¯åº¦æµ‹è¯•
    cprint("\næ¢¯åº¦åå‘ä¼ æ’­æµ‹è¯•ï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰:", "yellow")
    obs_grad2 = {k: v.clone().requires_grad_(True) for k, v in obs.items()}
    output2 = encoder_with_attn(obs_grad2)
    loss2 = output2.sum()
    loss2.backward()
    
    print(f"  left_tactile æ¢¯åº¦èŒƒæ•°: {obs_grad2['left_tactile'].grad.norm().item():.6f}")
    print(f"  left_wrist_cam æ¢¯åº¦èŒƒæ•°: {obs_grad2['left_wrist_cam'].grad.norm().item():.6f}")
    
    print("\n" + "="*80)
    cprint("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! äº¤å‰æ³¨æ„åŠ›åŠŸèƒ½æ­£å¸¸", "green", attrs=["bold"])
    print("="*80 + "\n")

