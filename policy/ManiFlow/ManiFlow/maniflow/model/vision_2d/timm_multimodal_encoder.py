"""
ç»Ÿä¸€çš„å¤šæ¨¡æ€è§‚æµ‹ç¼–ç å™¨ï¼Œæ”¯æŒRGBå›¾åƒã€è§¦è§‰ä¼ æ„Ÿå™¨å’Œä½ç»´çŠ¶æ€
å¤ç”¨TimmEncoderçš„RGBå¤„ç†é€»è¾‘å’ŒTimmTactileEncoderçš„è§¦è§‰å¤„ç†é€»è¾‘

åŠŸèƒ½è¯´æ˜:
    - RGBç›¸æœº: ä½¿ç”¨timmé¢„è®­ç»ƒæ¨¡å‹(ResNet/ViT/SigLIPç­‰)æå–è§†è§‰ç‰¹å¾
    - è§¦è§‰ä¼ æ„Ÿå™¨: ä½¿ç”¨TimmTactileEncoderæå–è§¦è§‰ç‰¹å¾
    - ä½ç»´çŠ¶æ€: ç›´æ¥å±•å¹³æ‹¼æ¥
    - æœ€ç»ˆè¾“å‡º: æ‰€æœ‰æ¨¡æ€ç‰¹å¾æŒ‰é¡ºåºæ‹¼æ¥ (RGB -> è§¦è§‰ -> ä½ç»´)
"""
import copy
import timm
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import logging
from termcolor import cprint

from maniflow.model.common.module_attr_mixin import ModuleAttrMixin
from maniflow.common.pytorch_util import replace_submodules
from maniflow.model.tactile.tactile_encoder import TimmTactileEncoder

logger = logging.getLogger(__name__)


class AttentionPool2d(nn.Module):
    """æ³¨æ„åŠ›æ± åŒ–å±‚ï¼Œç”¨äºRGBç‰¹å¾èšåˆ"""
    
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)
        x, _ = F.multi_head_attention_forward(
            query=x[:1], key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)

class TimmMultimodalEncoder(ModuleAttrMixin):
    """
    å¤šæ¨¡æ€è§‚æµ‹ç¼–ç å™¨ï¼Œç»Ÿä¸€å¤„ç†RGBã€è§¦è§‰å’Œä½ç»´çŠ¶æ€
    - RGBå›¾åƒ: ä½¿ç”¨TimmEncoderçš„é€»è¾‘
    - è§¦è§‰ä¼ æ„Ÿå™¨: ä½¿ç”¨ResNet18 + SpatialSoftmax
    - ä½ç»´çŠ¶æ€: ç›´æ¥å±•å¹³
    """
    
    def __init__(self,
            shape_meta: dict,
            model_name: str,
            pretrained: bool,
            frozen: bool,
            global_pool: str,
            transforms: list,
            # RGBç›¸å…³å‚æ•°
            use_group_norm: bool=False,
            share_rgb_model: bool=False,
            imagenet_norm: bool=False,
            feature_aggregation: str='spatial_embedding',
            downsample_ratio: int=32,
            position_encording: str='learnable',
            # è§¦è§‰ç›¸å…³å‚æ•°
            tactile_model_name: str='resnet18',
            tactile_pretrained: bool=False,
            tactile_frozen: bool=False,
            tactile_feature_dim: int=512,
            share_tactile_model: bool=False,
            tactile_output_all_patches: bool=False,  # ğŸ”¥ è§¦è§‰æ˜¯å¦è¾“å‡ºæ‰€æœ‰patch tokens
            # ğŸ†• æ¨¡æ€çº§MoEæ”¯æŒ
            output_token_sequence: bool=False,
        ):
        """
        Args:
            shape_meta: æ•°æ®å½¢çŠ¶å…ƒä¿¡æ¯
            model_name: RGBç¼–ç å™¨çš„æ¨¡å‹åç§°
            pretrained: RGBæ¨¡å‹æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            frozen: RGBæ¨¡å‹æ˜¯å¦å†»ç»“
            global_pool: å…¨å±€æ± åŒ–ç±»å‹
            transforms: æ•°æ®å¢å¼ºå˜æ¢åˆ—è¡¨
            feature_aggregation: RGBç‰¹å¾èšåˆæ–¹å¼
            downsample_ratio: RGBä¸‹é‡‡æ ·æ¯”ç‡
            tactile_model_name: è§¦è§‰ç¼–ç å™¨æ¨¡å‹åç§°
            tactile_feature_dim: è§¦è§‰ç‰¹å¾è¾“å‡ºç»´åº¦
            share_tactile_model: æ˜¯å¦åœ¨å¤šä¸ªè§¦è§‰ä¼ æ„Ÿå™¨é—´å…±äº«æƒé‡
            tactile_output_all_patches: è§¦è§‰æ˜¯å¦è¾“å‡ºæ‰€æœ‰patch tokens
            output_token_sequence: æ˜¯å¦è¾“å‡ºtokenåºåˆ—æ ¼å¼ï¼ˆç”¨äºæ¨¡æ€çº§MoEï¼‰
        """
        super().__init__()
        
        # åˆ†ç±»è§‚æµ‹key
        rgb_keys = []
        tactile_keys = []
        low_dim_keys = []
        key_shape_map = {}
        
        obs_shape_meta = shape_meta['obs']
        for key, attr in obs_shape_meta.items():
            shape = tuple(attr['shape'])
            obs_type = attr.get('type', 'low_dim')
            key_shape_map[key] = shape
            
            if obs_type == 'rgb':
                # åŒºåˆ†RGBå›¾åƒå’Œè§¦è§‰ä¼ æ„Ÿå™¨
                if 'tactile' in key.lower():
                    tactile_keys.append(key)
                else:
                    rgb_keys.append(key)
            elif obs_type == 'low_dim':
                if not attr.get('ignore_by_policy', False):
                    low_dim_keys.append(key)
        
        rgb_keys = sorted(rgb_keys)
        tactile_keys = sorted(tactile_keys)
        low_dim_keys = sorted(low_dim_keys)
        
        cprint(f"RGBç›¸æœº: {rgb_keys}", 'cyan')
        cprint(f"è§¦è§‰ä¼ æ„Ÿå™¨: {tactile_keys}", 'yellow')
        cprint(f"ä½ç»´çŠ¶æ€: {low_dim_keys}", 'green')
        
        # ============ RGBç¼–ç å™¨åˆå§‹åŒ– ============
        rgb_model_map = nn.ModuleDict()
        rgb_transform_map = nn.ModuleDict()
        rgb_feature_dim = None
        
        if len(rgb_keys) > 0:
            assert global_pool == ''
            
            # åˆ›å»ºRGBæ¨¡å‹
            if model_name == "r3m":
                from r3m import load_r3m
                rgb_base_model = load_r3m("resnet18", pretrained=pretrained)
                rgb_base_model.eval()
                cprint(f"ä½¿ç”¨R3Mæ¨¡å‹: {model_name}, pretrained={pretrained}", 'green')
            else:
                rgb_base_model = timm.create_model(
                    model_name=model_name,
                    pretrained=pretrained,
                    global_pool=global_pool,
                    num_classes=0
                )
            
            if frozen:
                assert pretrained
                for param in rgb_base_model.parameters():
                    param.requires_grad = False
            
            # ç¡®å®šRGBç‰¹å¾ç»´åº¦
            if model_name.startswith('resnet'):
                if downsample_ratio == 32:
                    modules = list(rgb_base_model.children())[:-2]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 512
                elif downsample_ratio == 16:
                    modules = list(rgb_base_model.children())[:-3]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 256
                else:
                    raise NotImplementedError(f"ä¸æ”¯æŒçš„ä¸‹é‡‡æ ·ç‡: {downsample_ratio}")
            elif model_name.startswith('convnext'):
                if downsample_ratio == 32:
                    modules = list(rgb_base_model.children())[:-2]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 1024
                else:
                    raise NotImplementedError(f"ä¸æ”¯æŒçš„ä¸‹é‡‡æ ·ç‡: {downsample_ratio}")
            elif model_name.startswith('r3m'):
                rgb_feature_dim = 512
            elif 'siglip' in model_name.lower():
                if 'base' in model_name:
                    rgb_feature_dim = 768
                elif 'large' in model_name:
                    rgb_feature_dim = 1024
                elif 'so400m' in model_name:
                    rgb_feature_dim = 1152
                else:
                    rgb_feature_dim = 768
                cprint(f"ä½¿ç”¨SigLIPæ¨¡å‹: {model_name}, feature_dim={rgb_feature_dim}", 'green')
            
            # GroupNormæ›¿æ¢
            if use_group_norm and not pretrained:
                rgb_base_model = replace_submodules(
                    root_module=rgb_base_model,
                    predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                    func=lambda x: nn.GroupNorm(
                        num_groups=(x.num_features // 16) if (x.num_features % 16 == 0) else (x.num_features // 8),
                        num_channels=x.num_features)
                )
            
            # è·å–RGBå›¾åƒå°ºå¯¸å¹¶åˆ›å»ºæ•°æ®å¢å¼º
            image_shape = key_shape_map[rgb_keys[0]][1:]  # (H, W)
            if transforms is not None and not isinstance(transforms[0], torch.nn.Module):
                if hasattr(transforms[0], 'type') and transforms[0].type == 'RandomCrop':
                    ratio = transforms[0].ratio
                    transforms = [
                        torchvision.transforms.RandomCrop(size=int(image_shape[0] * ratio)),
                        torchvision.transforms.Resize(size=image_shape[0], antialias=True)
                    ] + transforms[1:]
            transform = nn.Identity() if transforms is None else nn.Sequential(*transforms)
            
            # ä¸ºæ¯ä¸ªRGBç›¸æœºåˆ†é…æ¨¡å‹å’Œå˜æ¢
            for key in rgb_keys:
                this_model = rgb_base_model if share_rgb_model else copy.deepcopy(rgb_base_model)
                rgb_model_map[key] = this_model
                rgb_transform_map[key] = transform
            
            # åˆå§‹åŒ–ç‰¹å¾èšåˆæ¨¡å—
            feature_map_shape = [x // downsample_ratio for x in image_shape]
            self._init_rgb_aggregation(feature_aggregation, rgb_feature_dim, feature_map_shape, 
                                      position_encording, model_name)
        
        # ============ è§¦è§‰ç¼–ç å™¨åˆå§‹åŒ– ============
        tactile_encoder = None
        
        if len(tactile_keys) > 0:
            # æ„é€ è§¦è§‰ä¸“ç”¨çš„shape_metaï¼ˆåªéœ€è¦obsï¼Œä¸éœ€è¦actionï¼‰
            tactile_shape_meta = {
                'obs': {k: v for k, v in obs_shape_meta.items() if k in tactile_keys}
            }
            
            tactile_encoder = TimmTactileEncoder(
                shape_meta=tactile_shape_meta,
                model_name=tactile_model_name,
                pretrained=tactile_pretrained,
                frozen=tactile_frozen,
                use_group_norm=use_group_norm,
                share_tactile_model=share_tactile_model,
                feature_dim=tactile_feature_dim,
                output_all_patches=tactile_output_all_patches  # ğŸ”¥ ä¼ é€’patchè¾“å‡ºå‚æ•°
            )
            
            cprint(f"âœ“ è§¦è§‰ç¼–ç å™¨: {tactile_encoder.tactile_keys}, "
                   f"ç‰¹å¾ç»´åº¦={tactile_feature_dim}, å…±äº«æƒé‡={share_tactile_model}, "
                   f"è¾“å‡ºpatch tokens={tactile_output_all_patches}", 'green')
        
        # ============ è§¦è§‰æŠ•å½±å±‚ï¼ˆç”¨äºç»´åº¦å¯¹é½ï¼‰ ============
        self.left_rgb_keys = []
        self.right_rgb_keys = []
        self.left_tactile_keys = []
        self.right_tactile_keys = []
        
        # åŒºåˆ†å·¦å³æ‰‹çš„RGBç›¸æœºå’Œè§¦è§‰ä¼ æ„Ÿå™¨ï¼ˆç”¨äºtokenåºåˆ—æ¨¡å¼ï¼‰
        for key in rgb_keys:
            if 'left' in key.lower():
                self.left_rgb_keys.append(key)
            elif 'right' in key.lower():
                self.right_rgb_keys.append(key)
        
        for key in tactile_keys:
            if 'left' in key.lower():
                self.left_tactile_keys.append(key)
            elif 'right' in key.lower():
                self.right_tactile_keys.append(key)
        
        # å¦‚æœRGBå’Œè§¦è§‰ç‰¹å¾ç»´åº¦ä¸åŒï¼Œåˆ›å»ºæŠ•å½±å±‚
        if len(tactile_keys) > 0 and rgb_feature_dim is not None and tactile_feature_dim is not None:
            if rgb_feature_dim != tactile_feature_dim:
                if len(self.left_tactile_keys) > 0:
                    self.left_tactile_proj = nn.Linear(tactile_feature_dim, rgb_feature_dim)
                    cprint(f"âœ“ å·¦æ‰‹è§¦è§‰æŠ•å½±å±‚: {tactile_feature_dim} -> {rgb_feature_dim}", 'yellow')
                
                if len(self.right_tactile_keys) > 0:
                    self.right_tactile_proj = nn.Linear(tactile_feature_dim, rgb_feature_dim)
                    cprint(f"âœ“ å³æ‰‹è§¦è§‰æŠ•å½±å±‚: {tactile_feature_dim} -> {rgb_feature_dim}", 'yellow')
            else:
                cprint(f"âœ“ RGBå’Œè§¦è§‰ç‰¹å¾ç»´åº¦ç›¸åŒ({rgb_feature_dim})ï¼Œæ— éœ€æŠ•å½±", 'green')
        
        # ä¿å­˜æ‰€æœ‰å±æ€§
        self.model_name = model_name
        self.shape_meta = shape_meta
        self.rgb_keys = rgb_keys
        self.tactile_keys = tactile_keys
        self.low_dim_keys = low_dim_keys
        self.key_shape_map = key_shape_map
        
        self.rgb_model_map = rgb_model_map
        self.rgb_transform_map = rgb_transform_map
        self.rgb_feature_dim = rgb_feature_dim
        self.feature_aggregation = feature_aggregation
        
        self.tactile_encoder = tactile_encoder
        self.tactile_feature_dim = tactile_feature_dim
        
        self.share_rgb_model = share_rgb_model
        self.share_tactile_model = share_tactile_model
        
        # ğŸ†• æ¨¡æ€çº§MoEæ”¯æŒ
        self.output_token_sequence = output_token_sequence
        if output_token_sequence:
            cprint(f"âœ“ å¯ç”¨tokenåºåˆ—è¾“å‡ºæ¨¡å¼ (ç”¨äºæ¨¡æ€çº§MoE)", 'cyan')
            # åˆ›å»ºæœ¬ä½“æ„ŸçŸ¥æŠ•å½±å±‚ï¼ˆå°†ä½ç»´çŠ¶æ€æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦ï¼‰
            if len(low_dim_keys) > 0:
                total_low_dim = sum(key_shape_map[k][0] if len(key_shape_map[k]) == 1 
                                  else key_shape_map[k][-1] for k in low_dim_keys)
                self.proprio_proj = nn.Linear(total_low_dim, rgb_feature_dim)
                nn.init.xavier_uniform_(self.proprio_proj.weight)
                nn.init.zeros_(self.proprio_proj.bias)
                cprint(f"  âœ“ æœ¬ä½“æ„ŸçŸ¥æŠ•å½±å±‚: {total_low_dim} -> {rgb_feature_dim}", 'green')
        
        logger.info(f"å¤šæ¨¡æ€ç¼–ç å™¨å‚æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def _init_rgb_aggregation(self, feature_aggregation, feature_dim, feature_map_shape, 
                              position_encording, model_name):
        """åˆå§‹åŒ–RGBç‰¹å¾èšåˆæ¨¡å—"""
        # ViTæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
        if model_name.startswith('vit'):
            if feature_aggregation == 'all_tokens':
                pass
            elif feature_aggregation is not None:
                logger.warn(f'ViTä½¿ç”¨CLS tokenï¼Œfeature_aggregation ({feature_aggregation})è¢«å¿½ç•¥')
                self.feature_aggregation = None
        
        # åˆ›å»ºèšåˆæ¨¡å—
        if feature_aggregation == 'soft_attention':
            self.rgb_attention = nn.Sequential(
                nn.Linear(feature_dim, 1, bias=False),
                nn.Softmax(dim=1)
            )
        elif feature_aggregation == 'spatial_embedding':
            self.rgb_spatial_embedding = nn.Parameter(
                torch.randn(feature_map_shape[0] * feature_map_shape[1], feature_dim))
        elif feature_aggregation == 'transformer':
            if position_encording == 'learnable':
                self.rgb_position_embedding = nn.Parameter(
                    torch.randn(feature_map_shape[0] * feature_map_shape[1] + 1, feature_dim))
            elif position_encording == 'sinusoidal':
                num_features = feature_map_shape[0] * feature_map_shape[1] + 1
                pos_embed = torch.zeros(num_features, feature_dim)
                position = torch.arange(0, num_features, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, feature_dim, 2).float() * 
                                    (-math.log(2 * num_features) / feature_dim))
                pos_embed[:, 0::2] = torch.sin(position * div_term)
                pos_embed[:, 1::2] = torch.cos(position * div_term)
                self.rgb_position_embedding = pos_embed
            self.rgb_aggregation_transformer = nn.TransformerEncoder(
                encoder_layer=nn.TransformerEncoderLayer(d_model=feature_dim, nhead=4),
                num_layers=4)
        elif feature_aggregation == 'attention_pool_2d':
            self.rgb_attention_pool_2d = AttentionPool2d(
                spacial_dim=feature_map_shape[0],
                embed_dim=feature_dim,
                num_heads=feature_dim // 64,
                output_dim=feature_dim
            )
    
    def aggregate_rgb_feature(self, feature):
        """èšåˆRGBç‰¹å¾"""
        if self.model_name == 'r3m':
            return feature
        
        # SigLIP/CLIPæ¨¡å‹å¤„ç†
        if 'siglip' in self.model_name.lower() or 'clip' in self.model_name.lower():
            if self.feature_aggregation == 'all_tokens':
                # ğŸ”¥ è¾“å‡ºæ‰€æœ‰tokens: (B, N, D) å…¶ä¸­ N = num_patches
                return feature
            elif self.feature_aggregation == 'avg' or self.feature_aggregation is None:
                # é»˜è®¤ä½¿ç”¨mean pooling
                return torch.mean(feature, dim=1)
            else:
                logger.warn(f'SigLIP/CLIPä½¿ç”¨mean poolingä½œä¸ºé»˜è®¤èšåˆæ–¹å¼')
                return torch.mean(feature, dim=1)
        
        # ViTæ¨¡å‹å¤„ç†
        if self.model_name.startswith('vit'):
            if self.feature_aggregation == 'all_tokens':
                # ğŸ”¥ è¾“å‡ºæ‰€æœ‰tokens: (B, 1+P, D) - CLS + patches
                return feature
            elif self.feature_aggregation is None or self.feature_aggregation == 'cls_token':
                return feature[:, 0, :]
            else:
                logger.warn(f'ViTä½¿ç”¨CLS tokenä½œä¸ºé»˜è®¤èšåˆæ–¹å¼')
                return feature[:, 0, :]
        
        # ResNetå¤„ç†
        assert len(feature.shape) == 4
        if self.feature_aggregation == 'attention_pool_2d':
            return self.rgb_attention_pool_2d(feature)
        
        feature = torch.flatten(feature, start_dim=-2)  # B, C, H*W
        feature = torch.transpose(feature, 1, 2)  # B, H*W, C
        
        if self.feature_aggregation == 'avg':
            return torch.mean(feature, dim=[1])
        elif self.feature_aggregation == 'max':
            return torch.amax(feature, dim=[1])
        elif self.feature_aggregation == 'soft_attention':
            weight = self.rgb_attention(feature)
            return torch.sum(feature * weight, dim=1)
        elif self.feature_aggregation == 'spatial_embedding':
            return torch.mean(feature * self.rgb_spatial_embedding, dim=1)
        elif self.feature_aggregation == 'transformer':
            zero_feature = torch.zeros(feature.shape[0], 1, feature.shape[-1], device=feature.device)
            if self.rgb_position_embedding.device != feature.device:
                self.rgb_position_embedding = self.rgb_position_embedding.to(feature.device)
            feature_with_pos = torch.cat([zero_feature, feature], dim=1) + self.rgb_position_embedding
            feature_output = self.rgb_aggregation_transformer(feature_with_pos)
            return feature_output[:, 0]
        else:
            # feature_aggregationä¸ºNoneæ—¶ï¼Œå±•å¹³æ‰€æœ‰ç©ºé—´ç‰¹å¾
            assert self.feature_aggregation is None
            return feature.reshape(feature.shape[0], -1)  # B, H*W*C
    
    def _extract_rgb_tokens(self, obs_dict, key):
        """
        æå–RGBå›¾åƒçš„tokenè¡¨ç¤ºï¼ˆç”¨äºäº¤å‰æ³¨æ„åŠ›ï¼‰
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸
            key: RGBç›¸æœºçš„key
            
        Returns:
            tokens: (B*T, N, D) - Nä¸ºtokenæ•°é‡ï¼ˆ1+P for ViT, H*W for CNNï¼‰
            batch_size: B
            time_steps: T
        """
        img = obs_dict[key]
        
        # å½’ä¸€åŒ–
        if img.max() > 1.0:
            img = img / 255.0
        
        # è°ƒæ•´ç»´åº¦é¡ºåº: (B,T,H,W,C) -> (B,T,C,H,W)
        if img.shape[-1] == 3:
            if len(img.shape) == 5:
                img = img.permute(0, 1, 4, 2, 3)
            elif len(img.shape) == 4:
                img = img.permute(0, 3, 1, 2)
        
        B, T = img.shape[:2]
        img = img.reshape(B*T, *img.shape[2:])
        
        # Resizeåˆ°æœŸæœ›å°ºå¯¸
        if img.shape[1:] != self.key_shape_map[key]:
            target_H, target_W = self.key_shape_map[key][1], self.key_shape_map[key][2]
            img = F.interpolate(img, size=(target_H, target_W), 
                               mode='bilinear', align_corners=False)
        
        # å‰å‘ä¼ æ’­è·å–åŸå§‹ç‰¹å¾ï¼ˆä¸èšåˆï¼‰
        img = self.rgb_transform_map[key](img).to(self.device)
        img = img.float()
        raw_feature = self.rgb_model_map[key](img).to(self.device)
        
        # è½¬æ¢ä¸ºtokenæ ¼å¼
        if self.model_name.startswith('vit') or 'siglip' in self.model_name.lower():
            # ViT/SigLIP: å·²ç»æ˜¯tokenæ ¼å¼ (B*T, N, D)
            tokens = raw_feature
        else:
            # CNN: éœ€è¦è½¬æ¢ (B*T, C, H, W) -> (B*T, H*W, C)
            # æ·»åŠ ä¸€ä¸ªè™šæ‹Ÿçš„CLS token
            tokens = torch.flatten(raw_feature, start_dim=-2)  # (B*T, C, H*W)
            tokens = torch.transpose(tokens, 1, 2)  # (B*T, H*W, C)
            # æ·»åŠ CLS token (å‡å€¼æ± åŒ–)
            cls_token = torch.mean(tokens, dim=1, keepdim=True)  # (B*T, 1, C)
            tokens = torch.cat([cls_token, tokens], dim=1)  # (B*T, 1+H*W, C)
        
        return tokens, B, T
    
    def forward(self, obs_dict):
        """
        å‰å‘ä¼ æ’­ï¼Œç»Ÿä¸€å¤„ç†æ‰€æœ‰æ¨¡æ€
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸ï¼Œæ¯ä¸ªkeyå¯¹åº”(B, T, ...)çš„å¼ é‡
            
        Returns:
            features: æ‹¼æ¥åçš„ç‰¹å¾å‘é‡ (B, D_total) æˆ– tokenåºåˆ— (B, L_tokens, D)
        """
        batch_size = next(iter(obs_dict.values())).shape[0]
        
        # ğŸ†• Tokenåºåˆ—æ¨¡å¼ï¼ˆç”¨äºæ¨¡æ€çº§MoEï¼‰
        if self.output_token_sequence:
            return self._forward_token_sequence(obs_dict, batch_size)
        
        # åŸå§‹æ¨¡å¼ï¼šæ‹¼æ¥æ‰€æœ‰ç‰¹å¾ä¸ºä¸€ä¸ªå‘é‡
        features = []
        
        # ============ å¤„ç†RGBå›¾åƒ ============
        for key in self.rgb_keys:
            img = obs_dict[key]
            
            # å½’ä¸€åŒ–
            if img.max() > 1.0:
                img = img / 255.0
            
            # è°ƒæ•´ç»´åº¦é¡ºåº: (B,T,H,W,C) -> (B,T,C,H,W)
            if img.shape[-1] == 3:
                if len(img.shape) == 5:
                    img = img.permute(0, 1, 4, 2, 3)
                elif len(img.shape) == 4:
                    img = img.permute(0, 3, 1, 2)
            
            B, T = img.shape[:2]
            assert B == batch_size
            img = img.reshape(B*T, *img.shape[2:])
            
            # Resizeåˆ°æœŸæœ›å°ºå¯¸
            if img.shape[1:] != self.key_shape_map[key]:
                target_H, target_W = self.key_shape_map[key][1], self.key_shape_map[key][2]
                img = F.interpolate(img, size=(target_H, target_W), 
                                   mode='bilinear', align_corners=False)
            
            # å‰å‘ä¼ æ’­
            img = self.rgb_transform_map[key](img).to(self.device)
            img = img.float()
            raw_feature = self.rgb_model_map[key](img).to(self.device)
            feature = self.aggregate_rgb_feature(raw_feature)
            
            assert len(feature.shape) == 2 and feature.shape[0] == B * T
            features.append(feature.reshape(B, -1))
        
        # ============ å¤„ç†è§¦è§‰ä¼ æ„Ÿå™¨ ============
        if self.tactile_encoder is not None and len(self.tactile_keys) > 0:
            tactile_obs = {k: obs_dict[k] for k in self.tactile_keys if k in obs_dict}
            tactile_features = self.tactile_encoder(tactile_obs)  # Dict[key, (B, T, D)] or (B, T*H*W, D)
            
            for key in self.tactile_keys:
                if key in tactile_features:
                    feat = tactile_features[key]  # (B, T, D) or (B, T*H*W, D)
                    features.append(feat.reshape(batch_size, -1))  # (B, T*D) or (B, T*H*W*D)
        
        # ============ å¤„ç†ä½ç»´çŠ¶æ€ ============
        for key in self.low_dim_keys:
            data = obs_dict[key].to(self.device)
            B, T = data.shape[:2]
            assert B == batch_size
            assert data.shape[2:] == self.key_shape_map[key]
            features.append(data.reshape(B, -1))
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        result = torch.cat(features, dim=-1)
        
        return result
    
    def _forward_token_sequence(self, obs_dict, batch_size):
        """
        ğŸ†• è¾“å‡ºtokenåºåˆ—æ ¼å¼: (B, L_tokens, D)
        
        æ¨¡æ€ç»„ç»‡ç­–ç•¥ï¼ˆå„æ¨¡æ€ç‹¬ç«‹è¾“å‡ºï¼‰:
        - head: head_cam tokens (å¦‚æœæœ‰)
        - wrist: left_wrist_cam + right_wrist_cam + left_tactile + right_tactile tokens
                (å„è‡ªç‹¬ç«‹è¾“å‡ºï¼Œä¸åšèåˆï¼ŒMoEå¯ä»¥å­¦ä¹ æ¨¡æ€é—´å…³ç³»)
        - proprio: agent_pos tokens (æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦)
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸
            batch_size: batchå¤§å°
            
        Returns:
            result: (B, L_tokens, D) tokenåºåˆ—
        """
        head_tokens_list = []
        wrist_tokens_list = []
        proprio_features_list = []
        
        # è·å–æ—¶é—´æ­¥æ•°ï¼ˆä»ä»»æ„è§‚æµ‹ä¸­è·å–ï¼‰
        time_steps = next(iter(obs_dict.values())).shape[1]
        
        # ============ å¤„ç†RGBå›¾åƒ ============
        for key in self.rgb_keys:
            is_head_cam = 'head' in key.lower() or 'front' in key.lower()
            
            tokens, B, T = self._extract_rgb_tokens(obs_dict, key)
            
            # ğŸ”¥ æ ¹æ®feature_aggregationå†³å®šæ˜¯å¦èšåˆ
            if self.feature_aggregation == 'all_tokens':
                # ä¿ç•™æ‰€æœ‰tokens: (B*T, N, D) -> (B, T*N, D)
                num_tokens = tokens.shape[1]
                token_seq = tokens.reshape(B, T * num_tokens, -1)  # (B, T*N, D)
            else:
                # èšåˆä¸ºæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªtoken
                token_agg = torch.mean(tokens, dim=1)  # (B*T, D)
                token_seq = token_agg.reshape(B, T, -1)  # (B, T, D)
            
            if is_head_cam:
                head_tokens_list.append(token_seq)
            else:
                wrist_tokens_list.append(token_seq)
        
        # ============ å¤„ç†è§¦è§‰ä¼ æ„Ÿå™¨ ============
        if self.tactile_encoder is not None and len(self.tactile_keys) > 0:
            tactile_obs = {k: obs_dict[k] for k in self.tactile_keys if k in obs_dict}
            tactile_features_dict = self.tactile_encoder(tactile_obs)
            
            for key in self.tactile_keys:
                if key in tactile_features_dict:
                    tact_tok = tactile_features_dict[key]  # (B, Q, D) - Qå¯ä»¥æ˜¯Tæˆ–T*H*W
                    
                    # æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if 'left' in key.lower() and hasattr(self, 'left_tactile_proj'):
                        tact_tok = self.left_tactile_proj(tact_tok)
                    elif 'right' in key.lower() and hasattr(self, 'right_tactile_proj'):
                        tact_tok = self.right_tactile_proj(tact_tok)
                    
                    # ğŸ”¥ è§¦è§‰ç¼–ç å™¨ä¿ç•™æ—¶åºç»´åº¦ï¼Œç›´æ¥ä½¿ç”¨
                    # output_all_patches=True: (B, T*H*W, D) - ä¿ç•™æ‰€æœ‰æ—¶é—´æ­¥çš„æ‰€æœ‰patch
                    # output_all_patches=False: (B, T, D) - ä¿ç•™æ‰€æœ‰æ—¶é—´æ­¥ï¼Œæ¯ä¸ªæ—¶é—´æ­¥1ä¸ªtoken
                    wrist_tokens_list.append(tact_tok)
        
        # ============ å¤„ç†ä½ç»´çŠ¶æ€ï¼ˆæœ¬ä½“æ„ŸçŸ¥ï¼‰ ============
        for key in self.low_dim_keys:
            data = obs_dict[key].to(self.device)
            B, T = data.shape[:2]
            assert B == batch_size
            proprio_features_list.append(data)  # (B, T, low_dim)
        
        # ============ ç»„è£…æœ€ç»ˆçš„tokenåºåˆ— ============
        all_tokens = []
        modality_info = {'head': 0, 'wrist': 0, 'proprio': 0}
        
        # Head tokens
        if head_tokens_list:
            head_tokens = torch.cat(head_tokens_list, dim=1)  # (B, n_head_cams*T, D)
            all_tokens.append(head_tokens)
            modality_info['head'] = head_tokens.shape[1]
        
        # Wrist tokens (åŒ…å«èåˆåçš„è§¦è§‰ä¿¡æ¯)
        if wrist_tokens_list:
            wrist_tokens = torch.cat(wrist_tokens_list, dim=1)  # (B, n_wrist_cams*T, D)
            all_tokens.append(wrist_tokens)
            modality_info['wrist'] = wrist_tokens.shape[1]
        
        # Proprio tokens (æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦)
        if proprio_features_list:
            proprio_concat = torch.cat(proprio_features_list, dim=-1)  # (B, T, total_low_dim)
            proprio_tokens = self.proprio_proj(proprio_concat.float())  # (B, T, D)
            all_tokens.append(proprio_tokens)
            modality_info['proprio'] = proprio_tokens.shape[1]
        
        result = torch.cat(all_tokens, dim=1)  # (B, L_total, D)
        
        # ä¿å­˜æ¨¡æ€ä¿¡æ¯ä¾›å¤–éƒ¨ä½¿ç”¨
        self._last_modality_info = modality_info
        
        return result
    
    def get_modality_info(self):
        """
        ğŸ†• è·å–æœ€è¿‘ä¸€æ¬¡forwardçš„æ¨¡æ€é•¿åº¦ä¿¡æ¯
        
        Returns:
            modality_info: dict {'head': L_head, 'wrist': L_wrist, 'proprio': L_proprio}
        """
        return getattr(self, '_last_modality_info', None)
    
    @torch.no_grad()
    def output_shape(self):
        """è®¡ç®—è¾“å‡ºç‰¹å¾çš„å½¢çŠ¶"""
        example_obs_dict = {}
        obs_shape_meta = self.shape_meta['obs']
        for key, attr in obs_shape_meta.items():
            shape = tuple(attr['shape'])
            this_obs = torch.zeros(
                (1, attr['horizon']) + shape,
                dtype=self.dtype,
                device=self.device)
            example_obs_dict[key] = this_obs
        
        example_output = self.forward(example_obs_dict)
        
        # ğŸ†• æ”¯æŒtokenåºåˆ—æ¨¡å¼
        if self.output_token_sequence:
            assert len(example_output.shape) == 3  # (B, L, D)
            assert example_output.shape[0] == 1
        else:
            assert len(example_output.shape) == 2  # (B, total_dim)
            assert example_output.shape[0] == 1
        
        return example_output.shape


if __name__ == '__main__':
    print("\n" + "="*80)
    cprint("TimmMultimodalEncoder æµ‹è¯• (ä½¿ç”¨TimmTactileEncoder)", "cyan", attrs=["bold"])
    print("="*80 + "\n")
    
    # æ„é€ shape_meta (RoboTwin2.0ç¯å¢ƒ)
    shape_meta = {
        'obs': {
            'head_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'left_wrist_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'right_wrist_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'left_tactile': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
            'right_tactile': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
            'agent_pos': {'shape': [14], 'type': 'low_dim', 'horizon': 2},
        },
        'action': {'shape': [14], 'horizon': 16}
    }
    
    # åˆ›å»ºç¼–ç å™¨ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰
    encoder = TimmMultimodalEncoder(
        shape_meta=shape_meta,
        model_name='resnet18',
        pretrained=False,
        frozen=False,
        global_pool='',
        transforms=None,
        use_group_norm=True,
        share_rgb_model=False,
        feature_aggregation=None,
        downsample_ratio=32,
        tactile_model_name='resnet18',
        tactile_pretrained=False,
        tactile_feature_dim=512,
        share_tactile_model=True,
    )
    
    # åˆ›å»ºtokenåºåˆ—è¾“å‡ºæ¨¡å¼çš„ç¼–ç å™¨
    cprint("\n" + "="*80, "cyan")
    cprint("æµ‹è¯•tokenåºåˆ—è¾“å‡ºæ¨¡å¼ï¼ˆç”¨äºæ¨¡æ€çº§MoEï¼‰", "cyan", attrs=["bold"])
    cprint("="*80, "cyan")
    
    encoder_token_seq = TimmMultimodalEncoder(
        shape_meta=shape_meta,
        model_name='resnet18',
        pretrained=False,
        frozen=False,
        global_pool='',
        transforms=None,
        use_group_norm=True,
        share_rgb_model=False,
        feature_aggregation='all_tokens',
        downsample_ratio=32,
        tactile_model_name='resnet18',
        tactile_pretrained=False,
        tactile_feature_dim=512,
        share_tactile_model=True,
        tactile_output_all_patches=True,
        output_token_sequence=True,
    )
    
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  - RGBç›¸æœº: {encoder.rgb_keys}")
    print(f"  - è§¦è§‰ä¼ æ„Ÿå™¨: {encoder.tactile_keys}")
    print(f"  - ä½ç»´çŠ¶æ€: {encoder.low_dim_keys}")
    print(f"  - RGBç‰¹å¾ç»´åº¦: {encoder.rgb_feature_dim}")
    print(f"  - è§¦è§‰ç‰¹å¾ç»´åº¦: {encoder.tactile_feature_dim}")
    print(f"  - æ€»å‚æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size, time_steps = 2, 2
    obs = {
        'head_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'left_wrist_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'right_wrist_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'left_tactile': torch.randn(batch_size, time_steps, 1, 16, 32),
        'right_tactile': torch.randn(batch_size, time_steps, 1, 16, 32),
        'agent_pos': torch.randn(batch_size, time_steps, 14),
    }
    
    cprint("\nå‰å‘ä¼ æ’­æµ‹è¯•:", "yellow")
    with torch.no_grad():
        output = encoder(obs)
    
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # ç»´åº¦éªŒè¯ (æ³¨æ„: TimmTactileEncoderä¿ç•™æ—¶åºç»´åº¦)
    rgb_dim = 3 * 512 * 7 * 7 * time_steps  # 3ç›¸æœº Ã— 512ç‰¹å¾ Ã— 7Ã—7 Ã— 2T
    tactile_dim = 2 * 512 * time_steps  # 2ä¼ æ„Ÿå™¨ Ã— 512ç‰¹å¾ Ã— 2T (ä¿ç•™æ—¶åºç»´åº¦)
    lowdim_dim = 14 * time_steps  # 14ç»´ Ã— 2T
    expected_dim = rgb_dim + tactile_dim + lowdim_dim
    
    print(f"  é¢„æœŸç»´åº¦: {expected_dim} (RGB:{rgb_dim} + è§¦è§‰:{tactile_dim} + ä½ç»´:{lowdim_dim})")
    assert output.shape == (batch_size, expected_dim), f"ç»´åº¦ä¸åŒ¹é…! {output.shape} != ({batch_size}, {expected_dim})"
    
    # æ¢¯åº¦æµ‹è¯•
    cprint("\næ¢¯åº¦åå‘ä¼ æ’­æµ‹è¯•:", "yellow")
    obs_grad = {k: v.clone().requires_grad_(True) for k, v in obs.items()}
    output = encoder(obs_grad)
    loss = output.sum()
    loss.backward()
    
    print(f"  left_tactile æ¢¯åº¦èŒƒæ•°: {obs_grad['left_tactile'].grad.norm().item():.6f}")
    print(f"  head_cam æ¢¯åº¦èŒƒæ•°: {obs_grad['head_cam'].grad.norm().item():.6f}")
    
    print("\n" + "="*80)
    cprint("âœ… æ ‡å‡†ç‰ˆæœ¬æµ‹è¯•é€šè¿‡!", "green", attrs=["bold"])
    print("="*80 + "\n")
    
    # æµ‹è¯•tokenåºåˆ—è¾“å‡ºç‰ˆæœ¬
    cprint("\nå‰å‘ä¼ æ’­æµ‹è¯•ï¼ˆtokenåºåˆ—è¾“å‡ºï¼‰:", "yellow")
    with torch.no_grad():
        output_token_seq = encoder_token_seq(obs)
    
    print(f"  è¾“å‡ºå½¢çŠ¶: {output_token_seq.shape}")
    modality_info = encoder_token_seq.get_modality_info()
    if modality_info:
        print(f"  æ¨¡æ€ä¿¡æ¯: {modality_info}")
    
    # æ¢¯åº¦æµ‹è¯•
    cprint("\næ¢¯åº¦åå‘ä¼ æ’­æµ‹è¯•ï¼ˆtokenåºåˆ—è¾“å‡ºï¼‰:", "yellow")
    obs_grad2 = {k: v.clone().requires_grad_(True) for k, v in obs.items()}
    output2 = encoder_token_seq(obs_grad2)
    loss2 = output2.sum()
    loss2.backward()
    
    print(f"  left_tactile æ¢¯åº¦èŒƒæ•°: {obs_grad2['left_tactile'].grad.norm().item():.6f}")
    print(f"  left_wrist_cam æ¢¯åº¦èŒƒæ•°: {obs_grad2['left_wrist_cam'].grad.norm().item():.6f}")
    
    print("\n" + "="*80)
    cprint("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! Tokenåºåˆ—è¾“å‡ºåŠŸèƒ½æ­£å¸¸", "green", attrs=["bold"])
    print("="*80 + "\n")

