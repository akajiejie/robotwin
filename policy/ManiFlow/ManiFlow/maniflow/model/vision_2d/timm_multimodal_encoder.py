"""
ç»Ÿä¸€çš„å¤šæ¨¡æ€è§‚æµ‹ç¼–ç å™¨ï¼Œæ”¯æŒRGBå›¾åƒã€è§¦è§‰ä¼ æ„Ÿå™¨å’Œä½ç»´çŠ¶æ€
å¤ç”¨TimmEncoderçš„RGBå¤„ç†é€»è¾‘å’ŒTimmTactileEncoderçš„è§¦è§‰å¤„ç†é€»è¾‘

åŠŸèƒ½è¯´æ˜:
    - RGBç›¸æœº: ä½¿ç”¨timmé¢„è®­ç»ƒæ¨¡å‹(ResNet/ViT/SigLIPç­‰)æå–è§†è§‰ç‰¹å¾
    - è§¦è§‰ä¼ æ„Ÿå™¨: ä½¿ç”¨TimmTactileEncoderæå–è§¦è§‰ç‰¹å¾
    - ä½ç»´çŠ¶æ€: ç›´æ¥å±•å¹³æ‹¼æ¥
    - æœ€ç»ˆè¾“å‡º: æ‰€æœ‰æ¨¡æ€ç‰¹å¾æŒ‰é¡ºåºæ‹¼æ¥ (RGB -> è§¦è§‰ -> ä½ç»´)
"""
import copy
import timm
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import logging
from typing import Optional, Tuple, Dict, Union
from termcolor import cprint

from maniflow.model.common.module_attr_mixin import ModuleAttrMixin
from maniflow.common.pytorch_util import replace_submodules
from maniflow.model.tactile.tactile_encoder import TimmTactileEncoder

logger = logging.getLogger(__name__)


class AttentionPool2d(nn.Module):
    """æ³¨æ„åŠ›æ± åŒ–å±‚ï¼Œç”¨äºRGBç‰¹å¾èšåˆ"""
    
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)
        x, _ = F.multi_head_attention_forward(
            query=x[:1], key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)

class TimmMultimodalEncoder(ModuleAttrMixin):
    """
    å¤šæ¨¡æ€è§‚æµ‹ç¼–ç å™¨ï¼Œç»Ÿä¸€å¤„ç†RGBã€è§¦è§‰å’Œä½ç»´çŠ¶æ€
    - RGBå›¾åƒ: ä½¿ç”¨TimmEncoderçš„é€»è¾‘
    - è§¦è§‰ä¼ æ„Ÿå™¨: ä½¿ç”¨ResNet18 + SpatialSoftmax
    - ä½ç»´çŠ¶æ€: ç›´æ¥å±•å¹³
    """
    
    def __init__(self,
            shape_meta: dict,
            model_name: str,
            pretrained: bool,
            frozen: bool,
            global_pool: str,
            transforms: list,
            # RGBç›¸å…³å‚æ•°
            use_group_norm: bool=False,
            share_rgb_model: bool=False,
            imagenet_norm: bool=False,
            feature_aggregation: str='spatial_embedding',
            downsample_ratio: int=32,
            position_encording: str='learnable',
            # è§¦è§‰ç›¸å…³å‚æ•°
            tactile_model_name: str='resnet18',
            tactile_pretrained: bool=False,
            tactile_frozen: bool=False,
            tactile_feature_dim: int=64,  # ğŸ”¥ é»˜è®¤å€¼æ”¹ä¸º64ï¼ˆä¸robomimicä¸€è‡´ï¼‰
            tactile_num_kp: int=32,  # ğŸ†• SpatialSoftmaxå…³é”®ç‚¹æ•°é‡
            tactile_crop_shape: Optional[Union[Tuple[int, int], Dict[str, Tuple[int, int]]]]=None,  # ğŸ†• è§¦è§‰è£å‰ªå°ºå¯¸
            share_tactile_model: bool=False,
            tactile_output_all_patches: bool=False,  # ğŸ”¥ è§¦è§‰æ˜¯å¦è¾“å‡ºæ‰€æœ‰patch tokens
            # ğŸ†• æ¨¡æ€çº§MoEæ”¯æŒ
            output_token_sequence: bool=False,
            head_grid_size: int=1,  # ğŸ”¥ Headç›¸æœºçš„ç©ºé—´é‡é‡‡æ ·ç½‘æ ¼å¤§å° (NxN), é»˜è®¤1=å•token
        ):
        """
        Args:
            shape_meta: æ•°æ®å½¢çŠ¶å…ƒä¿¡æ¯
            model_name: RGBç¼–ç å™¨çš„æ¨¡å‹åç§°
            pretrained: RGBæ¨¡å‹æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            frozen: RGBæ¨¡å‹æ˜¯å¦å†»ç»“
            global_pool: å…¨å±€æ± åŒ–ç±»å‹
            transforms: æ•°æ®å¢å¼ºå˜æ¢åˆ—è¡¨
            feature_aggregation: RGBç‰¹å¾èšåˆæ–¹å¼
            downsample_ratio: RGBä¸‹é‡‡æ ·æ¯”ç‡
            tactile_model_name: è§¦è§‰ç¼–ç å™¨æ¨¡å‹åç§°
            tactile_feature_dim: è§¦è§‰ç‰¹å¾è¾“å‡ºç»´åº¦
            tactile_num_kp: SpatialSoftmaxå…³é”®ç‚¹æ•°é‡ï¼ˆé»˜è®¤32ï¼Œä¸robomimicä¸€è‡´ï¼‰
            tactile_crop_shape: è§¦è§‰å›¾åƒè£å‰ªå°ºå¯¸ï¼Œå¯é€‰
            share_tactile_model: æ˜¯å¦åœ¨å¤šä¸ªè§¦è§‰ä¼ æ„Ÿå™¨é—´å…±äº«æƒé‡
            tactile_output_all_patches: è§¦è§‰æ˜¯å¦è¾“å‡ºæ‰€æœ‰patch tokens
            output_token_sequence: æ˜¯å¦è¾“å‡ºtokenåºåˆ—æ ¼å¼ï¼ˆç”¨äºæ¨¡æ€çº§MoEï¼‰
            head_grid_size: Headç›¸æœºçš„è¾“å‡ºtokenç½‘æ ¼å¤§å°ï¼Œç”Ÿæˆ head_grid_size^2 ä¸ªtokens
        """
        super().__init__()
        
        # åˆ†ç±»è§‚æµ‹key
        rgb_keys = []
        tactile_keys = []
        low_dim_keys = []
        key_shape_map = {}
        
        obs_shape_meta = shape_meta['obs']
        for key, attr in obs_shape_meta.items():
            shape = tuple(attr['shape'])
            obs_type = attr.get('type', 'low_dim')
            key_shape_map[key] = shape
            
            if obs_type == 'rgb':
                # åŒºåˆ†RGBå›¾åƒå’Œè§¦è§‰ä¼ æ„Ÿå™¨
                if 'tactile' in key.lower():
                    tactile_keys.append(key)
                else:
                    rgb_keys.append(key)
            elif obs_type == 'low_dim':
                if not attr.get('ignore_by_policy', False):
                    low_dim_keys.append(key)
        
        rgb_keys = sorted(rgb_keys)
        tactile_keys = sorted(tactile_keys)
        low_dim_keys = sorted(low_dim_keys)
        
        cprint(f"RGBç›¸æœº: {rgb_keys}", 'cyan')
        cprint(f"è§¦è§‰ä¼ æ„Ÿå™¨: {tactile_keys}", 'yellow')
        cprint(f"ä½ç»´çŠ¶æ€: {low_dim_keys}", 'green')
        
        # ============ RGBç¼–ç å™¨åˆå§‹åŒ– ============
        rgb_model_map = nn.ModuleDict()
        rgb_transform_map = nn.ModuleDict()
        rgb_feature_dim = None
        
        if len(rgb_keys) > 0:
            assert global_pool == ''
            
            # åˆ›å»ºRGBæ¨¡å‹
            if model_name == "r3m":
                from r3m import load_r3m
                rgb_base_model = load_r3m("resnet18", pretrained=pretrained)
                rgb_base_model.eval()
                cprint(f"ä½¿ç”¨R3Mæ¨¡å‹: {model_name}, pretrained={pretrained}", 'green')
            else:
                rgb_base_model = timm.create_model(
                    model_name=model_name,
                    pretrained=pretrained,
                    global_pool=global_pool,
                    num_classes=0
                )
            
            if frozen:
                assert pretrained
                for param in rgb_base_model.parameters():
                    param.requires_grad = False
            
            # ç¡®å®šRGBç‰¹å¾ç»´åº¦
            if model_name.startswith('resnet'):
                if downsample_ratio == 32:
                    modules = list(rgb_base_model.children())[:-2]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 512
                elif downsample_ratio == 16:
                    modules = list(rgb_base_model.children())[:-3]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 256
                else:
                    raise NotImplementedError(f"ä¸æ”¯æŒçš„ä¸‹é‡‡æ ·ç‡: {downsample_ratio}")
            elif model_name.startswith('convnext'):
                if downsample_ratio == 32:
                    modules = list(rgb_base_model.children())[:-2]
                    rgb_base_model = nn.Sequential(*modules)
                    rgb_feature_dim = 1024
                else:
                    raise NotImplementedError(f"ä¸æ”¯æŒçš„ä¸‹é‡‡æ ·ç‡: {downsample_ratio}")
            elif model_name.startswith('r3m'):
                rgb_feature_dim = 512
            elif 'siglip' in model_name.lower():
                if 'base' in model_name:
                    rgb_feature_dim = 768
                elif 'large' in model_name:
                    rgb_feature_dim = 1024
                elif 'so400m' in model_name:
                    rgb_feature_dim = 1152
                else:
                    rgb_feature_dim = 768
                cprint(f"ä½¿ç”¨SigLIPæ¨¡å‹: {model_name}, feature_dim={rgb_feature_dim}", 'green')
            
            # GroupNormæ›¿æ¢
            if use_group_norm and not pretrained:
                rgb_base_model = replace_submodules(
                    root_module=rgb_base_model,
                    predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                    func=lambda x: nn.GroupNorm(
                        num_groups=(x.num_features // 16) if (x.num_features % 16 == 0) else (x.num_features // 8),
                        num_channels=x.num_features)
                )
            
            # è·å–RGBå›¾åƒå°ºå¯¸å¹¶åˆ›å»ºæ•°æ®å¢å¼º
            image_shape = key_shape_map[rgb_keys[0]][1:]  # (H, W)
            if transforms is not None and not isinstance(transforms[0], torch.nn.Module):
                if hasattr(transforms[0], 'type') and transforms[0].type == 'RandomCrop':
                    ratio = transforms[0].ratio
                    transforms = [
                        torchvision.transforms.RandomCrop(size=int(image_shape[0] * ratio)),
                        torchvision.transforms.Resize(size=image_shape[0], antialias=True)
                    ] + transforms[1:]
            transform = nn.Identity() if transforms is None else nn.Sequential(*transforms)
            
            # ä¸ºæ¯ä¸ªRGBç›¸æœºåˆ†é…æ¨¡å‹å’Œå˜æ¢
            for key in rgb_keys:
                this_model = rgb_base_model if share_rgb_model else copy.deepcopy(rgb_base_model)
                rgb_model_map[key] = this_model
                rgb_transform_map[key] = transform
            
            # åˆå§‹åŒ–ç‰¹å¾èšåˆæ¨¡å—
            feature_map_shape = [x // downsample_ratio for x in image_shape]
            self._init_rgb_aggregation(feature_aggregation, rgb_feature_dim, feature_map_shape, 
                                      position_encording, model_name)
        
        # ============ è§¦è§‰ç¼–ç å™¨åˆå§‹åŒ– ============
        tactile_encoder = None
        
        if len(tactile_keys) > 0:
            # æ„é€ è§¦è§‰ä¸“ç”¨çš„shape_metaï¼ˆåªéœ€è¦obsï¼Œä¸éœ€è¦actionï¼‰
            tactile_shape_meta = {
                'obs': {k: v for k, v in obs_shape_meta.items() if k in tactile_keys}
            }
            
            tactile_encoder = TimmTactileEncoder(
                shape_meta=tactile_shape_meta,
                model_name=tactile_model_name,
                pretrained=tactile_pretrained,
                frozen=tactile_frozen,
                use_group_norm=use_group_norm,
                share_tactile_model=share_tactile_model,
                feature_dim=tactile_feature_dim,
                num_kp=tactile_num_kp,  # ğŸ†• ä¼ é€’å…³é”®ç‚¹æ•°é‡
                crop_shape=tactile_crop_shape,  # ğŸ†• ä¼ é€’è£å‰ªå°ºå¯¸
                output_all_patches=tactile_output_all_patches
            )
            
            cprint(f"âœ“ è§¦è§‰ç¼–ç å™¨: {tactile_encoder.tactile_keys}, "
                   f"ç‰¹å¾ç»´åº¦={tactile_feature_dim}, num_kp={tactile_num_kp}, "
                   f"crop={tactile_crop_shape}, å…±äº«æƒé‡={share_tactile_model}, "
                   f"è¾“å‡ºpatch tokens={tactile_output_all_patches}", 'green')
        
        # ============ è§¦è§‰æŠ•å½±å±‚ï¼ˆç”¨äºç»´åº¦å¯¹é½ï¼‰ ============
        self.left_rgb_keys = []
        self.right_rgb_keys = []
        self.left_tactile_keys = []
        self.right_tactile_keys = []
        
        # åŒºåˆ†å·¦å³æ‰‹çš„RGBç›¸æœºå’Œè§¦è§‰ä¼ æ„Ÿå™¨ï¼ˆç”¨äºtokenåºåˆ—æ¨¡å¼ï¼‰
        for key in rgb_keys:
            if 'left' in key.lower():
                self.left_rgb_keys.append(key)
            elif 'right' in key.lower():
                self.right_rgb_keys.append(key)
        
        for key in tactile_keys:
            if 'left' in key.lower():
                self.left_tactile_keys.append(key)
            elif 'right' in key.lower():
                self.right_tactile_keys.append(key)
        
        # å¦‚æœRGBå’Œè§¦è§‰ç‰¹å¾ç»´åº¦ä¸åŒï¼Œåˆ›å»ºæŠ•å½±å±‚
        if len(tactile_keys) > 0 and rgb_feature_dim is not None and tactile_feature_dim is not None:
            if rgb_feature_dim != tactile_feature_dim:
                if len(self.left_tactile_keys) > 0:
                    self.left_tactile_proj = nn.Linear(tactile_feature_dim, rgb_feature_dim)
                    cprint(f"âœ“ å·¦æ‰‹è§¦è§‰æŠ•å½±å±‚: {tactile_feature_dim} -> {rgb_feature_dim}", 'yellow')
                
                if len(self.right_tactile_keys) > 0:
                    self.right_tactile_proj = nn.Linear(tactile_feature_dim, rgb_feature_dim)
                    cprint(f"âœ“ å³æ‰‹è§¦è§‰æŠ•å½±å±‚: {tactile_feature_dim} -> {rgb_feature_dim}", 'yellow')
            else:
                cprint(f"âœ“ RGBå’Œè§¦è§‰ç‰¹å¾ç»´åº¦ç›¸åŒ({rgb_feature_dim})ï¼Œæ— éœ€æŠ•å½±", 'green')
        
        # ä¿å­˜æ‰€æœ‰å±æ€§
        self.model_name = model_name
        self.shape_meta = shape_meta
        self.rgb_keys = rgb_keys
        self.tactile_keys = tactile_keys
        self.low_dim_keys = low_dim_keys
        self.key_shape_map = key_shape_map
        
        self.rgb_model_map = rgb_model_map
        self.rgb_transform_map = rgb_transform_map
        self.rgb_feature_dim = rgb_feature_dim
        self.feature_aggregation = feature_aggregation
        
        self.tactile_encoder = tactile_encoder
        self.tactile_feature_dim = tactile_feature_dim
        self.tactile_num_kp = tactile_num_kp  # ğŸ†• ä¿å­˜æ–°å‚æ•°
        self.tactile_crop_shape = tactile_crop_shape  # ğŸ†• ä¿å­˜æ–°å‚æ•°
        
        self.share_rgb_model = share_rgb_model
        self.share_tactile_model = share_tactile_model
        
        # ğŸ†• æ¨¡æ€çº§MoEæ”¯æŒ
        self.output_token_sequence = output_token_sequence
        if output_token_sequence:
            cprint(f"âœ“ å¯ç”¨tokenåºåˆ—è¾“å‡ºæ¨¡å¼ (ç”¨äºæ¨¡æ€çº§MoE)", 'cyan')
            # åˆ›å»ºæœ¬ä½“æ„ŸçŸ¥æŠ•å½±å±‚ï¼ˆå°†ä½ç»´çŠ¶æ€æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦ï¼‰
            if len(low_dim_keys) > 0:
                total_low_dim = sum(key_shape_map[k][0] if len(key_shape_map[k]) == 1 
                                  else key_shape_map[k][-1] for k in low_dim_keys)
                self.proprio_proj = nn.Linear(total_low_dim, rgb_feature_dim)
                nn.init.xavier_uniform_(self.proprio_proj.weight)
                nn.init.zeros_(self.proprio_proj.bias)
                cprint(f"  âœ“ æœ¬ä½“æ„ŸçŸ¥æŠ•å½±å±‚: {total_low_dim} -> {rgb_feature_dim}", 'green')
        
        # ğŸ†• Headç›¸æœºç©ºé—´é™é‡‡æ ·é…ç½®
        self.head_grid_size = head_grid_size
        self.downsample_ratio = downsample_ratio
        if output_token_sequence and head_grid_size > 1:
            # åˆ›å»ºç©ºé—´é™é‡‡æ ·å±‚ (å°†ç‰¹å¾å›¾èšåˆä¸º NxN ä¸ªtokens)
            self.head_spatial_pool = nn.AdaptiveAvgPool2d((head_grid_size, head_grid_size))
            cprint(f"âœ“ Headç›¸æœºç©ºé—´é‡é‡‡æ ·: {head_grid_size}x{head_grid_size} = {head_grid_size**2} tokens per timestep", 'cyan')
        elif output_token_sequence:
            cprint(f"âœ“ Headç›¸æœºä½¿ç”¨å•tokenæ¨¡å¼ (head_grid_size=1)", 'cyan')
        
        # ğŸ”¥ æ¨¡æ€Dropé…ç½®ï¼ˆç”¨äºè®­ç»ƒæ—¶çš„æ•°æ®å¢å¼ºï¼‰
        # è®¾è®¡ç†å¿µï¼šæ¨¡æ‹ŸçœŸå®æœºå™¨äººæ“ä½œä¸­çš„ä¼ æ„Ÿå™¨å¤±æ•ˆåœºæ™¯
        self.modality_drop_config = {
            'head': 0.0,        # Headç›¸æœºdropæ¦‚ç‡ï¼ˆæ¨¡æ‹Ÿé®æŒ¡ï¼‰
            'rgb_wrist': 0.0,   # Wristç›¸æœºdropæ¦‚ç‡ï¼ˆè¿‘è·ç¦»è§†è§’ï¼‰
            'tactile': 0.0,     # Tactile dropæ¦‚ç‡ï¼ˆæ¥è§¦ä¿¡æ¯ï¼‰
            'proprio': 0.0,     # Proprio dropæ¦‚ç‡ï¼ˆæœ¬ä½“æ„ŸçŸ¥ï¼‰
        }
        cprint(f"âœ“ æ¨¡æ€Dropé…ç½®å·²åˆå§‹åŒ–ï¼ˆé»˜è®¤å…³é—­ï¼‰", 'cyan')
        
        logger.info(f"å¤šæ¨¡æ€ç¼–ç å™¨å‚æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def _init_rgb_aggregation(self, feature_aggregation, feature_dim, feature_map_shape, 
                              position_encording, model_name):
        """åˆå§‹åŒ–RGBç‰¹å¾èšåˆæ¨¡å—"""
        # ViTæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
        if model_name.startswith('vit'):
            if feature_aggregation == 'all_tokens':
                pass
            elif feature_aggregation is not None:
                logger.warn(f'ViTä½¿ç”¨CLS tokenï¼Œfeature_aggregation ({feature_aggregation})è¢«å¿½ç•¥')
                self.feature_aggregation = None
        
        # åˆ›å»ºèšåˆæ¨¡å—
        if feature_aggregation == 'soft_attention':
            self.rgb_attention = nn.Sequential(
                nn.Linear(feature_dim, 1, bias=False),
                nn.Softmax(dim=1)
            )
        elif feature_aggregation == 'spatial_embedding':
            self.rgb_spatial_embedding = nn.Parameter(
                torch.randn(feature_map_shape[0] * feature_map_shape[1], feature_dim))
        elif feature_aggregation == 'transformer':
            if position_encording == 'learnable':
                self.rgb_position_embedding = nn.Parameter(
                    torch.randn(feature_map_shape[0] * feature_map_shape[1] + 1, feature_dim))
            elif position_encording == 'sinusoidal':
                num_features = feature_map_shape[0] * feature_map_shape[1] + 1
                pos_embed = torch.zeros(num_features, feature_dim)
                position = torch.arange(0, num_features, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, feature_dim, 2).float() * 
                                    (-math.log(2 * num_features) / feature_dim))
                pos_embed[:, 0::2] = torch.sin(position * div_term)
                pos_embed[:, 1::2] = torch.cos(position * div_term)
                self.rgb_position_embedding = pos_embed
            self.rgb_aggregation_transformer = nn.TransformerEncoder(
                encoder_layer=nn.TransformerEncoderLayer(d_model=feature_dim, nhead=4),
                num_layers=4)
        elif feature_aggregation == 'attention_pool_2d':
            self.rgb_attention_pool_2d = AttentionPool2d(
                spacial_dim=feature_map_shape[0],
                embed_dim=feature_dim,
                num_heads=feature_dim // 64,
                output_dim=feature_dim
            )
    
    def aggregate_rgb_feature(self, feature):
        """èšåˆRGBç‰¹å¾"""
        if self.model_name == 'r3m':
            return feature
        
        # SigLIP/CLIPæ¨¡å‹å¤„ç†
        if 'siglip' in self.model_name.lower() or 'clip' in self.model_name.lower():
            if self.feature_aggregation == 'all_tokens':
                # ğŸ”¥ è¾“å‡ºæ‰€æœ‰tokens: (B, N, D) å…¶ä¸­ N = num_patches
                return feature
            elif self.feature_aggregation == 'avg' or self.feature_aggregation is None:
                # é»˜è®¤ä½¿ç”¨mean pooling
                return torch.mean(feature, dim=1)
            else:
                logger.warn(f'SigLIP/CLIPä½¿ç”¨mean poolingä½œä¸ºé»˜è®¤èšåˆæ–¹å¼')
                return torch.mean(feature, dim=1)
        
        # ViTæ¨¡å‹å¤„ç†
        if self.model_name.startswith('vit'):
            if self.feature_aggregation == 'all_tokens':
                # ğŸ”¥ è¾“å‡ºæ‰€æœ‰tokens: (B, 1+P, D) - CLS + patches
                return feature
            elif self.feature_aggregation is None or self.feature_aggregation == 'cls_token':
                return feature[:, 0, :]
            else:
                logger.warn(f'ViTä½¿ç”¨CLS tokenä½œä¸ºé»˜è®¤èšåˆæ–¹å¼')
                return feature[:, 0, :]
        
        # ResNetå¤„ç†
        assert len(feature.shape) == 4
        if self.feature_aggregation == 'attention_pool_2d':
            return self.rgb_attention_pool_2d(feature)
        
        feature = torch.flatten(feature, start_dim=-2)  # B, C, H*W
        feature = torch.transpose(feature, 1, 2)  # B, H*W, C
        
        if self.feature_aggregation == 'avg':
            return torch.mean(feature, dim=[1])
        elif self.feature_aggregation == 'max':
            return torch.amax(feature, dim=[1])
        elif self.feature_aggregation == 'soft_attention':
            weight = self.rgb_attention(feature)
            return torch.sum(feature * weight, dim=1)
        elif self.feature_aggregation == 'spatial_embedding':
            return torch.mean(feature * self.rgb_spatial_embedding, dim=1)
        elif self.feature_aggregation == 'transformer':
            zero_feature = torch.zeros(feature.shape[0], 1, feature.shape[-1], device=feature.device)
            if self.rgb_position_embedding.device != feature.device:
                self.rgb_position_embedding = self.rgb_position_embedding.to(feature.device)
            feature_with_pos = torch.cat([zero_feature, feature], dim=1) + self.rgb_position_embedding
            feature_output = self.rgb_aggregation_transformer(feature_with_pos)
            return feature_output[:, 0]
        else:
            # feature_aggregationä¸ºNoneæ—¶ï¼Œå±•å¹³æ‰€æœ‰ç©ºé—´ç‰¹å¾
            assert self.feature_aggregation is None
            return feature.reshape(feature.shape[0], -1)  # B, H*W*C
    
    def _extract_rgb_tokens(self, obs_dict, key, is_head_cam=False):
        """
        æå–RGBå›¾åƒçš„tokenè¡¨ç¤ºï¼ˆç”¨äºäº¤å‰æ³¨æ„åŠ›ï¼‰
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸
            key: RGBç›¸æœºçš„key
            is_head_cam: æ˜¯å¦æ˜¯Headç›¸æœºï¼ˆç”¨äºç©ºé—´é‡é‡‡æ ·ï¼‰
            
        Returns:
            tokens: (B*T, N, D) - Nä¸ºtokenæ•°é‡
                   - Headç›¸æœº (head_grid_size > 1): N = head_grid_size^2 (ç©ºé—´é‡é‡‡æ ·å)
                   - Headç›¸æœº (head_grid_size = 1): N = 1 (èšåˆåçš„å•token)
                   - Wristç›¸æœº: N = 1 (èšåˆåçš„å•token)
            batch_size: B
            time_steps: T
        """
        img = obs_dict[key]
        
        # å½’ä¸€åŒ–
        if img.max() > 1.0:
            img = img / 255.0
        
        # è°ƒæ•´ç»´åº¦é¡ºåº: (B,T,H,W,C) -> (B,T,C,H,W)
        if img.shape[-1] == 3:
            if len(img.shape) == 5:
                img = img.permute(0, 1, 4, 2, 3)
            elif len(img.shape) == 4:
                img = img.permute(0, 3, 1, 2)
        
        B, T = img.shape[:2]
        img = img.reshape(B*T, *img.shape[2:])
        
        # Resizeåˆ°æœŸæœ›å°ºå¯¸
        if img.shape[1:] != self.key_shape_map[key]:
            target_H, target_W = self.key_shape_map[key][1], self.key_shape_map[key][2]
            img = F.interpolate(img, size=(target_H, target_W), 
                               mode='bilinear', align_corners=False)
        
        # å‰å‘ä¼ æ’­è·å–åŸå§‹ç‰¹å¾ï¼ˆä¸èšåˆï¼‰
        img = self.rgb_transform_map[key](img).to(self.device)
        img = img.float()
        raw_feature = self.rgb_model_map[key](img).to(self.device)
        
        # ğŸ”¥ æ ¸å¿ƒé€»è¾‘ï¼šæ ¹æ®ç›¸æœºç±»å‹å’Œgrid_sizeå†³å®štokenç”Ÿæˆç­–ç•¥
        if is_head_cam and hasattr(self, 'head_spatial_pool') and self.head_grid_size > 1:
            # === Headç›¸æœºç©ºé—´é‡é‡‡æ ·æ¨¡å¼ (head_grid_size > 1) ===
            # å°†ç‰¹å¾è½¬æ¢ä¸º2Dç©ºé—´ç‰¹å¾å›¾ï¼Œç„¶åé‡é‡‡æ ·ä¸º NxN ä¸ªtokens
            
            if self.model_name.startswith('vit') or 'siglip' in self.model_name.lower():
                # ViT/SigLIP: è¾“å‡ºæ ¼å¼ä¸º (B*T, 1+P, D) æˆ– (B*T, P, D)
                # éœ€è¦å°†patch tokensè½¬æ¢ä¸ºç©ºé—´ç‰¹å¾å›¾
                
                feature_dim = raw_feature.shape[-1]
                
                # å»æ‰CLS tokenï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if raw_feature.shape[1] == 197:  # ViT-Base: 1 CLS + 196 patches (14x14)
                    patch_tokens = raw_feature[:, 1:, :]  # (B*T, 196, D)
                    spatial_size = int(math.sqrt(patch_tokens.shape[1]))  # 14
                elif 'siglip' in self.model_name.lower():
                    # SigLIPè¾“å‡º: (B*T, P, D) æ²¡æœ‰CLS token
                    patch_tokens = raw_feature
                    spatial_size = int(math.sqrt(patch_tokens.shape[1]))
                else:
                    # å…¶ä»–æƒ…å†µï¼šå‡è®¾æ²¡æœ‰CLS token
                    patch_tokens = raw_feature
                    spatial_size = int(math.sqrt(patch_tokens.shape[1]))
                
                # é‡å¡‘ä¸ºç©ºé—´ç‰¹å¾å›¾: (B*T, P, D) -> (B*T, D, H, W)
                spatial_feature = patch_tokens.permute(0, 2, 1).reshape(
                    B*T, feature_dim, spatial_size, spatial_size
                )
                
            else:
                # CNN: å·²ç»æ˜¯ç©ºé—´ç‰¹å¾å›¾æ ¼å¼ (B*T, C, H, W)
                spatial_feature = raw_feature
            
            # ğŸ”¥ ç©ºé—´é‡é‡‡æ ·: (B*T, D, H, W) -> (B*T, D, N, N)
            downsampled = self.head_spatial_pool(spatial_feature)  # (B*T, D, grid_size, grid_size)
            
            # æ‹‰å¹³ä¸ºtokens: (B*T, D, N, N) -> (B*T, N^2, D)
            tokens = downsampled.flatten(2).permute(0, 2, 1)  # (B*T, grid_size^2, D)
            
        else:
            # === å•tokenæ¨¡å¼ï¼ˆWristç›¸æœº æˆ– Headç›¸æœºgrid_size=1ï¼‰ ===
            # ä½¿ç”¨mean poolingèšåˆä¸ºå•ä¸ªtoken
            
            if self.model_name.startswith('vit') or 'siglip' in self.model_name.lower():
                # ViT/SigLIP: ç›´æ¥å¯¹æ‰€æœ‰tokensåšmean pooling
                tokens = torch.mean(raw_feature, dim=1, keepdim=True)  # (B*T, 1, D)
            else:
                # CNN: å…ˆå±•å¹³ç©ºé—´ç»´åº¦ï¼Œå†mean pooling
                tokens = torch.flatten(raw_feature, start_dim=-2)  # (B*T, C, H*W)
                tokens = torch.transpose(tokens, 1, 2)  # (B*T, H*W, C)
                tokens = torch.mean(tokens, dim=1, keepdim=True)  # (B*T, 1, C)
        
        return tokens, B, T
    
    def forward(self, obs_dict):
        """
        å‰å‘ä¼ æ’­ï¼Œç»Ÿä¸€å¤„ç†æ‰€æœ‰æ¨¡æ€
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸ï¼Œæ¯ä¸ªkeyå¯¹åº”(B, T, ...)çš„å¼ é‡
            
        Returns:
            features: æ‹¼æ¥åçš„ç‰¹å¾å‘é‡ (B, D_total) æˆ– tokenåºåˆ— (B, L_tokens, D)
        """
        batch_size = next(iter(obs_dict.values())).shape[0]
        
        # ğŸ†• Tokenåºåˆ—æ¨¡å¼ï¼ˆç”¨äºæ¨¡æ€çº§MoEï¼‰
        if self.output_token_sequence:
            return self._forward_token_sequence(obs_dict, batch_size)
        
        # åŸå§‹æ¨¡å¼ï¼šæ‹¼æ¥æ‰€æœ‰ç‰¹å¾ä¸ºä¸€ä¸ªå‘é‡
        features = []
        
        # ============ å¤„ç†RGBå›¾åƒ ============
        for key in self.rgb_keys:
            img = obs_dict[key]
            
            # å½’ä¸€åŒ–
            if img.max() > 1.0:
                img = img / 255.0
            
            # è°ƒæ•´ç»´åº¦é¡ºåº: (B,T,H,W,C) -> (B,T,C,H,W)
            if img.shape[-1] == 3:
                if len(img.shape) == 5:
                    img = img.permute(0, 1, 4, 2, 3)
                elif len(img.shape) == 4:
                    img = img.permute(0, 3, 1, 2)
            
            B, T = img.shape[:2]
            assert B == batch_size
            img = img.reshape(B*T, *img.shape[2:])
            
            # Resizeåˆ°æœŸæœ›å°ºå¯¸
            if img.shape[1:] != self.key_shape_map[key]:
                target_H, target_W = self.key_shape_map[key][1], self.key_shape_map[key][2]
                img = F.interpolate(img, size=(target_H, target_W), 
                                   mode='bilinear', align_corners=False)
            
            # å‰å‘ä¼ æ’­
            img = self.rgb_transform_map[key](img).to(self.device)
            img = img.float()
            raw_feature = self.rgb_model_map[key](img).to(self.device)
            feature = self.aggregate_rgb_feature(raw_feature)
            
            assert len(feature.shape) == 2 and feature.shape[0] == B * T
            features.append(feature.reshape(B, -1))
        
        # ============ å¤„ç†è§¦è§‰ä¼ æ„Ÿå™¨ ============
        if self.tactile_encoder is not None and len(self.tactile_keys) > 0:
            tactile_obs = {k: obs_dict[k] for k in self.tactile_keys if k in obs_dict}
            tactile_features = self.tactile_encoder(tactile_obs)  # Dict[key, (B, T, D)] or (B, T*H*W, D)
            
            for key in self.tactile_keys:
                if key in tactile_features:
                    feat = tactile_features[key]  # (B, T, D) or (B, T*H*W, D)
                    features.append(feat.reshape(batch_size, -1))  # (B, T*D) or (B, T*H*W*D)
        
        # ============ å¤„ç†ä½ç»´çŠ¶æ€ ============
        for key in self.low_dim_keys:
            data = obs_dict[key].to(self.device)
            B, T = data.shape[:2]
            assert B == batch_size
            assert data.shape[2:] == self.key_shape_map[key]
            features.append(data.reshape(B, -1))
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        result = torch.cat(features, dim=-1)
        
        return result
    
    def _forward_token_sequence(self, obs_dict, batch_size):
        """
        ğŸ†• è¾“å‡ºtokenåºåˆ—æ ¼å¼: (B, L_tokens, D)
        
        æ¨¡æ€ç»„ç»‡ç­–ç•¥ï¼ˆå„æ¨¡æ€ç‹¬ç«‹è¾“å‡ºï¼Œæ”¯æŒæ¨¡æ€Dropï¼‰:
        - head: head_cam tokens (å¯è¢«Dropï¼Œæ¨¡æ‹Ÿé®æŒ¡åœºæ™¯)
        - rgb_wrist: left_wrist_cam + right_wrist_cam tokens (ä½Dropç‡ï¼Œè¿‘è·ç¦»è§†è§’é‡è¦)
        - tactile: left_tactile + right_tactile tokens (æä½Dropç‡ï¼Œæ¥è§¦ä¿¡æ¯å…³é”®)
        - proprio: agent_pos tokens (å¯è¢«Dropï¼Œå¼ºåˆ¶å­¦ä¹ è§†è§‰-è§¦è§‰ä¾èµ–)
        
        Dropç­–ç•¥è®¾è®¡ç†å¿µ:
        - Headç›¸æœº: åœ¨å®é™…æ“ä½œä¸­å®¹æ˜“è¢«æœºæ¢°è‡‚é®æŒ¡ â†’ é«˜Dropç‡æ¨¡æ‹Ÿé®æŒ¡
        - Wristç›¸æœº: è¿‘è·ç¦»è§†è§’ï¼Œä¸æ˜“é®æŒ¡ â†’ ä½Dropç‡
        - Tactile: æ¥è§¦æ—¶çš„å…³é”®ä¿¡æ¯ â†’ æä½/ä¸Drop
        - Proprio: å¯èƒ½å­˜åœ¨ç¼–ç å™¨å™ªå£° â†’ ä¸­ç­‰Dropç‡
        
        Args:
            obs_dict: è§‚æµ‹å­—å…¸
            batch_size: batchå¤§å°
            
        Returns:
            result: (B, L_tokens, D) tokenåºåˆ—
        """
        head_tokens_list = []
        rgb_wrist_tokens_list = []  # ğŸ”¥ RGBç›¸æœºtokensï¼ˆåˆ†ç¦»å‡ºæ¥ï¼‰
        tactile_tokens_list = []     # ğŸ”¥ è§¦è§‰tokensï¼ˆåˆ†ç¦»å‡ºæ¥ï¼‰
        proprio_features_list = []
        
        # è·å–æ—¶é—´æ­¥æ•°ï¼ˆä»ä»»æ„è§‚æµ‹ä¸­è·å–ï¼‰
        time_steps = next(iter(obs_dict.values())).shape[1]
        
        # ============ å¤„ç†RGBå›¾åƒ ============
        for key in self.rgb_keys:
            is_head_cam = 'head' in key.lower() or 'front' in key.lower()
            
            tokens, B, T = self._extract_rgb_tokens(obs_dict, key, is_head_cam)
            
            # ğŸ”¥ tokenså·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼š
            # - Headç›¸æœº (grid_size > 1): (B*T, grid_size^2, D)
            # - Headç›¸æœº (grid_size = 1): (B*T, 1, D)
            # - Wristç›¸æœº: (B*T, 1, D)
            
            # é‡å¡‘ä¸º (B, T*num_tokens, D)
            num_tokens_per_timestep = tokens.shape[1]  # grid_size^2 æˆ– 1
            token_seq = tokens.reshape(B, T * num_tokens_per_timestep, -1)  # (B, T*N, D)
            
            if is_head_cam:
                head_tokens_list.append(token_seq)
            else:
                # ğŸ”¥ åˆ†ç¦»RGB wristç›¸æœº
                rgb_wrist_tokens_list.append(token_seq)
        
        # ============ å¤„ç†è§¦è§‰ä¼ æ„Ÿå™¨ ============
        if self.tactile_encoder is not None and len(self.tactile_keys) > 0:
            tactile_obs = {k: obs_dict[k] for k in self.tactile_keys if k in obs_dict}
            tactile_features_dict = self.tactile_encoder(tactile_obs)
            
            for key in self.tactile_keys:
                if key in tactile_features_dict:
                    tact_tok = tactile_features_dict[key]  # (B, Q, D) - Qå¯ä»¥æ˜¯Tæˆ–T*H*W
                    
                    # æŠ•å½±åˆ°RGBç‰¹å¾ç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if 'left' in key.lower() and hasattr(self, 'left_tactile_proj'):
                        tact_tok = self.left_tactile_proj(tact_tok)
                    elif 'right' in key.lower() and hasattr(self, 'right_tactile_proj'):
                        tact_tok = self.right_tactile_proj(tact_tok)
                    
                    # è§¦è§‰ç¼–ç å™¨ä¿ç•™æ—¶åºç»´åº¦ï¼Œç›´æ¥ä½¿ç”¨
                    # output_all_patches=True: (B, T*H*W, D) - ä¿ç•™æ‰€æœ‰æ—¶é—´æ­¥çš„æ‰€æœ‰patch
                    # output_all_patches=False: (B, T, D) - ä¿ç•™æ‰€æœ‰æ—¶é—´æ­¥ï¼Œæ¯ä¸ªæ—¶é—´æ­¥1ä¸ªtoken
                    tactile_tokens_list.append(tact_tok)
        
        # ============ å¤„ç†ä½ç»´çŠ¶æ€ï¼ˆæœ¬ä½“æ„ŸçŸ¥ï¼‰ ============
        for key in self.low_dim_keys:
            data = obs_dict[key].to(self.device)
            B, T = data.shape[:2]
            assert B == batch_size
            proprio_features_list.append(data)  # (B, T, low_dim)
        
        # ============ ç»„è£…æœ€ç»ˆçš„tokenåºåˆ—ï¼ˆåº”ç”¨æ¨¡æ€Dropï¼‰ ============
        all_tokens = []
        modality_info = {'head': 0, 'rgb_wrist': 0, 'tactile': 0, 'proprio': 0}
        
        # ğŸ”¥ Head tokens (é«˜Dropç‡ - æ¨¡æ‹Ÿè¢«æœºæ¢°è‡‚é®æŒ¡çš„åœºæ™¯)
        # å®é™…æ“ä½œä¸­ï¼Œå¤´éƒ¨ç›¸æœºç»å¸¸è¢«æœºæ¢°è‡‚é®æŒ¡ï¼Œæ¨¡å‹éœ€è¦å­¦ä¼šåœ¨é®æŒ¡æ—¶ä¾èµ–è…•éƒ¨ç›¸æœºå’Œè§¦è§‰
        if head_tokens_list:
            head_tokens = torch.cat(head_tokens_list, dim=1)  # (B, n_head_cams*T, D)
            head_tokens = self._apply_modality_drop(head_tokens, 'head')
            all_tokens.append(head_tokens)
            modality_info['head'] = head_tokens.shape[1]
        
        # ğŸ”¥ RGB Wrist tokens (ä½Dropç‡ - è¿‘è·ç¦»è§†è§’ï¼Œä¸æ˜“é®æŒ¡)
        # è…•éƒ¨ç›¸æœºè´´è¿‘æ“ä½œç‰©ä½“ï¼Œæä¾›å…³é”®çš„è¿‘è·ç¦»è§†è§‰ä¿¡æ¯
        if rgb_wrist_tokens_list:
            rgb_wrist_tokens = torch.cat(rgb_wrist_tokens_list, dim=1)  # (B, n_wrist_cams*T, D)
            rgb_wrist_tokens = self._apply_modality_drop(rgb_wrist_tokens, 'rgb_wrist')
            all_tokens.append(rgb_wrist_tokens)
            modality_info['rgb_wrist'] = rgb_wrist_tokens.shape[1]
        
        # ğŸ”¥ Tactile tokens (æä½Dropç‡ - æ¥è§¦ä¿¡æ¯æ˜¯å…œåº•ä¿éšœ)
        # è§¦è§‰ä¼ æ„Ÿå™¨æä¾›ç›´æ¥æ¥è§¦ä¿¡æ¯ï¼Œæ˜¯è§†è§‰å¤±æ•ˆæ—¶çš„å…³é”®ä¿¡æ¯æº
        if tactile_tokens_list:
            tactile_tokens = torch.cat(tactile_tokens_list, dim=1)  # (B, n_tactile*T, D)
            tactile_tokens = self._apply_modality_drop(tactile_tokens, 'tactile')
            all_tokens.append(tactile_tokens)
            modality_info['tactile'] = tactile_tokens.shape[1]
        
        # ğŸ”¥ Proprio tokens (ä¸­ç­‰Dropç‡ - å¼ºåˆ¶å­¦ä¹ è§†è§‰-è§¦è§‰ä¾èµ–)
        # æœ¬ä½“æ„ŸçŸ¥å¯èƒ½å­˜åœ¨ç¼–ç å™¨å™ªå£°ï¼ŒDropåå¼ºåˆ¶æ¨¡å‹ä»è§†è§‰å’Œè§¦è§‰æ¨æ–­ä½ç½®
        if proprio_features_list:
            proprio_concat = torch.cat(proprio_features_list, dim=-1)  # (B, T, total_low_dim)
            proprio_tokens = self.proprio_proj(proprio_concat.float())  # (B, T, D)
            proprio_tokens = self._apply_modality_drop(proprio_tokens, 'proprio')
            all_tokens.append(proprio_tokens)
            modality_info['proprio'] = proprio_tokens.shape[1]
        
        result = torch.cat(all_tokens, dim=1)  # (B, L_total, D)
        
        # ä¿å­˜æ¨¡æ€ä¿¡æ¯ä¾›å¤–éƒ¨ä½¿ç”¨
        self._last_modality_info = modality_info
        
        return result
    
    def get_modality_info(self):
        """
        ğŸ†• è·å–æœ€è¿‘ä¸€æ¬¡forwardçš„æ¨¡æ€é•¿åº¦ä¿¡æ¯
        
        Returns:
            modality_info: dict {'head': L_head, 'wrist': L_wrist, 'proprio': L_proprio}
        """
        return getattr(self, '_last_modality_info', None)
    
    def set_modality_drop_config(self, head_drop=0.0, rgb_wrist_drop=0.0, tactile_drop=0.0, proprio_drop=0.0):
        """
        ğŸ”¥ è®¾ç½®æ¨¡æ€Dropæ¦‚ç‡
        
        è®¾è®¡ç†å¿µï¼šæ¨¡æ‹ŸçœŸå®æœºå™¨äººæ“ä½œä¸­çš„ä¼ æ„Ÿå™¨å¤±æ•ˆåœºæ™¯
        - Headç›¸æœº: å®¹æ˜“è¢«æœºæ¢°è‡‚é®æŒ¡ â†’ å»ºè®®é«˜Dropç‡ (0.2-0.3)
        - Wristç›¸æœº: è¿‘è·ç¦»è§†è§’ï¼Œä¸æ˜“é®æŒ¡ â†’ å»ºè®®ä½Dropç‡ (0.05-0.1)
        - Tactile: æ¥è§¦ä¿¡æ¯å…³é”® â†’ å»ºè®®æä½/ä¸Drop (0.0-0.02)
        - Proprio: å¯èƒ½æœ‰ç¼–ç å™¨å™ªå£° â†’ å»ºè®®ä¸­ç­‰Dropç‡ (0.1-0.15)
        
        Args:
            head_drop: Headç›¸æœºçš„dropæ¦‚ç‡ (0.0-1.0)
            rgb_wrist_drop: Wristç›¸æœºçš„dropæ¦‚ç‡ (0.0-1.0)
            tactile_drop: Tactileä¼ æ„Ÿå™¨çš„dropæ¦‚ç‡ (0.0-1.0)
            proprio_drop: Proprioçš„dropæ¦‚ç‡ (0.0-1.0)
        """
        self.modality_drop_config = {
            'head': head_drop,
            'rgb_wrist': rgb_wrist_drop,
            'tactile': tactile_drop,
            'proprio': proprio_drop,
        }
        cprint(f"âœ“ æ¨¡æ€Dropé…ç½®å·²æ›´æ–°: head={head_drop}, rgb_wrist={rgb_wrist_drop}, "
               f"tactile={tactile_drop}, proprio={proprio_drop}", 'yellow')
    
    def _apply_modality_drop(self, tokens, modality_name):
        """
        ğŸ”¥ å¯¹æŒ‡å®šæ¨¡æ€çš„tokensåº”ç”¨Dropï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
        
        æœºåˆ¶è¯´æ˜:
        - è®­ç»ƒæ—¶: ä»¥æŒ‡å®šæ¦‚ç‡å°†æ•´ä¸ªbatchä¸­çš„æŸäº›æ ·æœ¬çš„è¯¥æ¨¡æ€tokenså…¨éƒ¨ç½®é›¶
        - æ¨ç†æ—¶: è‡ªåŠ¨å…³é—­ï¼ˆé€šè¿‡self.trainingåˆ¤æ–­ï¼‰
        - æ¢¯åº¦æµåŠ¨: ç½®é›¶çš„tokensä»ç„¶å‚ä¸åå‘ä¼ æ’­ï¼Œæ¢¯åº¦ä¼šæµå‘æœªè¢«Dropçš„æ¨¡æ€
        
        Args:
            tokens: (B, L, D) æ¨¡æ€çš„tokenåºåˆ—
            modality_name: 'proprio', 'rgb', 'tactile'
        
        Returns:
            tokens: Dropåçš„tokenåºåˆ—ï¼ˆæ¨ç†æ—¶è¿”å›åŸå§‹tokensï¼‰
        """
        # ğŸ”¥ å…³é”®ï¼šåªåœ¨è®­ç»ƒæ¨¡å¼ä¸‹æ‰Drop
        if not self.training:
            return tokens
        
        drop_prob = self.modality_drop_config.get(modality_name, 0.0)
        if drop_prob == 0.0:
            return tokens
        
        # ç”ŸæˆDrop maskï¼ˆæ¯ä¸ªbatchæ ·æœ¬ç‹¬ç«‹å†³å®šæ˜¯å¦Dropï¼‰
        B = tokens.shape[0]
        # drop_mask: (B, 1, 1) - å¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹é‡‡æ ·ï¼Œå¹¿æ’­åˆ°æ‰€æœ‰tokenså’Œç‰¹å¾ç»´åº¦
        drop_mask = torch.bernoulli(
            torch.full((B, 1, 1), 1.0 - drop_prob, device=tokens.device, dtype=tokens.dtype)
        )
        
        # åº”ç”¨maskï¼ˆç½®é›¶ä½†ä¿ç•™æ¢¯åº¦æµåŠ¨ï¼‰
        tokens_dropped = tokens * drop_mask
        
        # ç»Ÿè®¡Dropä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if not hasattr(self, '_drop_stats'):
            self._drop_stats = {}
        dropped_samples = (drop_mask.squeeze() == 0).sum().item()
        self._drop_stats[modality_name] = {
            'dropped_samples': dropped_samples,
            'total_samples': B,
            'drop_rate': dropped_samples / B if B > 0 else 0.0
        }
        
        return tokens_dropped
    
    def get_drop_stats(self):
        """
        è·å–æœ€è¿‘ä¸€æ¬¡forwardçš„Dropç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            drop_stats: dict {modality_name: {'dropped_samples': int, 'total_samples': int, 'drop_rate': float}}
        """
        stats = getattr(self, '_drop_stats', {})
        # æ¸…ç©ºç»Ÿè®¡ä¿¡æ¯
        self._drop_stats = {}
        return stats if stats else None
    
    @torch.no_grad()
    def output_shape(self):
        """è®¡ç®—è¾“å‡ºç‰¹å¾çš„å½¢çŠ¶"""
        example_obs_dict = {}
        obs_shape_meta = self.shape_meta['obs']
        for key, attr in obs_shape_meta.items():
            shape = tuple(attr['shape'])
            this_obs = torch.zeros(
                (1, attr['horizon']) + shape,
                dtype=self.dtype,
                device=self.device)
            example_obs_dict[key] = this_obs
        
        example_output = self.forward(example_obs_dict)
        
        # ğŸ†• æ”¯æŒtokenåºåˆ—æ¨¡å¼
        if self.output_token_sequence:
            assert len(example_output.shape) == 3  # (B, L, D)
            assert example_output.shape[0] == 1
        else:
            assert len(example_output.shape) == 2  # (B, total_dim)
            assert example_output.shape[0] == 1
        
        return example_output.shape


if __name__ == '__main__':
    print("\n" + "="*80)
    cprint("TimmMultimodalEncoder æµ‹è¯• (ä½¿ç”¨TimmTactileEncoder)", "cyan", attrs=["bold"])
    print("="*80 + "\n")
    
    # æ„é€ shape_meta (RoboTwin2.0ç¯å¢ƒ)
    shape_meta = {
        'obs': {
            'head_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'left_wrist_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'right_wrist_cam': {'shape': [3, 224, 224], 'type': 'rgb', 'horizon': 2},
            'left_tactile': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
            'right_tactile': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
            'agent_pos': {'shape': [14], 'type': 'low_dim', 'horizon': 2},
        },
        'action': {'shape': [14], 'horizon': 16}
    }
    
    # åˆ›å»ºç¼–ç å™¨ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰
    encoder = TimmMultimodalEncoder(
        shape_meta=shape_meta,
        model_name='resnet18',
        pretrained=False,
        frozen=False,
        global_pool='',
        transforms=None,
        use_group_norm=True,
        share_rgb_model=False,
        feature_aggregation=None,
        downsample_ratio=32,
        tactile_model_name='resnet18',
        tactile_pretrained=False,
        tactile_feature_dim=64,  # ğŸ”¥ ä¸robomimicä¸€è‡´
        tactile_num_kp=32,       # ğŸ†• å…³é”®ç‚¹æ•°é‡
        tactile_crop_shape=None, # ğŸ†• ä¸è£å‰ª
        share_tactile_model=True,
    )
    
    # åˆ›å»ºtokenåºåˆ—è¾“å‡ºæ¨¡å¼çš„ç¼–ç å™¨
    cprint("\n" + "="*80, "cyan")
    cprint("æµ‹è¯•tokenåºåˆ—è¾“å‡ºæ¨¡å¼ï¼ˆç”¨äºæ¨¡æ€çº§MoEï¼‰", "cyan", attrs=["bold"])
    cprint("="*80, "cyan")
    
    encoder_token_seq = TimmMultimodalEncoder(
        shape_meta=shape_meta,
        model_name='resnet18',
        pretrained=False,
        frozen=False,
        global_pool='',
        transforms=None,
        use_group_norm=True,
        share_rgb_model=False,
        feature_aggregation='all_tokens',
        downsample_ratio=32,
        tactile_model_name='resnet18',
        tactile_pretrained=False,
        tactile_feature_dim=512,  # ä¸RGBç‰¹å¾ç»´åº¦å¯¹é½
        tactile_num_kp=32,        # ğŸ†• å…³é”®ç‚¹æ•°é‡
        tactile_crop_shape=None,  # ğŸ†• ä¸è£å‰ª
        share_tactile_model=True,
        tactile_output_all_patches=True,
        output_token_sequence=True,
    )
    
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  - RGBç›¸æœº: {encoder.rgb_keys}")
    print(f"  - è§¦è§‰ä¼ æ„Ÿå™¨: {encoder.tactile_keys}")
    print(f"  - ä½ç»´çŠ¶æ€: {encoder.low_dim_keys}")
    print(f"  - RGBç‰¹å¾ç»´åº¦: {encoder.rgb_feature_dim}")
    print(f"  - è§¦è§‰ç‰¹å¾ç»´åº¦: {encoder.tactile_feature_dim}")
    print(f"  - æ€»å‚æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size, time_steps = 2, 2
    obs = {
        'head_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'left_wrist_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'right_wrist_cam': torch.randn(batch_size, time_steps, 3, 224, 224),
        'left_tactile': torch.randn(batch_size, time_steps, 1, 16, 32),
        'right_tactile': torch.randn(batch_size, time_steps, 1, 16, 32),
        'agent_pos': torch.randn(batch_size, time_steps, 14),
    }
    
    cprint("\nå‰å‘ä¼ æ’­æµ‹è¯•:", "yellow")
    with torch.no_grad():
        output = encoder(obs)
    
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # ç»´åº¦éªŒè¯ (æ³¨æ„: TimmTactileEncoderä¿ç•™æ—¶åºç»´åº¦)
    # æ ‡å‡†æ¨¡å¼ä¸‹tactile_feature_dim=64
    tactile_feat_dim = encoder.tactile_feature_dim
    rgb_dim = 3 * 512 * 7 * 7 * time_steps  # 3ç›¸æœº Ã— 512ç‰¹å¾ Ã— 7Ã—7 Ã— 2T
    tactile_dim = 2 * tactile_feat_dim * time_steps  # 2ä¼ æ„Ÿå™¨ Ã— feature_dim Ã— 2T
    lowdim_dim = 14 * time_steps  # 14ç»´ Ã— 2T
    expected_dim = rgb_dim + tactile_dim + lowdim_dim
    
    print(f"  é¢„æœŸç»´åº¦: {expected_dim} (RGB:{rgb_dim} + è§¦è§‰:{tactile_dim} + ä½ç»´:{lowdim_dim})")
    assert output.shape == (batch_size, expected_dim), f"ç»´åº¦ä¸åŒ¹é…! {output.shape} != ({batch_size}, {expected_dim})"
    
    # æ¢¯åº¦æµ‹è¯•
    cprint("\næ¢¯åº¦åå‘ä¼ æ’­æµ‹è¯•:", "yellow")
    obs_grad = {k: v.clone().requires_grad_(True) for k, v in obs.items()}
    output = encoder(obs_grad)
    loss = output.sum()
    loss.backward()
    
    print(f"  left_tactile æ¢¯åº¦èŒƒæ•°: {obs_grad['left_tactile'].grad.norm().item():.6f}")
    print(f"  head_cam æ¢¯åº¦èŒƒæ•°: {obs_grad['head_cam'].grad.norm().item():.6f}")
    
    print("\n" + "="*80)
    cprint("âœ… æ ‡å‡†ç‰ˆæœ¬æµ‹è¯•é€šè¿‡!", "green", attrs=["bold"])
    print("="*80 + "\n")
    
    # æµ‹è¯•tokenåºåˆ—è¾“å‡ºç‰ˆæœ¬
    cprint("\nå‰å‘ä¼ æ’­æµ‹è¯•ï¼ˆtokenåºåˆ—è¾“å‡ºï¼‰:", "yellow")
    with torch.no_grad():
        output_token_seq = encoder_token_seq(obs)
    
    print(f"  è¾“å‡ºå½¢çŠ¶: {output_token_seq.shape}")
    modality_info = encoder_token_seq.get_modality_info()
    if modality_info:
        print(f"  æ¨¡æ€ä¿¡æ¯: {modality_info}")
    
    # æ¢¯åº¦æµ‹è¯•
    cprint("\næ¢¯åº¦åå‘ä¼ æ’­æµ‹è¯•ï¼ˆtokenåºåˆ—è¾“å‡ºï¼‰:", "yellow")
    obs_grad2 = {k: v.clone().requires_grad_(True) for k, v in obs.items()}
    output2 = encoder_token_seq(obs_grad2)
    loss2 = output2.sum()
    loss2.backward()
    
    print(f"  left_tactile æ¢¯åº¦èŒƒæ•°: {obs_grad2['left_tactile'].grad.norm().item():.6f}")
    print(f"  left_wrist_cam æ¢¯åº¦èŒƒæ•°: {obs_grad2['left_wrist_cam'].grad.norm().item():.6f}")
    
    print("\n" + "="*80)
    cprint("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! Tokenåºåˆ—è¾“å‡ºåŠŸèƒ½æ­£å¸¸", "green", attrs=["bold"])
    print("="*80 + "\n")

