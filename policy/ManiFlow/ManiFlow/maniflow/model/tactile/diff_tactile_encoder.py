# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# å·®åˆ†æ„ŸçŸ¥è§¦è§‰ç¼–ç å™¨ - ä¸“æ³¨äºè§¦è§‰ä¿¡å·å˜åŒ–æ£€æµ‹
# 
# ä¸»è¦ç»„ä»¶:
# - SharpKeypointSpatialSoftmax: å¸¦ç¡¬é˜ˆå€¼é—¨æ§å’Œé”åˆ©æ¸©åº¦çš„å…³é”®ç‚¹æå–
# - KeypointSpatialSoftmax: åŸºç¡€ç‰ˆå…³é”®ç‚¹æå–
# - CompositeTactileEncoder: å¤åˆè§¦è§‰ç¼–ç å™¨ï¼ˆå…¨å±€+å…³é”®ç‚¹+åæ ‡ï¼‰
# - DiffAwareCompositeTactileEncoder: å·®åˆ†æ„ŸçŸ¥ç¼–ç å™¨ï¼ˆåŒæµæ¶æ„ï¼‰
# --------------------------------------------------------
from typing import Dict, Tuple, Optional
import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from maniflow.common.pytorch_util import replace_submodules
from maniflow.model.tactile.base_sensor import BaseSensoryEncoder


class SharpKeypointSpatialSoftmax(nn.Module):
    """
    ğŸ”¥ å¢å¼ºç‰ˆ SpatialSoftmaxï¼šå¸¦ç¡¬é˜ˆå€¼é—¨æ§å’Œé”åˆ©æ¸©åº¦æ§åˆ¶
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ç¡¬é˜ˆå€¼è¿‡æ»¤ï¼šSoftmax ä¹‹å‰å°†ä½äºå™ªå£°æ°´å¹³çš„åŒºåŸŸå¼ºåˆ¶è®¾ä¸º -inf
    2. å­¦ä¹ åŒ–æ¸©åº¦ï¼šè¶‹å‘äºè¾ƒå°å€¼ï¼ˆå¦‚ 0.1ï¼‰ï¼Œå¼ºåˆ¶åªæå–æœ€çªå‡ºçš„ç‚¹
    3. ä½¿ç”¨å¯å­¦ä¹ çš„ attention heads æå– K ä¸ªå…³é”®ç‚¹åæ ‡
    """
    
    def __init__(self, 
                 in_channels: int, 
                 num_keypoints: int = 4, 
                 init_temperature: float = 0.1,
                 noise_threshold: float = 0.1,
                 learnable_temperature: bool = True,
                 learnable_threshold: bool = True):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.in_channels = in_channels
        
        # å¯å­¦ä¹ çš„ attention heads
        self.attention_conv = nn.Conv2d(in_channels, num_keypoints, kernel_size=1)
        
        # å¯å­¦ä¹ æ¸©åº¦å‚æ•°ï¼ˆé€šè¿‡ softplus ç¡®ä¿ä¸ºæ­£ï¼Œè¶‹å‘å°å€¼å¢åŠ é”åº¦ï¼‰
        if learnable_temperature:
            init_temp_log = torch.log(torch.exp(torch.tensor(init_temperature)) - 1 + 1e-8)
            self.temperature_raw = nn.Parameter(init_temp_log.clone().detach())
        else:
            self.register_buffer('temperature_raw', torch.tensor(init_temperature))
        self.learnable_temperature = learnable_temperature
        
        # å¯å­¦ä¹ å™ªå£°é˜ˆå€¼ï¼ˆé€šè¿‡ sigmoid æ˜ å°„åˆ° [0, 1]ï¼‰
        if learnable_threshold:
            init_thresh_logit = torch.log(torch.tensor(noise_threshold / (1 - noise_threshold + 1e-8)))
            self.threshold_raw = nn.Parameter(init_thresh_logit.clone().detach())
        else:
            self.register_buffer('threshold_raw', torch.tensor(noise_threshold))
        self.learnable_threshold = learnable_threshold
        
        # åæ ‡ç½‘æ ¼ç¼“å­˜
        self._coord_cache = {}
    
    @property
    def temperature(self) -> torch.Tensor:
        """è·å–å½“å‰æ¸©åº¦å€¼ï¼ˆé€šè¿‡ softplus ç¡®ä¿ä¸ºæ­£ï¼‰"""
        if self.learnable_temperature:
            return F.softplus(self.temperature_raw).clamp(min=0.01, max=2.0)
        return self.temperature_raw
    
    @property
    def noise_threshold(self) -> torch.Tensor:
        """è·å–å½“å‰å™ªå£°é˜ˆå€¼ï¼ˆé€šè¿‡ sigmoid æ˜ å°„åˆ° [0, 1]ï¼‰"""
        if self.learnable_threshold:
            return torch.sigmoid(self.threshold_raw).clamp(min=0.01, max=0.5)
        return self.threshold_raw
    
    def _get_coord_grid(self, H: int, W: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–å½’ä¸€åŒ–åæ ‡ç½‘æ ¼ [-1, 1]"""
        cache_key = (H, W, device)
        if cache_key not in self._coord_cache:
            pos_x = torch.linspace(-1, 1, W, device=device)
            pos_y = torch.linspace(-1, 1, H, device=device)
            grid_x, grid_y = torch.meshgrid(pos_x, pos_y, indexing='xy')
            self._coord_cache[cache_key] = (grid_x.reshape(1, 1, H * W), grid_y.reshape(1, 1, H * W))
        return self._coord_cache[cache_key]
    
    def forward(self, feature_map: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è¾“å…¥: feature_map (B, C, H, W)
        è¾“å‡º:
            - keypoint_coords: (B, K, 2)
            - keypoint_features: (B, K, C)
            - attention_weights: (B, K, H, W)
        """
        B, C, H, W = feature_map.shape
        K = self.num_keypoints
        
        # 1. è®¡ç®—æ³¨æ„åŠ› logits
        attention_logits = self.attention_conv(feature_map)
        attention_flat = attention_logits.reshape(B, K, H * W)
        
        # 2. ç¡¬é˜ˆå€¼é—¨æ§ï¼šå°†ä½äºé˜ˆå€¼çš„åŒºåŸŸè®¾ä¸º -inf
        attn_min = attention_flat.min(dim=-1, keepdim=True)[0]
        attn_max = attention_flat.max(dim=-1, keepdim=True)[0]
        attn_normalized = (attention_flat - attn_min) / (attn_max - attn_min + 1e-8)
        
        noise_mask = attn_normalized < self.noise_threshold
        attention_masked = attention_flat.clone()
        attention_masked[noise_mask] = attention_masked[noise_mask] - 1e4
        
        # 3. ä½¿ç”¨é”åˆ©æ¸©åº¦çš„ Softmax
        attention_weights = F.softmax(attention_masked / self.temperature, dim=-1)
        
        # 4. è·å–åæ ‡ç½‘æ ¼
        pos_x, pos_y = self._get_coord_grid(H, W, feature_map.device)
        
        # 5. è®¡ç®—å…³é”®ç‚¹æœŸæœ›åæ ‡
        expected_x = (attention_weights * pos_x).sum(dim=-1)
        expected_y = (attention_weights * pos_y).sum(dim=-1)
        keypoint_coords = torch.stack([expected_x, expected_y], dim=-1)
        
        # 6. ä»ç‰¹å¾å›¾æå–å…³é”®ç‚¹ç‰¹å¾
        grid = keypoint_coords.unsqueeze(2)
        keypoint_features = F.grid_sample(
            feature_map, grid,
            mode='bilinear',
            padding_mode='border',
            align_corners=True
        )
        keypoint_features = keypoint_features.squeeze(-1).transpose(1, 2)
        
        # 7. é‡å¡‘ attention_weights
        attention_weights_2d = attention_weights.reshape(B, K, H, W)
        
        return keypoint_coords, keypoint_features, attention_weights_2d
    
    def get_sharpness_stats(self) -> Dict[str, float]:
        """è¿”å›å½“å‰é”åº¦å‚æ•°ç»Ÿè®¡ï¼ˆç”¨äºç›‘æ§ï¼‰"""
        return {
            'temperature': self.temperature.item(),
            'noise_threshold': self.noise_threshold.item() if self.learnable_threshold else float(self.threshold_raw),
        }


class KeypointSpatialSoftmax(nn.Module):
    """
    åŸºç¡€ç‰ˆ SpatialSoftmaxï¼šæå–å¤šä¸ªå…³é”®å—åŠ›ç‚¹çš„åæ ‡å’Œå¯¹åº”ç‰¹å¾
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ä½¿ç”¨å¯å­¦ä¹ çš„ attention heads ä»ç‰¹å¾å›¾ä¸­æå– K ä¸ªå…³é”®ç‚¹åæ ‡
    2. é€šè¿‡åŒçº¿æ€§æ’å€¼ä»ç‰¹å¾å›¾ä¸­ç´¢å¼•è¿™äº›åæ ‡å¯¹åº”çš„ç‰¹å¾å‘é‡
    3. åŒæ—¶ä¿ç•™å…³é”®ç‚¹åæ ‡ä¿¡æ¯ï¼ˆç‰©ç†æ„ä¹‰ï¼šå—åŠ›ç‚¹ä½ç½®ï¼‰
    """
    
    def __init__(self, 
                 in_channels: int, 
                 num_keypoints: int = 4, 
                 temperature: float = 1.0,
                 learnable_temperature: bool = True):
        super().__init__()
        self.num_keypoints = num_keypoints
        self.in_channels = in_channels
        
        self.attention_conv = nn.Conv2d(in_channels, num_keypoints, kernel_size=1)
        
        if learnable_temperature:
            self.temperature = nn.Parameter(torch.tensor(temperature))
        else:
            self.register_buffer('temperature', torch.tensor(temperature))
        
        self._coord_cache = {}
    
    def _get_coord_grid(self, H: int, W: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
        cache_key = (H, W, device)
        if cache_key not in self._coord_cache:
            pos_x = torch.linspace(-1, 1, W, device=device)
            pos_y = torch.linspace(-1, 1, H, device=device)
            grid_x, grid_y = torch.meshgrid(pos_x, pos_y, indexing='xy')
            self._coord_cache[cache_key] = (grid_x.reshape(1, 1, H * W), grid_y.reshape(1, 1, H * W))
        return self._coord_cache[cache_key]
    
    def forward(self, feature_map: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        B, C, H, W = feature_map.shape
        K = self.num_keypoints
        
        attention_logits = self.attention_conv(feature_map)
        attention_flat = attention_logits.reshape(B, K, H * W)
        attention_weights = F.softmax(attention_flat / self.temperature, dim=-1)
        
        pos_x, pos_y = self._get_coord_grid(H, W, feature_map.device)
        
        expected_x = (attention_weights * pos_x).sum(dim=-1)
        expected_y = (attention_weights * pos_y).sum(dim=-1)
        keypoint_coords = torch.stack([expected_x, expected_y], dim=-1)
        
        grid = keypoint_coords.unsqueeze(2)
        keypoint_features = F.grid_sample(
            feature_map, grid, 
            mode='bilinear', 
            padding_mode='border', 
            align_corners=True
        )
        keypoint_features = keypoint_features.squeeze(-1).transpose(1, 2)
        
        attention_weights_2d = attention_weights.reshape(B, K, H, W)
        
        return keypoint_coords, keypoint_features, attention_weights_2d


class CompositeTactileEncoder(BaseSensoryEncoder):
    """
    å¤åˆè§¦è§‰ç¼–ç å™¨ï¼šè¾“å‡ºå¤šä¸ªäº’è¡¥çš„ Token
    
    è¾“å‡ºç»“æ„ï¼ˆæ¯ä¸ªè§¦è§‰ä¼ æ„Ÿå™¨ï¼‰:
    - global_token: (B, T, D) - å…¨å±€å¹³å‡æ± åŒ–ç‰¹å¾
    - keypoint_tokens: (B, T, K, D) - Kä¸ªå…³é”®å—åŠ›ç‚¹çš„ç‰¹å¾
    - coord_token: (B, T, D) - å…³é”®ç‚¹åæ ‡ç¼–ç ï¼ˆç‰©ç†ä½ç½®ä¿¡æ¯ï¼‰
    
    æœ€ç»ˆåˆå¹¶ä¸º: (B, T, 1+K+1, D) çš„ token åºåˆ—
    """
    
    def __init__(self,
        shape_meta: dict,
        model_name: str = 'resnet18',
        pretrained: bool = False,
        frozen: bool = False,
        use_group_norm: bool = True,
        share_tactile_model: bool = False,
        feature_dim: int = 768,
        num_keypoints: int = 4,
        temperature: float = 1.0,
        include_coord_token: bool = True,
    ):
        super().__init__()
        
        tactile_keys = []
        key_shape_map = {}
        for key, attr in shape_meta['obs'].items():
            if attr.get('type') == 'rgb' and 'tactile' in key.lower():
                tactile_keys.append(key)
                key_shape_map[key] = tuple(attr['shape'])
        
        tactile_keys = sorted(tactile_keys)
        
        self.num_keypoints = num_keypoints
        self.include_coord_token = include_coord_token
        self.feature_dim = feature_dim
        
        key_backbone_map = nn.ModuleDict()
        key_keypoint_extractor_map = nn.ModuleDict()
        
        self.global_proj = nn.Linear(512, feature_dim)
        self.keypoint_proj = nn.Linear(512, feature_dim)
        if include_coord_token:
            self.coord_encoder = nn.Sequential(
                nn.Linear(num_keypoints * 2, 128),
                nn.GELU(),
                nn.Linear(128, feature_dim)
            )
        
        if share_tactile_model and len(tactile_keys) > 0:
            shared_backbone = self._create_backbone(
                key_shape_map[tactile_keys[0]], 
                model_name, pretrained, frozen, use_group_norm
            )
            shared_keypoint_extractor = KeypointSpatialSoftmax(
                in_channels=512, 
                num_keypoints=num_keypoints,
                temperature=temperature
            )
            for key in tactile_keys:
                key_backbone_map[key] = shared_backbone
                key_keypoint_extractor_map[key] = shared_keypoint_extractor
        else:
            for key in tactile_keys:
                key_backbone_map[key] = self._create_backbone(
                    key_shape_map[key],
                    model_name, pretrained, frozen, use_group_norm
                )
                key_keypoint_extractor_map[key] = KeypointSpatialSoftmax(
                    in_channels=512,
                    num_keypoints=num_keypoints,
                    temperature=temperature
                )
        
        self.tactile_keys = tactile_keys
        self.key_backbone_map = key_backbone_map
        self.key_keypoint_extractor_map = key_keypoint_extractor_map
        self.key_shape_map = key_shape_map
        
        self.num_tokens_per_sensor = 1 + num_keypoints + (1 if include_coord_token else 0)
        
        print(f"âœ“ å¤åˆè§¦è§‰ç¼–ç å™¨: {num_keypoints} å…³é”®ç‚¹ + 1 å…¨å±€" + 
              (f" + 1 åæ ‡" if include_coord_token else ""))
        print(f"  æ¯ä¸ªä¼ æ„Ÿå™¨è¾“å‡º {self.num_tokens_per_sensor} ä¸ª tokens, ç»´åº¦ {feature_dim}")
    
    def _create_backbone(self, shape, model_name, pretrained, frozen, use_group_norm):
        in_channels = shape[0]
        
        model = timm.create_model(
            model_name=model_name,
            pretrained=pretrained,
            in_chans=in_channels,
            global_pool='',
            num_classes=0
        )
        
        if frozen:
            for param in model.parameters():
                param.requires_grad = False
        
        if model_name.startswith('resnet'):
            modules = list(model.children())[:-2]
            backbone = nn.Sequential(*modules)
        else:
            raise NotImplementedError(f"Unsupported model: {model_name}")
        
        if use_group_norm and not pretrained:
            backbone = replace_submodules(
                root_module=backbone,
                predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                func=lambda x: nn.GroupNorm(
                    num_groups=max(1, x.num_features // 16),
                    num_channels=x.num_features
                )
            )
        
        return backbone
    
    def modalities(self):
        return ['tactile']
    
    def forward(self, obs: Dict[str, torch.Tensor], 
                return_attention: bool = False) -> Dict[str, torch.Tensor]:
        output = {}
        attention_output = {}
        
        for key in self.tactile_keys:
            if key not in obs:
                continue
            
            tactile_data = obs[key]
            
            if len(tactile_data.shape) == 5:
                B, T = tactile_data.shape[:2]
                tactile_data = tactile_data.reshape(B * T, *tactile_data.shape[2:])
            else:
                B = tactile_data.shape[0]
                T = 1
            
            if tactile_data.max() > 1.0:
                tactile_data = tactile_data / 255.0
            
            expected_shape = self.key_shape_map[key]
            if tactile_data.shape[1:] != expected_shape:
                target_H, target_W = expected_shape[1], expected_shape[2]
                tactile_data = F.interpolate(
                    tactile_data, 
                    size=(target_H, target_W), 
                    mode='bilinear', 
                    align_corners=False
                )
            
            feature_map = self.key_backbone_map[key](tactile_data)
            
            global_feat = F.adaptive_avg_pool2d(feature_map, 1).flatten(1)
            global_token = self.global_proj(global_feat)
            
            keypoint_coords, keypoint_feats, attention_weights = \
                self.key_keypoint_extractor_map[key](feature_map)
            keypoint_tokens = self.keypoint_proj(keypoint_feats)
            
            tokens_list = [global_token.unsqueeze(1)]
            tokens_list.append(keypoint_tokens)
            
            if self.include_coord_token:
                coord_flat = keypoint_coords.reshape(B * T, -1)
                coord_token = self.coord_encoder(coord_flat).unsqueeze(1)
                tokens_list.append(coord_token)
            
            all_tokens = torch.cat(tokens_list, dim=1)
            all_tokens = all_tokens.reshape(B, T, self.num_tokens_per_sensor, self.feature_dim)
            
            output[key] = all_tokens
            
            if return_attention:
                _, _, H_feat, W_feat = feature_map.shape
                attention_output[key] = attention_weights.reshape(B, T, self.num_keypoints, H_feat, W_feat)
        
        if return_attention:
            return output, attention_output
        return output
    
    def output_feature_dim(self):
        return {key: self.feature_dim for key in self.tactile_keys}
    
    def output_num_tokens(self):
        return {key: self.num_tokens_per_sensor for key in self.tactile_keys}


class DiffAwareCompositeTactileEncoder(BaseSensoryEncoder):
    """
    ğŸ”¥ å·®åˆ†æ„ŸçŸ¥å¤åˆè§¦è§‰ç¼–ç å™¨ - ä¸“æ³¨äºè§¦è§‰ä¿¡å·å˜åŒ–
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. å·®åˆ†è¾“å…¥åˆ†æ”¯ï¼šå°†å½“å‰å¸§ I_t ä¸å˜åŒ–å¸§ I_diff = I_t - I_{t-1} åˆ†åˆ«å¤„ç†
    2. ä½¿ç”¨ SharpKeypointSpatialSoftmax å®ç°é”åˆ©çš„å…³é”®ç‚¹æå–
    3. åŒæµæ¶æ„ï¼šåˆ†åˆ«å¤„ç†é™æ€ç‰¹å¾å’ŒåŠ¨æ€å˜åŒ–ç‰¹å¾
    
    è¾“å‡ºç»“æ„ï¼ˆæ¯ä¸ªè§¦è§‰ä¼ æ„Ÿå™¨ï¼‰:
    - global_token: å…¨å±€ç‰¹å¾ï¼ˆé™æ€ + åŠ¨æ€èåˆï¼‰
    - keypoint_tokens: K ä¸ªå…³é”®å—åŠ›ç‚¹çš„ç‰¹å¾
    - diff_token: å·®åˆ†/å˜åŒ–ç‰¹å¾ token
    - coord_token: å…³é”®ç‚¹åæ ‡ç¼–ç 
    """
    
    def __init__(self,
        shape_meta: dict,
        model_name: str = 'resnet18',
        pretrained: bool = False,
        frozen: bool = False,
        use_group_norm: bool = True,
        share_tactile_model: bool = False,
        feature_dim: int = 768,
        num_keypoints: int = 4,
        init_temperature: float = 0.1,
        noise_threshold: float = 0.15,
        include_coord_token: bool = True,
        include_diff_token: bool = True,
        diff_amplify: float = 2.0,
    ):
        super().__init__()
        
        tactile_keys = []
        key_shape_map = {}
        for key, attr in shape_meta['obs'].items():
            if attr.get('type') == 'rgb' and 'tactile' in key.lower():
                tactile_keys.append(key)
                key_shape_map[key] = tuple(attr['shape'])
        
        tactile_keys = sorted(tactile_keys)
        
        self.num_keypoints = num_keypoints
        self.include_coord_token = include_coord_token
        self.include_diff_token = include_diff_token
        self.feature_dim = feature_dim
        self.diff_amplify = diff_amplify
        
        # åŒæµéª¨å¹²ç½‘ç»œ
        key_static_backbone_map = nn.ModuleDict()
        key_diff_backbone_map = nn.ModuleDict()
        key_keypoint_extractor_map = nn.ModuleDict()
        
        # ç‰¹å¾èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(512 * 2, 512),
            nn.GELU(),
            nn.Linear(512, 512)
        )
        
        # æŠ•å½±å±‚
        self.global_proj = nn.Linear(512, feature_dim)
        self.keypoint_proj = nn.Linear(512, feature_dim)
        
        if include_diff_token:
            self.diff_proj = nn.Linear(512, feature_dim)
        
        if include_coord_token:
            self.coord_encoder = nn.Sequential(
                nn.Linear(num_keypoints * 2, 128),
                nn.GELU(),
                nn.Linear(128, feature_dim)
            )
        
        if share_tactile_model and len(tactile_keys) > 0:
            shared_static_backbone = self._create_backbone(
                key_shape_map[tactile_keys[0]], 
                model_name, pretrained, frozen, use_group_norm
            )
            shared_diff_backbone = self._create_backbone(
                key_shape_map[tactile_keys[0]], 
                model_name, False, frozen, use_group_norm
            )
            shared_keypoint_extractor = SharpKeypointSpatialSoftmax(
                in_channels=512, 
                num_keypoints=num_keypoints,
                init_temperature=init_temperature,
                noise_threshold=noise_threshold,
                learnable_temperature=True,
                learnable_threshold=True
            )
            for key in tactile_keys:
                key_static_backbone_map[key] = shared_static_backbone
                key_diff_backbone_map[key] = shared_diff_backbone
                key_keypoint_extractor_map[key] = shared_keypoint_extractor
        else:
            for key in tactile_keys:
                key_static_backbone_map[key] = self._create_backbone(
                    key_shape_map[key], model_name, pretrained, frozen, use_group_norm
                )
                key_diff_backbone_map[key] = self._create_backbone(
                    key_shape_map[key], model_name, False, frozen, use_group_norm
                )
                key_keypoint_extractor_map[key] = SharpKeypointSpatialSoftmax(
                    in_channels=512,
                    num_keypoints=num_keypoints,
                    init_temperature=init_temperature,
                    noise_threshold=noise_threshold
                )
        
        self.tactile_keys = tactile_keys
        self.key_static_backbone_map = key_static_backbone_map
        self.key_diff_backbone_map = key_diff_backbone_map
        self.key_keypoint_extractor_map = key_keypoint_extractor_map
        self.key_shape_map = key_shape_map
        
        self.num_tokens_per_sensor = (1 + num_keypoints + 
                                       (1 if include_coord_token else 0) +
                                       (1 if include_diff_token else 0))
        
        print(f"âœ“ å·®åˆ†æ„ŸçŸ¥è§¦è§‰ç¼–ç å™¨: {num_keypoints} å…³é”®ç‚¹ + 1 å…¨å±€" + 
              (f" + 1 åæ ‡" if include_coord_token else "") +
              (f" + 1 å·®åˆ†" if include_diff_token else ""))
        print(f"  æ¸©åº¦={init_temperature}, å™ªå£°é˜ˆå€¼={noise_threshold}, å·®åˆ†æ”¾å¤§={diff_amplify}")
        print(f"  æ¯ä¸ªä¼ æ„Ÿå™¨è¾“å‡º {self.num_tokens_per_sensor} ä¸ª tokens, ç»´åº¦ {feature_dim}")
    
    def _create_backbone(self, shape, model_name, pretrained, frozen, use_group_norm):
        in_channels = shape[0]
        
        model = timm.create_model(
            model_name=model_name,
            pretrained=pretrained,
            in_chans=in_channels,
            global_pool='',
            num_classes=0
        )
        
        if frozen:
            for param in model.parameters():
                param.requires_grad = False
        
        if model_name.startswith('resnet'):
            modules = list(model.children())[:-2]
            backbone = nn.Sequential(*modules)
        else:
            raise NotImplementedError(f"Unsupported model: {model_name}")
        
        if use_group_norm and not pretrained:
            backbone = replace_submodules(
                root_module=backbone,
                predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                func=lambda x: nn.GroupNorm(
                    num_groups=max(1, x.num_features // 16),
                    num_channels=x.num_features
                )
            )
        
        return backbone
    
    def modalities(self):
        return ['tactile']
    
    def forward(self, obs: Dict[str, torch.Tensor],
                prev_obs: Optional[Dict[str, torch.Tensor]] = None,
                return_attention: bool = False) -> Dict[str, torch.Tensor]:
        """
        è¾“å…¥: 
            - obs: å½“å‰å¸§ {key: (B, T, C, H, W) æˆ– (B, C, H, W)}
            - prev_obs: ä¸Šä¸€å¸§ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸º Noneï¼Œä½¿ç”¨é›¶å¸§æˆ–æ—¶åºå†…å·®åˆ†
        è¾“å‡º:
            - features: (B, T, num_tokens, D)
            - attention_maps (å¯é€‰)
        """
        output = {}
        attention_output = {}
        
        for key in self.tactile_keys:
            if key not in obs:
                continue
            
            tactile_data = obs[key]
            
            if len(tactile_data.shape) == 5:
                B, T = tactile_data.shape[:2]
                tactile_data = tactile_data.reshape(B * T, *tactile_data.shape[2:])
            else:
                B = tactile_data.shape[0]
                T = 1
            
            if tactile_data.max() > 1.0:
                tactile_data = tactile_data / 255.0
            
            expected_shape = self.key_shape_map[key]
            if tactile_data.shape[1:] != expected_shape:
                target_H, target_W = expected_shape[1], expected_shape[2]
                tactile_data = F.interpolate(
                    tactile_data, size=(target_H, target_W),
                    mode='bilinear', align_corners=False
                )
            
            # è®¡ç®—å·®åˆ†å¸§
            if prev_obs is not None and key in prev_obs:
                prev_data = prev_obs[key]
                if len(prev_data.shape) == 5:
                    prev_data = prev_data.reshape(B * T, *prev_data.shape[2:])
                if prev_data.max() > 1.0:
                    prev_data = prev_data / 255.0
                if prev_data.shape[1:] != expected_shape:
                    prev_data = F.interpolate(
                        prev_data, size=(target_H, target_W),
                        mode='bilinear', align_corners=False
                    )
                diff_data = (tactile_data - prev_data) * self.diff_amplify
            else:
                if T > 1:
                    tactile_seq = tactile_data.reshape(B, T, *tactile_data.shape[1:])
                    diff_seq = torch.zeros_like(tactile_seq)
                    diff_seq[:, 1:] = (tactile_seq[:, 1:] - tactile_seq[:, :-1]) * self.diff_amplify
                    diff_data = diff_seq.reshape(B * T, *tactile_data.shape[1:])
                else:
                    diff_data = torch.zeros_like(tactile_data)
            
            # åŒæµç‰¹å¾æå–
            static_feat_map = self.key_static_backbone_map[key](tactile_data)
            diff_feat_map = self.key_diff_backbone_map[key](diff_data.clamp(-1, 1))
            
            # ç‰¹å¾èåˆ
            static_global = F.adaptive_avg_pool2d(static_feat_map, 1).flatten(1)
            diff_global = F.adaptive_avg_pool2d(diff_feat_map, 1).flatten(1)
            fused_global = self.fusion_layer(torch.cat([static_global, diff_global], dim=-1))
            
            global_token = self.global_proj(fused_global)
            
            # åœ¨èåˆç‰¹å¾å›¾ä¸Šæå–å…³é”®ç‚¹
            combined_feat_map = static_feat_map + diff_feat_map * 0.5
            
            keypoint_coords, keypoint_feats, attention_weights = \
                self.key_keypoint_extractor_map[key](combined_feat_map)
            keypoint_tokens = self.keypoint_proj(keypoint_feats)
            
            # ç»„è£… tokens
            tokens_list = [global_token.unsqueeze(1)]
            tokens_list.append(keypoint_tokens)
            
            if self.include_diff_token:
                diff_token = self.diff_proj(diff_global).unsqueeze(1)
                tokens_list.append(diff_token)
            
            if self.include_coord_token:
                coord_flat = keypoint_coords.reshape(B * T, -1)
                coord_token = self.coord_encoder(coord_flat).unsqueeze(1)
                tokens_list.append(coord_token)
            
            all_tokens = torch.cat(tokens_list, dim=1)
            all_tokens = all_tokens.reshape(B, T, self.num_tokens_per_sensor, self.feature_dim)
            
            output[key] = all_tokens
            
            if return_attention:
                _, _, H_feat, W_feat = combined_feat_map.shape
                attention_output[key] = attention_weights.reshape(B, T, self.num_keypoints, H_feat, W_feat)
        
        if return_attention:
            return output, attention_output
        return output
    
    def output_feature_dim(self):
        return {key: self.feature_dim for key in self.tactile_keys}
    
    def output_num_tokens(self):
        return {key: self.num_tokens_per_sensor for key in self.tactile_keys}
    
    def get_sharpness_stats(self) -> Dict[str, Dict[str, float]]:
        """è·å–æ‰€æœ‰å…³é”®ç‚¹æå–å™¨çš„é”åº¦å‚æ•°"""
        stats = {}
        for key in self.tactile_keys:
            extractor = self.key_keypoint_extractor_map[key]
            if hasattr(extractor, 'get_sharpness_stats'):
                stats[key] = extractor.get_sharpness_stats()
        return stats


# ========== éªŒè¯è„šæœ¬ ==========
if __name__ == '__main__':
    import os
    import argparse
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.gridspec import GridSpec
    
    parser = argparse.ArgumentParser(description='å·®åˆ†æ„ŸçŸ¥è§¦è§‰ç¼–ç å™¨è®­ç»ƒä¸éªŒè¯')
    parser.add_argument('--zarr_path', type=str, 
                        default='/root/autodl-tmp/robotwin/policy/ManiFlow/data/feed_dual-40.zarr',
                        help='zarr æ•°æ®è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-3, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_samples', type=int, default=10000, help='è®­ç»ƒæ ·æœ¬æ•°')
    parser.add_argument('--feature_dim', type=int, default=128, help='ç‰¹å¾ç»´åº¦')
    parser.add_argument('--num_keypoints', type=int, default=4, help='å…³é”®ç‚¹æ•°é‡')
    parser.add_argument('--init_temperature', type=float, default=0.1, help='åˆå§‹æ¸©åº¦')
    parser.add_argument('--noise_threshold', type=float, default=0.15, help='å™ªå£°é˜ˆå€¼')
    parser.add_argument('--diff_amplify', type=float, default=2.0, help='å·®åˆ†æ”¾å¤§ç³»æ•°')
    parser.add_argument('--save_dir', type=str, 
                        default='/root/autodl-tmp/robotwin/policy/ManiFlow/data',
                        help='ä¿å­˜ç›®å½•')
    parser.add_argument('--use_diff_encoder', action='store_true', default=True,
                        help='ä½¿ç”¨å·®åˆ†æ„ŸçŸ¥ç¼–ç å™¨')
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸ”¬ DiffAwareCompositeTactileEncoder éªŒè¯")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    try:
        import zarr
        z = zarr.open(args.zarr_path, 'r')
        left_tactile = z['data/left_tactile_sensor'][:]
        right_tactile = z['data/right_tactile_sensor'][:]
        episode_ends = z['meta/episode_ends'][:]
        print(f"\nğŸ“Š æ•°æ®åŠ è½½æˆåŠŸ: {left_tactile.shape}")
        USE_REAL_DATA = True
    except Exception as e:
        print(f"\nâš ï¸ æ— æ³•åŠ è½½æ•°æ®: {e}")
        left_tactile = np.random.rand(5000, 1, 16, 32).astype(np.float32) * 255
        episode_ends = np.array([500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000])
        USE_REAL_DATA = False
    
    # æ„å»ºæ¨¡å‹
    shape_meta = {
        'obs': {
            'left_tactile_sensor': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
            'right_tactile_sensor': {'shape': [1, 16, 32], 'type': 'rgb', 'horizon': 2},
        }
    }
    
    encoder = DiffAwareCompositeTactileEncoder(
        shape_meta=shape_meta,
        model_name='resnet18',
        pretrained=False,
        use_group_norm=True,
        share_tactile_model=True,
        feature_dim=args.feature_dim,
        num_keypoints=args.num_keypoints,
        init_temperature=args.init_temperature,
        noise_threshold=args.noise_threshold,
        include_coord_token=True,
        include_diff_token=True,
        diff_amplify=args.diff_amplify,
    )
    
    print(f"\nğŸ”§ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in encoder.parameters()):,}")
    
    # ç®€å•æµ‹è¯•
    obs = {'left_tactile_sensor': torch.randn(4, 2, 1, 16, 32)}
    prev_obs = {'left_tactile_sensor': torch.randn(4, 2, 1, 16, 32)}
    
    with torch.no_grad():
        output, attn = encoder(obs, prev_obs=prev_obs, return_attention=True)
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output['left_tactile_sensor'].shape}")
    print(f"æ³¨æ„åŠ›å½¢çŠ¶: {attn['left_tactile_sensor'].shape}")
    print(f"é”åº¦å‚æ•°: {encoder.get_sharpness_stats()}")
    
    print("\nâœ… æµ‹è¯•é€šè¿‡!\n")
