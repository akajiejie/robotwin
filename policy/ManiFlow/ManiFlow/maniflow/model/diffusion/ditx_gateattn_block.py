# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# References:
# DiT: https://github.com/facebookresearch/DiT
# RDT: https://github.com/thu-ml/RoboticsDiffusionTransformer
# Qwen3: Gate-Attention mechanism
# --------------------------------------------------------
#
# ä½¿ç”¨è¯´æ˜:
# DiTX-GateAttn Block: ä½¿ç”¨Gate-Attentionè¿›è¡Œç‰¹å¾å…³æ³¨ï¼ˆå‚è€ƒQwen3ï¼‰
# è¿è¡Œæµ‹è¯•: python ditx_gateattn_block.py
# --------------------------------------------------------

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.jit import Final
from einops.layers.torch import Rearrange
from timm.models.vision_transformer import Mlp, use_fused_attn

logger = logging.getLogger(__name__)


FLASH_ATTN_AVAILABLE = False
try:
    from flash_attn import flash_attn_func
    FLASH_ATTN_AVAILABLE = True
    logger.info("Flash Attention 2 å·²å¯ç”¨ï¼Œè®­ç»ƒå°†æ˜¾è‘—åŠ é€Ÿï¼")
except ImportError:
    logger.info("Flash Attention æœªå®‰è£…ï¼Œä½¿ç”¨ PyTorch SDPA åç«¯")

def modulate(x, shift, scale):
    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)


class FlashSelfAttention(nn.Module):
    """
    Self-Attention with Flash Attention 2 support.
    
    å½“ flash-attn å¯ç”¨æ—¶ä½¿ç”¨ Flash Attention 2ï¼Œå¦åˆ™å›é€€åˆ° PyTorch SDPAã€‚
    æ¯” nn.MultiheadAttention æ›´å¿«ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿åºåˆ—ä¸Šã€‚
    """
    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = False,
        qk_norm: bool = False,
        attn_drop: float = 0.,
        proj_drop: float = 0.,
        norm_layer: nn.Module = nn.LayerNorm,
    ):
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        
        # QKV projection
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        
        self.use_flash_attn = FLASH_ATTN_AVAILABLE
        
    def forward(self, x: torch.Tensor, attn_mask=None):
        """
        Args:
            x: Input tensor of shape (B, N, C)
            attn_mask: Optional attention mask (not supported with Flash Attention)
        
        Returns:
            output: (B, N, C)
            attn_weights: None (Flash Attention doesn't return weights)
        """
        B, N, C = x.shape
        
        # QKV projection: (B, N, 3*C) -> (B, N, 3, num_heads, head_dim)
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        
        if self.use_flash_attn and x.is_cuda and x.dtype in [torch.float16, torch.bfloat16]:
            # ğŸš€ Flash Attention 2 è·¯å¾„
            # flash_attn_func éœ€è¦ (B, N, num_heads, head_dim) æ ¼å¼
            q, k, v = qkv.unbind(2)  # 3 x (B, N, num_heads, head_dim)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)
            
            # Flash Attention (è‡ªåŠ¨å¤„ç† causal=False)
            dropout_p = self.attn_drop.p if self.training else 0.
            out = flash_attn_func(q, k, v, dropout_p=dropout_p, causal=False)
            # out: (B, N, num_heads, head_dim)
            
        else:
            # PyTorch SDPA åç«¯ï¼ˆæ”¯æŒ FP32ï¼‰
            qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)
            q, k, v = qkv.unbind(0)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)
            
            # Scaled dot-product attention
            out = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=attn_mask,
                dropout_p=self.attn_drop.p if self.training else 0.,
            )
            # out: (B, num_heads, N, head_dim) -> (B, N, num_heads, head_dim)
            out = out.transpose(1, 2)
        
        # Reshape and project
        out = out.reshape(B, N, C)
        out = self.proj(out)
        out = self.proj_drop(out)
        
        return out, None  # è¿”å› None ä½œä¸º attn_weightsï¼Œä¿æŒæ¥å£å…¼å®¹


class CrossAttention(nn.Module):
    """
    Cross-attention layer with flash attention and gate mechanism.
    
    æ”¯æŒä¸¤ç§é—¨æ§æ¨¡å¼ï¼ˆå‚è€ƒQwen3çš„gated attentionï¼‰ï¼š
    - 'none': æ— é—¨æ§ï¼ˆæ ‡å‡†cross-attentionï¼‰
    - 'headwise': æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸€ä¸ªgateå€¼ï¼ˆè½»é‡çº§ï¼‰
    - 'elementwise': æ¯ä¸ªå…ƒç´ ä¸€ä¸ªgateå€¼ï¼ˆæœ€ç»†ç²’åº¦ï¼Œå‚è€ƒQwen3ï¼‰
    
    Args:
        gate_type: é—¨æ§ç±»å‹ ('none', 'headwise', 'elementwise')
    """
    fused_attn: Final[bool]
    def __init__(
            self,
            dim: int,
            num_heads: int = 8,
            qkv_bias: bool = False,
            qk_norm: bool = False,
            attn_drop: float = 0,
            proj_drop: float = 0,
            norm_layer: nn.Module = nn.LayerNorm,
            gate_type: str = 'none',  # ğŸ”¥ gate-attentionç±»å‹
    ) -> None:
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.fused_attn = use_fused_attn()
        self.use_flash_attn = FLASH_ATTN_AVAILABLE
        self.gate_type = gate_type

        # Query projection with optional gateï¼ˆå‚è€ƒQwen3ï¼‰
        if gate_type == 'headwise':
            # æ¯ä¸ªå¤´ä¸€ä¸ªgate: q_dim + num_heads
            self.q = nn.Linear(dim, dim + num_heads, bias=qkv_bias)
        elif gate_type == 'elementwise':
            # æ¯ä¸ªå…ƒç´ ä¸€ä¸ªgate: q_dim * 2ï¼ˆä¸Qwen3ä¸€è‡´ï¼‰
            self.q = nn.Linear(dim, dim * 2, bias=qkv_bias)
        else:
            # æ ‡å‡†query
            self.q = nn.Linear(dim, dim, bias=qkv_bias)
        
        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        
        if gate_type != 'none':
            logger.info(f"[CrossAttention] å¯ç”¨Gate-Attentionæœºåˆ¶: {gate_type}ï¼ˆå‚è€ƒQwen3ï¼‰")
    
    def forward(self, x: torch.Tensor, c: torch.Tensor, 
                mask: None) -> torch.Tensor:
        B, N, C = x.shape
        _, L, _ = c.shape
        
        # Query projection with gate extractionï¼ˆå‚è€ƒQwen3å®ç°ï¼‰
        q_output = self.q(x)
        
        if self.gate_type == 'headwise':
            # Headwise gate: æ¯ä¸ªå¤´ä¸€ä¸ªgateå€¼
            # q_output: (B, N, dim + num_heads)
            q_output = q_output.view(B, N, self.num_heads, -1)
            q, gate_score = torch.split(q_output, [self.head_dim, 1], dim=-1)
            # gate_score: (B, N, num_heads, 1)
            q = q.permute(0, 2, 1, 3)  # (B, num_heads, N, head_dim)
            
        elif self.gate_type == 'elementwise':
            # Elementwise gate: æ¯ä¸ªå…ƒç´ ä¸€ä¸ªgateå€¼ï¼ˆä¸Qwen3ä¸€è‡´ï¼‰
            # q_output: (B, N, dim * 2)
            q_output = q_output.view(B, N, self.num_heads, -1)
            q, gate_score = torch.split(q_output, [self.head_dim, self.head_dim], dim=-1)
            # gate_score: (B, N, num_heads, head_dim)
            q = q.permute(0, 2, 1, 3)  # (B, num_heads, N, head_dim)
            
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šæ— gate
            q = q_output.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
            gate_score = None
        
        # Key-Value projection
        kv = self.kv(c).reshape(B, L, 2, self.num_heads, self.head_dim)
        k, v = kv.unbind(2)  # k, v: (B, L, num_heads, head_dim)
        
        # Flash Attention è·¯å¾„
        if self.use_flash_attn and x.is_cuda and x.dtype in [torch.float16, torch.bfloat16] and mask is None:
            # Flash Attention éœ€è¦ (B, N, num_heads, head_dim) æ ¼å¼
            # q å½“å‰æ˜¯ (B, num_heads, N, head_dim)ï¼Œéœ€è¦è½¬æ¢
            q = q.transpose(1, 2)  # (B, N, num_heads, head_dim)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)
            
            # Flash Attention cross-attention
            dropout_p = self.attn_drop.p if self.training else 0.
            attn_output = flash_attn_func(q, k, v, dropout_p=dropout_p, causal=False)
            # attn_output: (B, N, num_heads, head_dim)
            
        else:
            # PyTorch SDPA åç«¯
            # è½¬æ¢ k, v åˆ° (B, num_heads, L, head_dim)
            k = k.permute(0, 2, 1, 3)
            v = v.permute(0, 2, 1, 3)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)

            # Prepare attn mask (B, L) to mask the condition
            if mask is not None:
                mask = mask.reshape(B, 1, 1, L)
                mask = mask.expand(-1, -1, N, -1)
            
            # Attention computation
            if self.fused_attn:
                attn_output = F.scaled_dot_product_attention(
                    query=q,
                    key=k,
                    value=v,
                    dropout_p=self.attn_drop.p if self.training else 0.,
                    attn_mask=mask
                )
            else:
                q = q * self.scale
                attn = q @ k.transpose(-2, -1)
                if mask is not None:
                    attn = attn.masked_fill_(mask.logical_not(), float('-inf'))
                attn = attn.softmax(dim=-1)
                if self.attn_drop.p > 0:
                    attn = self.attn_drop(attn)
                attn_output = attn @ v
            
            # attn_output: (B, num_heads, N, head_dim) -> (B, N, num_heads, head_dim)
            attn_output = attn_output.transpose(1, 2)
        
        # Gate-Attention: ç”¨sigmoid(gate)è°ƒåˆ¶attentionè¾“å‡ºï¼ˆå‚è€ƒQwen3ï¼‰
        if gate_score is not None:
            gate_activation = torch.sigmoid(gate_score)
            attn_output = attn_output * gate_activation
            
            # æ”¶é›†Gate-Attentionæ¿€æ´»ç»Ÿè®¡ï¼ˆç”¨äºwandbç›‘æ§ï¼‰
            if self.training:
                with torch.no_grad():
                    # è®¡ç®—æ¿€æ´»å€¼çš„å‡å€¼å’Œæ ‡å‡†å·®
                    gate_mean = gate_activation.mean().item()
                    gate_std = gate_activation.std().item()
                    gate_min = gate_activation.min().item()
                    gate_max = gate_activation.max().item()
                    
                    # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆåœ¨blockçš„get_gate_statsä¸­è®¿é—®ï¼‰
                    if not hasattr(self, '_gate_stats_buffer'):
                        self._gate_stats_buffer = []
                    self._gate_stats_buffer.append({
                        'mean': gate_mean,
                        'std': gate_std,
                        'min': gate_min,
                        'max': gate_max,
                    })
        
        # Reshape and project
        attn_output = attn_output.reshape(B, N, C)
        attn_output = self.proj(attn_output)
        if self.proj_drop.p > 0:
            attn_output = self.proj_drop(attn_output)
        
        return attn_output


class DiTXGateAttnBlock(nn.Module):
    """
    DiTX Block with Gate-Attention mechanism (å‚è€ƒQwen3).
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. Gate-Attentionï¼šCross-attentionè¾“å‡ºé€šè¿‡å¯å­¦ä¹ çš„gateè°ƒåˆ¶
    2. æ”¯æŒä¸‰ç§gateæ¨¡å¼ï¼š
       - 'none': æ ‡å‡†cross-attentionï¼ˆæ— gateï¼‰
       - 'headwise': æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸€ä¸ªgateå€¼ï¼ˆè½»é‡çº§ï¼‰
       - 'elementwise': æ¯ä¸ªå…ƒç´ ä¸€ä¸ªgateå€¼ï¼ˆæœ€ç»†ç²’åº¦ï¼‰
    3. Flash Attention 2ï¼šåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†
    
    Args:
        hidden_size: éšè—å±‚ç»´åº¦
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        mlp_ratio: MLPæ‰©å±•æ¯”ä¾‹
        gate_type: Gate-Attentionç±»å‹ ('none', 'headwise', 'elementwise')
        p_drop_attn: Attention dropoutæ¦‚ç‡
        qkv_bias: æ˜¯å¦ä½¿ç”¨QKV bias
        qk_norm: æ˜¯å¦å¯¹Qå’ŒKè¿›è¡Œå½’ä¸€åŒ–
    """
    def __init__(self, 
                hidden_size=768,
                num_heads=12,
                mlp_ratio=4.0,
                
                # Gate-Attentioné…ç½®
                gate_type='elementwise',      # 'none', 'headwise', 'elementwise'
                
                # å…¶ä»–å‚æ•°
                p_drop_attn=0.1,
                qkv_bias=False,
                qk_norm=False,
                **block_kwargs):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.gate_type = gate_type

        # Self-Attention with Flash Attention support
        self.self_attn = FlashSelfAttention(
            dim=hidden_size,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_norm=qk_norm,
            attn_drop=p_drop_attn,
            proj_drop=0.,
            norm_layer=nn.LayerNorm,
        )
        
        # Cross-Attention with Gate-Attention
        self.cross_attn = CrossAttention(
            dim=hidden_size, 
            num_heads=num_heads,
            qkv_bias=qkv_bias, 
            qk_norm=qk_norm,
            norm_layer=nn.LayerNorm,
            gate_type=gate_type,  # Gate-Attentioné…ç½®
            **block_kwargs
        )
       
        # MLP
        mlp_hidden_dim = int(hidden_size * mlp_ratio)
        approx_gelu = lambda: nn.GELU(approximate="tanh")
        self.mlp = Mlp(
            in_features=hidden_size, 
            hidden_features=mlp_hidden_dim, 
            act_layer=approx_gelu, 
            drop=0.0
        )

        # Normalization layers
        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.norm3 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)

        # AdaLN modulation
        modulation_size = 9 * hidden_size
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(hidden_size, modulation_size, bias=True)
        )
        
        logger.info(f"[DiTXGateAttnBlock] Initialized with Gate-Attention: {gate_type}")
    
    def get_gate_stats(self):
        """è·å–Gate-Attentionç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºwandbè®°å½•ï¼‰"""
        stats = {}
        
        # Gate-Attentionæ¿€æ´»åˆ†å¸ƒ
        if hasattr(self.cross_attn, '_gate_stats_buffer') and len(self.cross_attn._gate_stats_buffer) > 0:
            gate_stats = self.cross_attn._gate_stats_buffer[-1]
            stats['gate_activation_mean'] = gate_stats['mean']
            stats['gate_activation_std'] = gate_stats['std']
            stats['gate_activation_min'] = gate_stats['min']
            stats['gate_activation_max'] = gate_stats['max']
            # æ¸…ç©ºbuffer
            self.cross_attn._gate_stats_buffer.clear()
        
        return stats if stats else None
        
    def forward(self, x, time_c, context_c, attn_mask=None):
        """
        Forward pass of the DiTX-GateAttn block.
        
        Args:
            x: åŠ¨ä½œåºåˆ— (batch_size, seq_length, hidden_size)
            time_c: æ—¶é—´æ­¥åµŒå…¥ (batch_size, hidden_size)
            context_c: å¤šæ¨¡æ€ç‰¹å¾ (batch_size, L_total, hidden_size)
            attn_mask: å¯é€‰çš„æ³¨æ„åŠ›mask
        
        Returns:
            x: è¾“å‡ºç‰¹å¾ (batch_size, seq_length, hidden_size)
        """
        # adaLN modulation
        modulation = self.adaLN_modulation(time_c)
        chunks = modulation.chunk(9, dim=-1)
        
        shift_msa, scale_msa, gate_msa = chunks[0], chunks[1], chunks[2]
        shift_cross, scale_cross, gate_cross = chunks[3], chunks[4], chunks[5]
        shift_mlp, scale_mlp, gate_mlp = chunks[6], chunks[7], chunks[8]

        # 1. Self-Attention with adaLN conditioning (Flash Attention)
        normed_x = modulate(self.norm1(x), shift_msa, scale_msa)
        self_attn_output, _ = self.self_attn(normed_x, attn_mask=attn_mask)
        x = x + gate_msa.unsqueeze(1) * self_attn_output

        # 2. Cross-Attention with Gate-Attention
        normed_x_cross = modulate(self.norm2(x), shift_cross, scale_cross)
        cross_attn_output = self.cross_attn(normed_x_cross, context_c, mask=None)
        x = x + gate_cross.unsqueeze(1) * cross_attn_output

        # 3. MLP with adaLN conditioning
        normed_x_mlp = modulate(self.norm3(x), shift_mlp, scale_mlp)
        mlp_output = self.mlp(normed_x_mlp)
        x = x + gate_mlp.unsqueeze(1) * mlp_output

        return x


if __name__ == "__main__":
    """
    æµ‹è¯•DiTX-GateAttn Blockçš„åŠŸèƒ½
    è¿è¡Œæ–¹å¼: python ditx_gateattn_block.py
    """
    
    def test_ditx_gateattn_block():
        """æµ‹è¯•DiTXGateAttnBlockçš„åŸºæœ¬åŠŸèƒ½"""
        print("=" * 80)
        print("æµ‹è¯• DiTXGateAttnBlock (Gate-Attention)")
        print("=" * 80)
        
        # å‚æ•°è®¾ç½®
        batch_size = 4
        seq_len = 50          # åŠ¨ä½œåºåˆ—é•¿åº¦
        hidden_size = 768
        num_heads = 12
        L_total = 1180        # å¤šæ¨¡æ€ç‰¹å¾é•¿åº¦
        
        # åˆ›å»ºDiTXGateAttnBlock
        block = DiTXGateAttnBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mlp_ratio=4.0,
            gate_type='headwise',  # æµ‹è¯•headwise gate
            p_drop_attn=0.1
        )
        
        # è¾“å…¥æ•°æ®
        x = torch.randn(batch_size, seq_len, hidden_size)
        time_c = torch.randn(batch_size, hidden_size)
        context_c = torch.randn(batch_size, L_total, hidden_size)
        
        print(f"\nè¾“å…¥å½¢çŠ¶:")
        print(f"  x (åŠ¨ä½œåºåˆ—):    {x.shape}")
        print(f"  time_c (æ—¶é—´):   {time_c.shape}")
        print(f"  context_c (å¤šæ¨¡æ€): {context_c.shape}")
        
        # å‰å‘ä¼ æ’­
        print(f"\n" + "â”€" * 80)
        print("DiTXGateAttnBlock å‰å‘ä¼ æ’­...")
        print(f"  ä½¿ç”¨Gate-Attentionè°ƒåˆ¶cross-attentionè¾“å‡º")
        block.train()
        output = block(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æ£€æŸ¥Gateç»Ÿè®¡ä¿¡æ¯
        gate_stats = block.get_gate_stats()
        if gate_stats:
            print(f"  Gateç»Ÿè®¡:")
            print(f"    - mean: {gate_stats['gate_activation_mean']:.4f}")
            print(f"    - std: {gate_stats['gate_activation_std']:.4f}")
            print(f"    - min: {gate_stats['gate_activation_min']:.4f}")
            print(f"    - max: {gate_stats['gate_activation_max']:.4f}")
        print(f"  âœ… æˆåŠŸ!")
        
        # å‚æ•°ç»Ÿè®¡
        print(f"\n" + "=" * 80)
        print("å‚æ•°ç»Ÿè®¡:")
        print("=" * 80)
        
        params = sum(p.numel() for p in block.parameters())
        print(f"  æ€»å‚æ•°: {params:,}")
        print(f"  Gateç±»å‹: {block.gate_type}")
        
        print(f"\n" + "=" * 80)
        print("âœ… æµ‹è¯•é€šè¿‡!")
        print("=" * 80)

    def test_gradient_flow():
        """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•æ¢¯åº¦æµåŠ¨")
        print("=" * 80)
        
        block = DiTXGateAttnBlock(
            hidden_size=512,
            num_heads=8,
            gate_type='elementwise'
        )
        block.train()
        
        x = torch.randn(2, 32, 512, requires_grad=True)
        time_c = torch.randn(2, 512, requires_grad=True)
        context_c = torch.randn(2, 256, 512, requires_grad=True)
        
        output = block(x, time_c, context_c)
        
        loss = output.sum()
        loss.backward()
        
        print(f"  x.grad is not None: {x.grad is not None}")
        print(f"  time_c.grad is not None: {time_c.grad is not None}")
        print(f"  context_c.grad is not None: {context_c.grad is not None}")
        print(f"  âœ… æ¢¯åº¦æµåŠ¨æ­£å¸¸!")
        print("=" * 80)

    # è¿è¡Œæµ‹è¯•
    torch.manual_seed(42)
    
    test_ditx_gateattn_block()
    test_gradient_flow()
    
    print("\n" + "ğŸ‰" * 40)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ‰" * 40)

