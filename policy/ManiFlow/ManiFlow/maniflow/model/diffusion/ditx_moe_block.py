# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# References:
# DiT: https://github.com/facebookresearch/DiT
# RDT: https://github.com/thu-ml/RoboticsDiffusionTransformer
# --------------------------------------------------------
#
# ä½¿ç”¨è¯´æ˜:
# è¿è¡Œæµ‹è¯•: python ditx-moe_block.py (éœ€å®‰è£…ä¾èµ–: torch, einops, timm)
# --------------------------------------------------------

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.jit import Final
from einops.layers.torch import Rearrange
from timm.models.vision_transformer import Mlp, use_fused_attn
from maniflow.model.diffusion.ditx_block import DiTXBlock

logger = logging.getLogger(__name__)

def modulate(x, shift, scale):
    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)

class AdaptiveLayerNorm(nn.Module):
    def __init__(
        self,
        dim,
        dim_cond,
    ):
        super().__init__()

        self.ln = nn.LayerNorm(dim, elementwise_affine = False)
 
        self.cond_linear = nn.Linear(dim_cond, dim * 2)

        self.cond_modulation = nn.Sequential(
            Rearrange('b d -> b 1 d'),
            nn.SiLU(),
            self.cond_linear
        )

        # Initialize the weights and biases of the conditional linear layer
        nn.init.zeros_(self.cond_linear.weight)
        nn.init.constant_(self.cond_linear.bias[:dim], 1.)
        nn.init.zeros_(self.cond_linear.bias[dim:])

    def forward(
        self,
        x,
        cond = None
    ):
        x = self.ln(x)
        gamma, beta = self.cond_modulation(cond).chunk(2, dim = -1)
        x = x * gamma + beta

        return x


class ModalityMoE(nn.Module):
    """
    æ¨¡æ€çº§åˆ«MoEï¼šæŒ‰æ¨¡æ€ç»„åˆè¿›è¡Œè·¯ç”±ï¼Œæ¯ä¸ªä¸“å®¶åªå¤„ç†ç‰¹å®šæ¨¡æ€ç‰¹å¾
    
    ğŸ”¥ ä¸“å®¶ä¸“ä¸šåŒ–ç­–ç•¥ï¼ˆæ¯ä¸ªä¸“å®¶åªå¤„ç†å…¶å¯¹åº”çš„æ¨¡æ€ç‰¹å¾ï¼‰ï¼š
    - Expert 0: å…¨æ¨¡æ€ç»„åˆ (head + wrist + proprio) - å¤„ç†æ‰€æœ‰tokens
    - Expert 1: å¤´éƒ¨+æœ¬ä½“ä¸“å®¶ (head + proprio) - åªå¤„ç†headå’Œproprioçš„tokens
    - Expert 2: è…•éƒ¨+æœ¬ä½“ä¸“å®¶ (wrist + proprio) - åªå¤„ç†wristå’Œproprioçš„tokens
    - Expert 3+: é¢å¤–ä¸“å®¶ï¼Œé»˜è®¤å¤„ç†å…¨æ¨¡æ€
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    - Gateæ ¹æ®æ¨¡æ€ç»„åˆèšåˆç‰¹å¾é€‰æ‹©ä¸“å®¶
    - ä¸“å®¶åªå¤„ç†å…¶å¯¹åº”çš„æ¨¡æ€tokensï¼Œä¿è¯ä¸“ä¸šåŒ–
    - é¿å…æ— å…³æ¨¡æ€å¹²æ‰°ä¸“å®¶å­¦ä¹ 
    
    Args:
        embed_dim: ç‰¹å¾ç»´åº¦
        num_experts: ä¸“å®¶æ•°é‡ (>=3)
        num_experts_per_tok: æ¯æ¬¡æ¿€æ´»çš„ä¸“å®¶æ•°
        n_shared_experts: å…±äº«ä¸“å®¶æ•°é‡
        aux_loss_alpha: è´Ÿè½½å‡è¡¡æŸå¤±æƒé‡
        use_time_cond: æ˜¯å¦ä½¿ç”¨æ—¶é—´æ¡ä»¶è°ƒåˆ¶
    """
    def __init__(self, embed_dim, num_experts=4, num_experts_per_tok=2, 
                 n_shared_experts=1, aux_loss_alpha=0.01, use_time_cond=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_experts = num_experts
        self.num_experts_per_tok = num_experts_per_tok
        self.n_shared_experts = n_shared_experts
        self.aux_loss_alpha = aux_loss_alpha
        self.use_time_cond = use_time_cond
        
        # æ¨¡æ€çº§åˆ«é—¨æ§ï¼šè¾“å…¥ä¸ºæ¨¡æ€èšåˆç‰¹å¾
        # 3ä¸ªæ¨¡æ€ç»„åˆçš„èšåˆç‰¹å¾ -> ä¸“å®¶é€‰æ‹©
        self.gate_proj = nn.Linear(embed_dim * 3, num_experts)  # 3ç§æ¨¡æ€ç»„åˆ
        
        # æ—¶é—´æ¡ä»¶è°ƒåˆ¶é—¨æ§
        if use_time_cond:
            self.time_gate_modulation = nn.Sequential(
                nn.SiLU(),
                nn.Linear(embed_dim, num_experts * 2)  # scaleå’Œshift
            )
            nn.init.zeros_(self.time_gate_modulation[-1].weight)
            nn.init.zeros_(self.time_gate_modulation[-1].bias)
        
        # ä¸“å®¶ç½‘ç»œï¼šæ¯ä¸ªä¸“å®¶å¤„ç†å®Œæ•´çš„context_c
        mlp_hidden = int(embed_dim * 4)
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, mlp_hidden),
                nn.GELU(approximate="tanh"),
                nn.Linear(mlp_hidden, embed_dim)
            ) for _ in range(num_experts)
        ])
        
        # å…±äº«ä¸“å®¶
        if n_shared_experts > 0:
            self.shared_expert = nn.Sequential(
                nn.Linear(embed_dim, mlp_hidden * n_shared_experts),
                nn.GELU(approximate="tanh"),
                nn.Linear(mlp_hidden * n_shared_experts, embed_dim)
            )
        else:
            self.shared_expert = None
        
        # ç”¨äºè®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.moe_stats = None
        
        self._init_weights()
    
    def _init_weights(self):
        for expert in self.experts:
            nn.init.xavier_uniform_(expert[0].weight)
            nn.init.zeros_(expert[0].bias)
            nn.init.xavier_uniform_(expert[2].weight)
            nn.init.zeros_(expert[2].bias)
        if self.shared_expert is not None:
            nn.init.xavier_uniform_(self.shared_expert[0].weight)
            nn.init.zeros_(self.shared_expert[0].bias)
            nn.init.xavier_uniform_(self.shared_expert[2].weight)
            nn.init.zeros_(self.shared_expert[2].bias)
    
    def forward(self, context_c, time_cond=None, modality_lens=None):
        """
        Args:
            context_c: (B, L_total, D) å¤šæ¨¡æ€ç‰¹å¾åºåˆ—
            time_cond: (B, D) æ—¶é—´æ¡ä»¶åµŒå…¥
            modality_lens: dict å„æ¨¡æ€é•¿åº¦ {'head': L_head, 'wrist': L_wrist, 'proprio': L_proprio}
                          å¦‚æœä¸ºNoneï¼Œåˆ™å‡åˆ†
        Returns:
            output: (B, L_total, D) å¤„ç†åçš„ç‰¹å¾
        """
        B, L, D = context_c.shape
        
        # è§£ææ¨¡æ€é•¿åº¦
        if modality_lens is None:
            # é»˜è®¤å‡åˆ†
            L_head = L_wrist = L // 3
            L_proprio = L - L_head - L_wrist
        else:
            L_head = modality_lens.get('head', 0)
            L_wrist = modality_lens.get('wrist', 0)
            L_proprio = modality_lens.get('proprio', L - L_head - L_wrist)
        
        # åˆ†å‰²æ¨¡æ€ç‰¹å¾
        head_feat = context_c[:, :L_head, :]  # (B, L_head, D)
        wrist_feat = context_c[:, L_head:L_head+L_wrist, :]  # (B, L_wrist, D)
        proprio_feat = context_c[:, L_head+L_wrist:, :]  # (B, L_proprio, D)
        
        # è®¡ç®—æ¨¡æ€ç»„åˆçš„èšåˆç‰¹å¾ï¼ˆç”¨äºé—¨æ§ï¼‰
        # ç»„åˆ1: å…¨æ¨¡æ€ (head + wrist + proprio)
        full_agg = context_c.mean(dim=1)  # (B, D)
        # ç»„åˆ2: å¤´éƒ¨+æœ¬ä½“
        head_proprio_agg = torch.cat([head_feat, proprio_feat], dim=1).mean(dim=1) if L_head > 0 else proprio_feat.mean(dim=1)
        # ç»„åˆ3: è…•éƒ¨+æœ¬ä½“
        wrist_proprio_agg = torch.cat([wrist_feat, proprio_feat], dim=1).mean(dim=1) if L_wrist > 0 else proprio_feat.mean(dim=1)
        
        # æ‹¼æ¥èšåˆç‰¹å¾ç”¨äºé—¨æ§
        gate_input = torch.cat([full_agg, head_proprio_agg, wrist_proprio_agg], dim=-1)  # (B, 3*D)
        
        # è®¡ç®—é—¨æ§åˆ†æ•°
        gate_logits = self.gate_proj(gate_input)  # (B, num_experts)
        
        # æ—¶é—´æ¡ä»¶è°ƒåˆ¶
        if self.use_time_cond and time_cond is not None:
            modulation = self.time_gate_modulation(time_cond)
            scale, shift = modulation.chunk(2, dim=-1)
            gate_logits = gate_logits * (1 + scale) + shift
        
        gate_scores = F.softmax(gate_logits, dim=-1)  # (B, num_experts)
        
        # é€‰æ‹©top-kä¸“å®¶
        topk_weights, topk_indices = torch.topk(gate_scores, k=self.num_experts_per_tok, dim=-1)
        topk_weights = topk_weights / (topk_weights.sum(dim=-1, keepdim=True) + 1e-8)  # å½’ä¸€åŒ–
        
        # ğŸ†• ä¸“å®¶è®¡ç®—ï¼ˆä¸“å®¶åªå¤„ç†ç‰¹å®šæ¨¡æ€ï¼‰
        output = torch.zeros_like(context_c)
        for k in range(self.num_experts_per_tok):
            expert_idx = topk_indices[:, k]  # (B,)
            expert_weight = topk_weights[:, k:k+1].unsqueeze(-1)  # (B, 1, 1)
            
            # å¯¹æ¯ä¸ªbatchæ ·æœ¬åº”ç”¨å¯¹åº”ä¸“å®¶
            for b in range(B):
                idx = expert_idx[b].item()
                
                # ğŸ”¥ æ ¹æ®ä¸“å®¶ç´¢å¼•å†³å®šå¤„ç†å“ªäº›æ¨¡æ€
                # Expert 0: å…¨æ¨¡æ€ (head + wrist + proprio)
                # Expert 1: å¤´éƒ¨+æœ¬ä½“ (head + proprio)
                # Expert 2: è…•éƒ¨+æœ¬ä½“ (wrist + proprio)
                # Expert 3+: é»˜è®¤å¤„ç†å…¨æ¨¡æ€
                
                if idx == 0 or idx >= 3:  # å…¨æ¨¡æ€ä¸“å®¶
                    expert_input = context_c[b]  # (L_total, D)
                    expert_output = self.experts[idx](expert_input)
                    output[b] += expert_weight[b] * expert_output
                    
                elif idx == 1:  # å¤´éƒ¨+æœ¬ä½“ä¸“å®¶
                    if L_head > 0 and L_proprio > 0:
                        # æ‹¼æ¥å¤´éƒ¨å’Œæœ¬ä½“ç‰¹å¾
                        expert_input = torch.cat([head_feat[b], proprio_feat[b]], dim=0)  # (L_head+L_proprio, D)
                        expert_output = self.experts[idx](expert_input)
                        # åˆ†é…å›å¯¹åº”ä½ç½®
                        output[b, :L_head] += expert_weight[b, 0, 0] * expert_output[:L_head]
                        output[b, L_head+L_wrist:] += expert_weight[b, 0, 0] * expert_output[L_head:]
                    elif L_head > 0:  # åªæœ‰å¤´éƒ¨
                        expert_output = self.experts[idx](head_feat[b])
                        output[b, :L_head] += expert_weight[b, 0, 0] * expert_output
                    elif L_proprio > 0:  # åªæœ‰æœ¬ä½“
                        expert_output = self.experts[idx](proprio_feat[b])
                        output[b, L_head+L_wrist:] += expert_weight[b, 0, 0] * expert_output
                        
                elif idx == 2:  # è…•éƒ¨+æœ¬ä½“ä¸“å®¶
                    if L_wrist > 0 and L_proprio > 0:
                        # æ‹¼æ¥è…•éƒ¨å’Œæœ¬ä½“ç‰¹å¾
                        expert_input = torch.cat([wrist_feat[b], proprio_feat[b]], dim=0)  # (L_wrist+L_proprio, D)
                        expert_output = self.experts[idx](expert_input)
                        # åˆ†é…å›å¯¹åº”ä½ç½®
                        output[b, L_head:L_head+L_wrist] += expert_weight[b, 0, 0] * expert_output[:L_wrist]
                        output[b, L_head+L_wrist:] += expert_weight[b, 0, 0] * expert_output[L_wrist:]
                    elif L_wrist > 0:  # åªæœ‰è…•éƒ¨
                        expert_output = self.experts[idx](wrist_feat[b])
                        output[b, L_head:L_head+L_wrist] += expert_weight[b, 0, 0] * expert_output
                    elif L_proprio > 0:  # åªæœ‰æœ¬ä½“
                        expert_output = self.experts[idx](proprio_feat[b])
                        output[b, L_head+L_wrist:] += expert_weight[b, 0, 0] * expert_output
        
        # æ·»åŠ å…±äº«ä¸“å®¶
        if self.shared_expert is not None:
            output = output + self.shared_expert(context_c)
        
        # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
        if self.training and self.aux_loss_alpha > 0:
            # ä¸“å®¶ä½¿ç”¨é¢‘ç‡
            expert_mask = F.one_hot(topk_indices.view(-1), num_classes=self.num_experts).float()
            expert_usage = expert_mask.mean(0)
            # è·¯ç”±æ¦‚ç‡
            router_prob = gate_scores.mean(0)
            # è´Ÿè½½å‡è¡¡æŸå¤±
            aux_loss = (expert_usage * router_prob).sum() * self.num_experts * self.aux_loss_alpha
            
            # è®¡ç®—topk_weightsç»Ÿè®¡ (å¤„ç†batch_size=1çš„æƒ…å†µ)
            topk_mean = topk_weights.mean().detach().item()
            # åªæœ‰å½“æœ‰å¤šä¸ªæ ·æœ¬æ—¶æ‰è®¡ç®—stdï¼Œå¦åˆ™è®¾ä¸º0
            if topk_weights.numel() > 1:
                topk_std = topk_weights.std().detach().item()
            else:
                topk_std = 0.0
            
            self.moe_stats = {
                'aux_loss': aux_loss.detach().item(),
                'expert_usage': expert_usage.detach(),
                'router_scores': router_prob.detach(),
                'topk_weights_mean': topk_mean,
                'topk_weights_std': topk_std,
            }
        
        return output

class CrossAttention(nn.Module):
    """
    A cross-attention layer with flash attention.
    """
    fused_attn: Final[bool]
    def __init__(
            self,
            dim: int,
            num_heads: int = 8,
            qkv_bias: bool = False,
            qk_norm: bool = False,
            attn_drop: float = 0,
            proj_drop: float = 0,
            norm_layer: nn.Module = nn.LayerNorm,
    ) -> None:
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.fused_attn = use_fused_attn()

        self.q = nn.Linear(dim, dim, bias=qkv_bias)
        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
    
    def forward(self, x: torch.Tensor, c: torch.Tensor, 
                mask: None) -> torch.Tensor:
        B, N, C = x.shape
        _, L, _ = c.shape
        q = self.q(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        kv = self.kv(c).reshape(B, L, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        k, v = kv.unbind(0)
        q, k = self.q_norm(q), self.k_norm(k)

        # Prepare attn mask (B, L) to mask the conditioion
        if mask is not None:
            mask = mask.reshape(B, 1, 1, L)
            mask = mask.expand(-1, -1, N, -1)
        
        if self.fused_attn:
            x = F.scaled_dot_product_attention(
                query=q,
                key=k,
                value=v,
                dropout_p=self.attn_drop.p if self.training else 0.,
                attn_mask=mask
            )
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
            if mask is not None:
                attn = attn.masked_fill_(mask.logical_not(), float('-inf'))
            attn = attn.softmax(dim=-1)
            if self.attn_drop.p > 0:
                attn = self.attn_drop(attn)
            x = attn @ v
            
        x = x.permute(0, 2, 1, 3).reshape(B, N, C)
        x = self.proj(x)
        if self.proj_drop.p > 0:
            x = self.proj_drop(x)
        return x


class DiTXMoEBlock(nn.Module):
    """
    DiTX Block with Modality-level Mixture of Experts (MoE).
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. æ¨¡æ€çº§åˆ«è·¯ç”±ï¼šæŒ‰æ¨¡æ€ç»„åˆï¼ˆå…¨æ¨¡æ€/å¤´éƒ¨+æœ¬ä½“/è…•éƒ¨+æœ¬ä½“ï¼‰è¿›è¡Œè·¯ç”±ï¼Œä¿æŒæ¨¡æ€å†…è¯­ä¹‰ä¸€è‡´æ€§
    2. æ—¶é—´æ¡ä»¶æ„ŸçŸ¥ï¼šMoEé—¨æ§æ„ŸçŸ¥æ‰©æ•£æ—¶é—´æ­¥ï¼Œæ ¹æ®å™ªå£°é˜¶æ®µè°ƒæ•´ä¸“å®¶é€‰æ‹©
    3. AdaLNåè°ƒï¼šcontext_cåœ¨è¿›å…¥MoEå‰é€šè¿‡AdaLNæ„ŸçŸ¥æ—¶é—´æ¡ä»¶
    
    Args:
        hidden_size: éšè—å±‚ç»´åº¦
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        mlp_ratio: MLPæ‰©å±•æ¯”ä¾‹
        use_modality_moe: æ˜¯å¦ä½¿ç”¨æ¨¡æ€MoE
        num_experts: MoEä¸“å®¶æ•°é‡
        num_experts_per_tok: æ¯ä¸ªtokenæ¿€æ´»çš„ä¸“å®¶æ•°
        n_shared_experts: å…±äº«ä¸“å®¶æ•°é‡
        moe_aux_loss_alpha: MoEè¾…åŠ©æŸå¤±æƒé‡
        p_drop_attn: Attention dropoutæ¦‚ç‡
        qkv_bias: æ˜¯å¦ä½¿ç”¨QKV bias
        qk_norm: æ˜¯å¦å¯¹Qå’ŒKè¿›è¡Œå½’ä¸€åŒ–
    """
    def __init__(self, 
                hidden_size=768,
                num_heads=12,
                mlp_ratio=4.0,
                
                # MoEé…ç½®
                use_modality_moe=True,
                num_experts=4,                # æ¨¡æ€çº§MoEå»ºè®®4-8ä¸ªä¸“å®¶
                num_experts_per_tok=2,
                n_shared_experts=1,
                moe_aux_loss_alpha=0.01,
                
                # å…¶ä»–å‚æ•°
                p_drop_attn=0.1,
                qkv_bias=False,
                qk_norm=False,
                **block_kwargs):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.use_modality_moe = use_modality_moe

        # Self-Attention
        self.self_attn = nn.MultiheadAttention(
            hidden_size, num_heads, 
            batch_first=True, 
            dropout=p_drop_attn
        )
        
        # â­ æ¨¡æ€çº§åˆ«MoE (æ›¿ä»£tokençº§åˆ«çš„SparseMoeBlock)
        if use_modality_moe:
            self.modality_moe = ModalityMoE(
                embed_dim=hidden_size,
                num_experts=num_experts,
                num_experts_per_tok=num_experts_per_tok,
                n_shared_experts=n_shared_experts,
                aux_loss_alpha=moe_aux_loss_alpha,
                use_time_cond=True  # å¯ç”¨æ—¶é—´æ¡ä»¶æ„ŸçŸ¥
            )
            # context_cçš„AdaLNï¼šè®©MoEè¾“å…¥æ„ŸçŸ¥æ—¶é—´æ¡ä»¶
            self.context_adaln = AdaptiveLayerNorm(dim=hidden_size, dim_cond=hidden_size)
            logger.info(f"[DiTXMoEBlock] Initialized ModalityMoE with {num_experts} experts, "
                       f"top-{num_experts_per_tok}, {n_shared_experts} shared, time_cond=True")
        
        # Cross-Attention
        self.cross_attn = CrossAttention(
            dim=hidden_size, 
            num_heads=num_heads,
            qkv_bias=qkv_bias, 
            qk_norm=qk_norm,
            norm_layer=nn.LayerNorm, 
            **block_kwargs
        )
       
        # MLP
        mlp_hidden_dim = int(hidden_size * mlp_ratio)
        approx_gelu = lambda: nn.GELU(approximate="tanh")
        self.mlp = Mlp(
            in_features=hidden_size, 
            hidden_features=mlp_hidden_dim, 
            act_layer=approx_gelu, 
            drop=0.0
        )

        # Normalization layers
        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.norm3 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)

        # AdaLN modulation
        modulation_size = 9 * hidden_size
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(hidden_size, modulation_size, bias=True)
        )
        
    def forward(self, x, time_c, context_c, attn_mask=None, modality_lens=None):
        """
        Forward pass of the DiTX-MoE block.
        
        Args:
            x: åŠ¨ä½œåºåˆ— (batch_size, seq_length, hidden_size)
            time_c: æ—¶é—´æ­¥åµŒå…¥ (batch_size, hidden_size)
            context_c: å¤šæ¨¡æ€ç‰¹å¾ (batch_size, L_total, hidden_size)
            attn_mask: å¯é€‰çš„æ³¨æ„åŠ›mask
            modality_lens: æ¨¡æ€é•¿åº¦ä¿¡æ¯ {'head': L_head, 'wrist': L_wrist, 'proprio': L_proprio}
        
        Returns:
            x: è¾“å‡ºç‰¹å¾ (batch_size, seq_length, hidden_size)
        """
        # adaLN modulation
        modulation = self.adaLN_modulation(time_c)
        chunks = modulation.chunk(9, dim=-1)
        
        shift_msa, scale_msa, gate_msa = chunks[0], chunks[1], chunks[2]
        shift_cross, scale_cross, gate_cross = chunks[3], chunks[4], chunks[5]
        shift_mlp, scale_mlp, gate_mlp = chunks[6], chunks[7], chunks[8]

        # 1. Self-Attention with adaLN conditioning
        normed_x = modulate(self.norm1(x), shift_msa, scale_msa)
        self_attn_output, _ = self.self_attn(normed_x, normed_x, normed_x, attn_mask=attn_mask)
        x = x + gate_msa.unsqueeze(1) * self_attn_output

        # 2. â­ æ¨¡æ€MoEå¤„ç†å¤šæ¨¡æ€è¾“å…¥ç‰¹å¾
        if self.use_modality_moe:
            # å…ˆé€šè¿‡AdaLNè®©context_cæ„ŸçŸ¥æ—¶é—´æ¡ä»¶
            context_c_normed = self.context_adaln(context_c, time_c)
            # æ¨¡æ€çº§åˆ«MoEå¤„ç†ï¼ˆä¼ å…¥æ—¶é—´æ¡ä»¶å’Œæ¨¡æ€é•¿åº¦ï¼‰
            context_c_processed = self.modality_moe(context_c_normed, time_c, modality_lens)
        else:
            context_c_processed = context_c

        # 3. Cross-Attention with adaLN conditioning
        normed_x_cross = modulate(self.norm2(x), shift_cross, scale_cross)
        cross_attn_output = self.cross_attn(normed_x_cross, context_c_processed, mask=None)
        x = x + gate_cross.unsqueeze(1) * cross_attn_output

        # 4. MLP with adaLN conditioning
        normed_x_mlp = modulate(self.norm3(x), shift_mlp, scale_mlp)
        mlp_output = self.mlp(normed_x_mlp)
        x = x + gate_mlp.unsqueeze(1) * mlp_output

        return x

if __name__ == "__main__":
    """
    æµ‹è¯•DiTX-MoE Blockçš„åŠŸèƒ½
    è¿è¡Œæ–¹å¼: python ditx_moe_block.py
    """
    
    def test_ditx_moe_block():
        """æµ‹è¯•DiTXMoEBlockçš„åŸºæœ¬åŠŸèƒ½"""
        print("=" * 80)
        print("æµ‹è¯• DiTXMoEBlock (æ¨¡æ€çº§åˆ«MoE + æ—¶é—´æ¡ä»¶æ„ŸçŸ¥)")
        print("=" * 80)
        
        # å‚æ•°è®¾ç½®
        batch_size = 4
        seq_len = 50          # åŠ¨ä½œåºåˆ—é•¿åº¦
        hidden_size = 768
        num_heads = 12
        
        # å¤šæ¨¡æ€ç‰¹å¾é•¿åº¦
        L_head = 256          # å¤´éƒ¨ç›¸æœºç‰¹å¾é•¿åº¦
        L_wrist = 256         # è…•éƒ¨ç›¸æœº+è§¦è§‰ç‰¹å¾é•¿åº¦
        L_proprio = 16        # æœ¬ä½“æ„ŸçŸ¥ç‰¹å¾é•¿åº¦
        L_total = L_head + L_wrist + L_proprio
        
        # æ¨¡æ€é•¿åº¦ä¿¡æ¯
        modality_lens = {'head': L_head, 'wrist': L_wrist, 'proprio': L_proprio}
        
        # åˆ›å»ºDiTXMoEBlock
        block_moe = DiTXMoEBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mlp_ratio=4.0,
            use_modality_moe=True,
            num_experts=4,        # æ¨¡æ€çº§MoEå»ºè®®4ä¸ªä¸“å®¶
            num_experts_per_tok=2,
            n_shared_experts=1,
            moe_aux_loss_alpha=0.01,
            p_drop_attn=0.1
        )
        
        # åˆ›å»ºåŸå§‹DiTXBlockä½œä¸ºå¯¹æ¯”
        block_vanilla = DiTXBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mlp_ratio=4.0,
            p_drop_attn=0.1
        )
        
        # è¾“å…¥æ•°æ®
        x = torch.randn(batch_size, seq_len, hidden_size)
        time_c = torch.randn(batch_size, hidden_size)
        context_c = torch.randn(batch_size, L_total, hidden_size)
        
        print(f"\nè¾“å…¥å½¢çŠ¶:")
        print(f"  x (åŠ¨ä½œåºåˆ—):    {x.shape}")
        print(f"  time_c (æ—¶é—´):   {time_c.shape}")
        print(f"  context_c (å¤šæ¨¡æ€): {context_c.shape}")
        print(f"    â””â”€ å¤´éƒ¨: {L_head}, è…•éƒ¨: {L_wrist}, æœ¬ä½“: {L_proprio}")
        
        # å‰å‘ä¼ æ’­ - MoEç‰ˆæœ¬ï¼ˆå¸¦æ¨¡æ€é•¿åº¦ä¿¡æ¯ï¼‰
        print(f"\n" + "â”€" * 80)
        print("DiTXMoEBlock å‰å‘ä¼ æ’­ (æ¨¡æ€çº§åˆ«è·¯ç”±)...")
        block_moe.train()
        output_moe = block_moe(x, time_c, context_c, modality_lens=modality_lens)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output_moe.shape}")
        
        # æ£€æŸ¥MoEç»Ÿè®¡ä¿¡æ¯
        if hasattr(block_moe, 'modality_moe') and block_moe.modality_moe.moe_stats:
            stats = block_moe.modality_moe.moe_stats
            print(f"  MoEç»Ÿè®¡:")
            print(f"    - aux_loss: {stats['aux_loss']:.6f}")
            print(f"    - expert_usage: {stats['expert_usage'].tolist()}")
            print(f"    - topk_weights_mean: {stats['topk_weights_mean']:.4f}")
        print(f"  âœ… æˆåŠŸ!")
        
        # å‰å‘ä¼ æ’­ - åŸå§‹ç‰ˆæœ¬
        print(f"\n" + "â”€" * 80)
        print("DiTXBlock (åŸå§‹) å‰å‘ä¼ æ’­...")
        output_vanilla = block_vanilla(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output_vanilla.shape}")
        print(f"  âœ… æˆåŠŸ!")
        
        # å‚æ•°ç»Ÿè®¡
        print(f"\n" + "=" * 80)
        print("å‚æ•°å¯¹æ¯”:")
        print("=" * 80)
        
        params_moe = sum(p.numel() for p in block_moe.parameters())
        params_vanilla = sum(p.numel() for p in block_vanilla.parameters())
        params_diff = params_moe - params_vanilla
        
        print(f"  DiTXMoEBlock:  {params_moe:,} å‚æ•°")
        print(f"  DiTXBlock:     {params_vanilla:,} å‚æ•°")
        print(f"  å¢åŠ :          {params_diff:,} å‚æ•° (+{params_diff/params_vanilla*100:.1f}%)")
        
        # æ£€æŸ¥MoEæ¨¡å—
        if hasattr(block_moe, 'modality_moe'):
            moe_params = sum(p.numel() for p in block_moe.modality_moe.parameters())
            print(f"\n  ModalityMoEå‚æ•°: {moe_params:,}")
            print(f"    â”œâ”€ ä¸“å®¶æ•°é‡: {block_moe.modality_moe.num_experts}")
            print(f"    â”œâ”€ Top-K: {block_moe.modality_moe.num_experts_per_tok}")
            print(f"    â”œâ”€ å…±äº«ä¸“å®¶: {block_moe.modality_moe.n_shared_experts}")
            print(f"    â””â”€ æ—¶é—´æ¡ä»¶: {block_moe.modality_moe.use_time_cond}")
        
        print(f"\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("=" * 80)


    def test_batch_sizes():
        """æµ‹è¯•ä¸åŒbatch size"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•ä¸åŒBatch Size")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_modality_moe=True,
            num_experts=4,
            num_experts_per_tok=2
        )
        block.eval()
        
        modality_lens = {'head': 128, 'wrist': 112, 'proprio': 16}
        
        for batch_size in [1, 2, 4, 8]:
            x = torch.randn(batch_size, 32, 512)
            time_c = torch.randn(batch_size, 512)
            context_c = torch.randn(batch_size, 256, 512)
            
            with torch.no_grad():
                output = block(x, time_c, context_c, modality_lens=modality_lens)
            
            print(f"  Batch size {batch_size}: {output.shape} âœ…")
        
        print("=" * 80)


    def test_without_moe():
        """æµ‹è¯•å…³é—­MoEçš„æƒ…å†µ"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•å…³é—­MoE (use_modality_moe=False)")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_modality_moe=False
        )
        
        x = torch.randn(2, 32, 512)
        time_c = torch.randn(2, 512)
        context_c = torch.randn(2, 256, 512)
        
        output = block(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  âœ… å…³é—­MoEæ¨¡å¼æ­£å¸¸å·¥ä½œ!")
        print("=" * 80)
    
    
    def test_gradient_flow():
        """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•æ¢¯åº¦æµåŠ¨")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_modality_moe=True,
            num_experts=4,
            num_experts_per_tok=2
        )
        block.train()
        
        x = torch.randn(2, 32, 512, requires_grad=True)
        time_c = torch.randn(2, 512, requires_grad=True)
        context_c = torch.randn(2, 256, 512, requires_grad=True)
        
        modality_lens = {'head': 128, 'wrist': 112, 'proprio': 16}
        output = block(x, time_c, context_c, modality_lens=modality_lens)
        loss = output.sum()
        loss.backward()
        
        print(f"  x.grad is not None: {x.grad is not None}")
        print(f"  time_c.grad is not None: {time_c.grad is not None}")
        print(f"  context_c.grad is not None: {context_c.grad is not None}")
        print(f"  âœ… æ¢¯åº¦æµåŠ¨æ­£å¸¸!")
        print("=" * 80)


    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    torch.manual_seed(42)
    
    test_ditx_moe_block()
    test_batch_sizes()
    test_without_moe()
    test_gradient_flow()
    
    print("\n" + "ğŸ‰" * 40)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ‰" * 40)

