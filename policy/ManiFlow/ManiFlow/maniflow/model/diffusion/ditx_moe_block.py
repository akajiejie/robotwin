# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# References:
# DiT: https://github.com/facebookresearch/DiT
# RDT: https://github.com/thu-ml/RoboticsDiffusionTransformer
# --------------------------------------------------------
#
# ä½¿ç”¨è¯´æ˜:
# è¿è¡Œæµ‹è¯•: python ditx-moe_block.py (éœ€å®‰è£…ä¾èµ–: torch, einops, timm)
# --------------------------------------------------------

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.jit import Final
from einops.layers.torch import Rearrange
from timm.models.vision_transformer import Mlp, use_fused_attn
from maniflow.model.diffusion.ditx_block import DiTXBlock

logger = logging.getLogger(__name__)

def modulate(x, shift, scale):
    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)

class AdaptiveLayerNorm(nn.Module):
    def __init__(
        self,
        dim,
        dim_cond,
    ):
        super().__init__()

        self.ln = nn.LayerNorm(dim, elementwise_affine = False)
 
        self.cond_linear = nn.Linear(dim_cond, dim * 2)

        self.cond_modulation = nn.Sequential(
            Rearrange('b d -> b 1 d'),
            nn.SiLU(),
            self.cond_linear
        )

        # Initialize the weights and biases of the conditional linear layer
        nn.init.zeros_(self.cond_linear.weight)
        nn.init.constant_(self.cond_linear.bias[:dim], 1.)
        nn.init.zeros_(self.cond_linear.bias[dim:])

    def forward(
        self,
        x,
        cond = None
    ):
        x = self.ln(x)
        gamma, beta = self.cond_modulation(cond).chunk(2, dim = -1)
        x = x * gamma + beta

        return x

class CrossAttention(nn.Module):
    """
    A cross-attention layer with flash attention.
    """
    fused_attn: Final[bool]
    def __init__(
            self,
            dim: int,
            num_heads: int = 8,
            qkv_bias: bool = False,
            qk_norm: bool = False,
            attn_drop: float = 0,
            proj_drop: float = 0,
            norm_layer: nn.Module = nn.LayerNorm,
    ) -> None:
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.fused_attn = use_fused_attn()

        self.q = nn.Linear(dim, dim, bias=qkv_bias)
        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
    
    def forward(self, x: torch.Tensor, c: torch.Tensor, 
                mask: None) -> torch.Tensor:
        B, N, C = x.shape
        _, L, _ = c.shape
        q = self.q(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        kv = self.kv(c).reshape(B, L, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        k, v = kv.unbind(0)
        q, k = self.q_norm(q), self.k_norm(k)

        # Prepare attn mask (B, L) to mask the conditioion
        if mask is not None:
            mask = mask.reshape(B, 1, 1, L)
            mask = mask.expand(-1, -1, N, -1)
        
        if self.fused_attn:
            x = F.scaled_dot_product_attention(
                query=q,
                key=k,
                value=v,
                dropout_p=self.attn_drop.p if self.training else 0.,
                attn_mask=mask
            )
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
            if mask is not None:
                attn = attn.masked_fill_(mask.logical_not(), float('-inf'))
            attn = attn.softmax(dim=-1)
            if self.attn_drop.p > 0:
                attn = self.attn_drop(attn)
            x = attn @ v
            
        x = x.permute(0, 2, 1, 3).reshape(B, N, C)
        x = self.proj(x)
        if self.proj_drop.p > 0:
            x = self.proj_drop(x)
        return x


class DiTXMoEBlock(nn.Module):
    """
    DiTX Block with Modality-level Mixture of Experts (MoE).
    
    åœ¨CrossAttentionä¹‹å‰å¯¹å¤šæ¨¡æ€è¾“å…¥ç‰¹å¾(context_c)åº”ç”¨MoEï¼Œ
    è®©ä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæ¨¡æ€ç»„åˆçš„é‡è¦æ€§æƒé‡ã€‚
    
    Args:
        hidden_size: éšè—å±‚ç»´åº¦
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        mlp_ratio: MLPæ‰©å±•æ¯”ä¾‹
        use_modality_moe: æ˜¯å¦ä½¿ç”¨æ¨¡æ€MoE
        num_experts: MoEä¸“å®¶æ•°é‡
        num_experts_per_tok: æ¯ä¸ªtokenæ¿€æ´»çš„ä¸“å®¶æ•°
        n_shared_experts: å…±äº«ä¸“å®¶æ•°é‡
        moe_aux_loss_alpha: MoEè¾…åŠ©æŸå¤±æƒé‡
        p_drop_attn: Attention dropoutæ¦‚ç‡
        qkv_bias: æ˜¯å¦ä½¿ç”¨QKV bias
        qk_norm: æ˜¯å¦å¯¹Qå’ŒKè¿›è¡Œå½’ä¸€åŒ–
    """
    def __init__(self, 
                hidden_size=768,              # éšè—å±‚ç»´åº¦
                num_heads=12,                 # æ³¨æ„åŠ›å¤´æ•°
                mlp_ratio=4.0,               # MLPæ‰©å±•æ¯”ä¾‹
                
                # MoEé…ç½®
                use_modality_moe=True,        # å¯ç”¨æ¨¡æ€MoE
                num_experts=8,                # 8ä¸ªæ¨¡æ€ä¸“å®¶
                num_experts_per_tok=2,        # æ¯æ¬¡æ¿€æ´»2ä¸ªä¸“å®¶
                n_shared_experts=1,           # 1ä¸ªå…±äº«ä¸“å®¶
                moe_aux_loss_alpha=0.01,      # è¾…åŠ©æŸå¤±æƒé‡
                
                # å…¶ä»–å‚æ•°
                p_drop_attn=0.1,              # Attention dropoutæ¦‚ç‡
                qkv_bias=False,               # æ˜¯å¦ä½¿ç”¨QKV bias
                qk_norm=False,                # æ˜¯å¦å¯¹Qå’ŒKè¿›è¡Œå½’ä¸€åŒ–
                **block_kwargs):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.use_modality_moe = use_modality_moe

        # Self-Attention
        self.self_attn = nn.MultiheadAttention(
            hidden_size, num_heads, 
            batch_first=True, 
            dropout=p_drop_attn
        )
        
        # â­ æ¨¡æ€ä¸“å®¶MoE (å¤„ç†å¤šæ¨¡æ€è¾“å…¥context_c)
        if use_modality_moe:
            from maniflow.model.gate.MoEgate import SparseMoeBlock
            self.modality_moe = SparseMoeBlock(
                embed_dim=hidden_size,
                mlp_ratio=mlp_ratio,
                num_experts=num_experts,
                num_experts_per_tok=num_experts_per_tok,
                n_shared_experts=n_shared_experts,
                aux_loss_alpha=moe_aux_loss_alpha
            )
            logger.info(f"[DiTXMoEBlock] Initialized Modality MoE with {num_experts} experts, "
                       f"top-{num_experts_per_tok} per token, {n_shared_experts} shared experts")
        
        # Cross-Attention
        self.cross_attn = CrossAttention(
            dim=hidden_size, 
            num_heads=num_heads,
            qkv_bias=qkv_bias, 
            qk_norm=qk_norm,
            norm_layer=nn.LayerNorm, 
            **block_kwargs
        )
       
        # MLP (ä¿æŒä¸å˜ï¼Œä½¿ç”¨æ ‡å‡†MLP)
        mlp_hidden_dim = int(hidden_size * mlp_ratio)
        approx_gelu = lambda: nn.GELU(approximate="tanh")
        self.mlp = Mlp(
            in_features=hidden_size, 
            hidden_features=mlp_hidden_dim, 
            act_layer=approx_gelu, 
            drop=0.0
        )

        # Normalization layers
        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)  # For self-attention
        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)  # For cross-attention
        self.norm3 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)  # For MLP

        # AdaLN modulation
        modulation_size = 9 * hidden_size
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(hidden_size, modulation_size, bias=True)
        )
        
    def forward(self, x, time_c, context_c, attn_mask=None):
        """
        Forward pass of the DiTX-MoE block.
        
        Args:
            x: åŠ¨ä½œåºåˆ— (batch_size, seq_length, hidden_size)
            time_c: æ—¶é—´æ­¥åµŒå…¥ (batch_size, hidden_size)
            context_c: å¤šæ¨¡æ€ç‰¹å¾ (batch_size, L_total, hidden_size)
                      åŒ…å«: [å¤´éƒ¨è§†è§‰, è…•éƒ¨è§†è§‰+è§¦è§‰, æœ¬ä½“æ„ŸçŸ¥]
            attn_mask: å¯é€‰çš„æ³¨æ„åŠ›mask (batch_size, seq_length, seq_length)
        
        Returns:
            x: è¾“å‡ºç‰¹å¾ (batch_size, seq_length, hidden_size)
        """

        # adaLN modulation for self-attention, cross-attention, and MLP
        modulation = self.adaLN_modulation(time_c)

        # Split into 9 chunks of hidden_size each
        chunks = modulation.chunk(9, dim=-1)
        
        # Self-Attention parameters
        shift_msa, scale_msa, gate_msa = chunks[0], chunks[1], chunks[2]
        
        # Cross-Attention parameters  
        shift_cross, scale_cross, gate_cross = chunks[3], chunks[4], chunks[5]
        
        # MLP parameters
        shift_mlp, scale_mlp, gate_mlp = chunks[6], chunks[7], chunks[8]


        # 1. Self-Attention with adaLN conditioning
        normed_x = modulate(self.norm1(x), shift_msa, scale_msa)
        self_attn_output, _ = self.self_attn(normed_x, normed_x, normed_x, 
                                             attn_mask=attn_mask)
        x = x + gate_msa.unsqueeze(1) * self_attn_output
        

        # 2. â­ æ¨¡æ€MoEå¤„ç†å¤šæ¨¡æ€è¾“å…¥ç‰¹å¾
        if self.use_modality_moe:
            context_c_processed = self.modality_moe(context_c)
        else:
            context_c_processed = context_c
        

        # 3. Cross-Attention with adaLN conditioning
        # ä½¿ç”¨MoEå¤„ç†åçš„å¤šæ¨¡æ€ç‰¹å¾è¿›è¡Œäº¤å‰æ³¨æ„åŠ›
        normed_x_cross = modulate(self.norm2(x), shift_cross, scale_cross)
        cross_attn_output = self.cross_attn(normed_x_cross, context_c_processed, 
                                            mask=None)
        x = x + gate_cross.unsqueeze(1) * cross_attn_output
       

        # 4. MLP with adaLN conditioning (ä¿æŒåŸæœ‰çš„æ ‡å‡†MLP)
        normed_x_mlp = modulate(self.norm3(x), shift_mlp, scale_mlp)
        mlp_output = self.mlp(normed_x_mlp)
        x = x + gate_mlp.unsqueeze(1) * mlp_output

        return x

if __name__ == "__main__":
    """
    æµ‹è¯•DiTX-MoE Blockçš„åŠŸèƒ½
    è¿è¡Œæ–¹å¼: python ditx-moe_block.py
    """
    
    def test_ditx_moe_block():
        """æµ‹è¯•DiTXMoEBlockçš„åŸºæœ¬åŠŸèƒ½"""
        print("=" * 80)
        print("æµ‹è¯• DiTXMoEBlock")
        print("=" * 80)
        
        # å‚æ•°è®¾ç½®
        batch_size = 4
        seq_len = 50          # åŠ¨ä½œåºåˆ—é•¿åº¦
        hidden_size = 768
        num_heads = 12
        
        # å¤šæ¨¡æ€ç‰¹å¾é•¿åº¦
        L_head = 256          # å¤´éƒ¨ç›¸æœºç‰¹å¾é•¿åº¦
        L_wrist = 256         # è…•éƒ¨ç›¸æœº+è§¦è§‰ç‰¹å¾é•¿åº¦
        L_proprio = 16        # æœ¬ä½“æ„ŸçŸ¥ç‰¹å¾é•¿åº¦
        L_total = L_head + L_wrist + L_proprio  # æ€»ç‰¹å¾é•¿åº¦
        
        # åˆ›å»ºDiTXMoEBlock
        block_moe = DiTXMoEBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mlp_ratio=4.0,
            use_modality_moe=True,
            num_experts=8,
            num_experts_per_tok=2,
            n_shared_experts=1,
            moe_aux_loss_alpha=0.01,
            p_drop_attn=0.1
        )
        
        # åˆ›å»ºåŸå§‹DiTXBlockä½œä¸ºå¯¹æ¯”
        block_vanilla = DiTXBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mlp_ratio=4.0,
            p_drop_attn=0.1
        )
        
        # è¾“å…¥æ•°æ®
        x = torch.randn(batch_size, seq_len, hidden_size)
        time_c = torch.randn(batch_size, hidden_size)
        context_c = torch.randn(batch_size, L_total, hidden_size)
        
        print(f"\nè¾“å…¥å½¢çŠ¶:")
        print(f"  x (åŠ¨ä½œåºåˆ—):    {x.shape}")
        print(f"  time_c (æ—¶é—´):   {time_c.shape}")
        print(f"  context_c (å¤šæ¨¡æ€): {context_c.shape}")
        print(f"    â””â”€ å¤´éƒ¨: {L_head}, è…•éƒ¨: {L_wrist}, æœ¬ä½“: {L_proprio}")
        
        # å‰å‘ä¼ æ’­ - MoEç‰ˆæœ¬
        print(f"\n" + "â”€" * 80)
        print("DiTXMoEBlock å‰å‘ä¼ æ’­...")
        block_moe.train()  # è®­ç»ƒæ¨¡å¼æ‰ä¼šè®¡ç®—aux_loss
        output_moe = block_moe(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output_moe.shape}")
        print(f"  âœ… æˆåŠŸ!")
        
        # å‰å‘ä¼ æ’­ - åŸå§‹ç‰ˆæœ¬
        print(f"\n" + "â”€" * 80)
        print("DiTXBlock (åŸå§‹) å‰å‘ä¼ æ’­...")
        output_vanilla = block_vanilla(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output_vanilla.shape}")
        print(f"  âœ… æˆåŠŸ!")
        
        # å‚æ•°ç»Ÿè®¡
        print(f"\n" + "=" * 80)
        print("å‚æ•°å¯¹æ¯”:")
        print("=" * 80)
        
        params_moe = sum(p.numel() for p in block_moe.parameters())
        params_vanilla = sum(p.numel() for p in block_vanilla.parameters())
        params_diff = params_moe - params_vanilla
        
        print(f"  DiTXMoEBlock:  {params_moe:,} å‚æ•°")
        print(f"  DiTXBlock:     {params_vanilla:,} å‚æ•°")
        print(f"  å¢åŠ :          {params_diff:,} å‚æ•° (+{params_diff/params_vanilla*100:.1f}%)")
        
        # æ£€æŸ¥MoEæ¨¡å—
        if hasattr(block_moe, 'modality_moe'):
            moe_params = sum(p.numel() for p in block_moe.modality_moe.parameters())
            print(f"\n  MoEæ¨¡å—å‚æ•°:   {moe_params:,}")
            print(f"    â”œâ”€ ä¸“å®¶æ•°é‡: {block_moe.modality_moe.gate.n_routed_experts}")
            print(f"    â”œâ”€ Top-K: {block_moe.modality_moe.gate.top_k}")
            print(f"    â””â”€ å…±äº«ä¸“å®¶: {block_moe.modality_moe.n_shared_experts}")
        
        print(f"\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("=" * 80)


    def test_batch_sizes():
        """æµ‹è¯•ä¸åŒbatch size"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•ä¸åŒBatch Size")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_modality_moe=True,
            num_experts=8,
            num_experts_per_tok=2
        )
        block.eval()  # æ¨ç†æ¨¡å¼
        
        for batch_size in [1, 2, 4, 8]:
            x = torch.randn(batch_size, 32, 512)
            time_c = torch.randn(batch_size, 512)
            context_c = torch.randn(batch_size, 256, 512)
            
            with torch.no_grad():
                output = block(x, time_c, context_c)
            
            print(f"  Batch size {batch_size}: {output.shape} âœ…")
        
        print("=" * 80)


    def test_without_moe():
        """æµ‹è¯•å…³é—­MoEçš„æƒ…å†µ"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•å…³é—­MoE (use_modality_moe=False)")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_modality_moe=False  # å…³é—­MoE
        )
        
        x = torch.randn(2, 32, 512)
        time_c = torch.randn(2, 512)
        context_c = torch.randn(2, 256, 512)
        
        output = block(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  âœ… å…³é—­MoEæ¨¡å¼æ­£å¸¸å·¥ä½œ!")
        print("=" * 80)


    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    torch.manual_seed(42)
    
    test_ditx_moe_block()
    test_batch_sizes()
    test_without_moe()
    
    print("\n" + "ğŸ‰" * 40)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ‰" * 40)

