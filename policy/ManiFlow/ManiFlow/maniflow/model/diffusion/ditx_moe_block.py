# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# References:
# DiT: https://github.com/facebookresearch/DiT
# RDT: https://github.com/thu-ml/RoboticsDiffusionTransformer
# --------------------------------------------------------
#
# ä½¿ç”¨è¯´æ˜:
# è¿è¡Œæµ‹è¯•: python ditx-moe_block.py (éœ€å®‰è£…ä¾èµ–: torch, einops, timm)
# --------------------------------------------------------

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.jit import Final
from einops.layers.torch import Rearrange
from timm.models.vision_transformer import Mlp, use_fused_attn
from maniflow.model.diffusion.ditx_block import DiTXBlock
from maniflow.model.gate.MoEgate import SparseMoeBlock

logger = logging.getLogger(__name__)


FLASH_ATTN_AVAILABLE = False
try:
    from flash_attn import flash_attn_func
    FLASH_ATTN_AVAILABLE = True
    logger.info("ğŸš€ Flash Attention 2 å·²å¯ç”¨ï¼Œè®­ç»ƒå°†æ˜¾è‘—åŠ é€Ÿï¼")
except ImportError:
    logger.info("âš ï¸ Flash Attention æœªå®‰è£…ï¼Œä½¿ç”¨ PyTorch SDPA åç«¯")

def modulate(x, shift, scale):
    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)


class FlashSelfAttention(nn.Module):
    """
    Self-Attention with Flash Attention 2 support.
    
    å½“ flash-attn å¯ç”¨æ—¶ä½¿ç”¨ Flash Attention 2ï¼Œå¦åˆ™å›é€€åˆ° PyTorch SDPAã€‚
    æ¯” nn.MultiheadAttention æ›´å¿«ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿åºåˆ—ä¸Šã€‚
    """
    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = False,
        qk_norm: bool = False,
        attn_drop: float = 0.,
        proj_drop: float = 0.,
        norm_layer: nn.Module = nn.LayerNorm,
    ):
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        
        # QKV projection
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        
        self.use_flash_attn = FLASH_ATTN_AVAILABLE
        
    def forward(self, x: torch.Tensor, attn_mask=None):
        """
        Args:
            x: Input tensor of shape (B, N, C)
            attn_mask: Optional attention mask (not supported with Flash Attention)
        
        Returns:
            output: (B, N, C)
            attn_weights: None (Flash Attention doesn't return weights)
        """
        B, N, C = x.shape
        
        # QKV projection: (B, N, 3*C) -> (B, N, 3, num_heads, head_dim)
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        
        if self.use_flash_attn and x.is_cuda and x.dtype in [torch.float16, torch.bfloat16]:
            # ğŸš€ Flash Attention 2 è·¯å¾„
            # flash_attn_func éœ€è¦ (B, N, num_heads, head_dim) æ ¼å¼
            q, k, v = qkv.unbind(2)  # 3 x (B, N, num_heads, head_dim)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)
            
            # Flash Attention (è‡ªåŠ¨å¤„ç† causal=False)
            dropout_p = self.attn_drop.p if self.training else 0.
            out = flash_attn_func(q, k, v, dropout_p=dropout_p, causal=False)
            # out: (B, N, num_heads, head_dim)
            
        else:
            # PyTorch SDPA åç«¯ï¼ˆæ”¯æŒ FP32ï¼‰
            qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)
            q, k, v = qkv.unbind(0)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)
            
            # Scaled dot-product attention
            out = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=attn_mask,
                dropout_p=self.attn_drop.p if self.training else 0.,
            )
            # out: (B, num_heads, N, head_dim) -> (B, N, num_heads, head_dim)
            out = out.transpose(1, 2)
        
        # Reshape and project
        out = out.reshape(B, N, C)
        out = self.proj(out)
        out = self.proj_drop(out)
        
        return out, None  # è¿”å› None ä½œä¸º attn_weightsï¼Œä¿æŒæ¥å£å…¼å®¹


class AdaptiveLayerNorm(nn.Module):
    def __init__(
        self,
        dim,
        dim_cond,
    ):
        super().__init__()

        self.ln = nn.LayerNorm(dim, elementwise_affine = False)
 
        self.cond_linear = nn.Linear(dim_cond, dim * 2)

        self.cond_modulation = nn.Sequential(
            Rearrange('b d -> b 1 d'),
            nn.SiLU(),
            self.cond_linear
        )

        # Initialize the weights and biases of the conditional linear layer
        nn.init.zeros_(self.cond_linear.weight)
        nn.init.constant_(self.cond_linear.bias[:dim], 1.)
        nn.init.zeros_(self.cond_linear.bias[dim:])

    def forward(
        self,
        x,
        cond = None
    ):
        x = self.ln(x)
        gamma, beta = self.cond_modulation(cond).chunk(2, dim = -1)
        x = x * gamma + beta

        return x

class CrossAttention(nn.Module):
    """
    Cross-attention layer with flash attention and optional gate mechanism.
    
    æ”¯æŒä¸¤ç§é—¨æ§æ¨¡å¼ï¼ˆå‚è€ƒQwen3çš„gated attentionï¼‰ï¼š
    - 'none': æ— é—¨æ§ï¼ˆæ ‡å‡†cross-attentionï¼‰
    - 'headwise': æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸€ä¸ªgateå€¼ï¼ˆè½»é‡çº§ï¼‰
    - 'elementwise': æ¯ä¸ªå…ƒç´ ä¸€ä¸ªgateå€¼ï¼ˆæœ€ç»†ç²’åº¦ï¼Œå‚è€ƒQwen3ï¼‰
    
    Args:
        gate_type: é—¨æ§ç±»å‹ ('none', 'headwise', 'elementwise')
    """
    fused_attn: Final[bool]
    def __init__(
            self,
            dim: int,
            num_heads: int = 8,
            qkv_bias: bool = False,
            qk_norm: bool = False,
            attn_drop: float = 0,
            proj_drop: float = 0,
            norm_layer: nn.Module = nn.LayerNorm,
            gate_type: str = 'none',  # ğŸ”¥ æ–°å¢ï¼šgate-attentionç±»å‹
    ) -> None:
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.fused_attn = use_fused_attn()
        self.use_flash_attn = FLASH_ATTN_AVAILABLE
        self.gate_type = gate_type

        # ğŸ”¥ Query projection with optional gateï¼ˆå‚è€ƒQwen3ï¼‰
        if gate_type == 'headwise':
            # æ¯ä¸ªå¤´ä¸€ä¸ªgate: q_dim + num_heads
            self.q = nn.Linear(dim, dim + num_heads, bias=qkv_bias)
        elif gate_type == 'elementwise':
            # æ¯ä¸ªå…ƒç´ ä¸€ä¸ªgate: q_dim * 2ï¼ˆä¸Qwen3ä¸€è‡´ï¼‰
            self.q = nn.Linear(dim, dim * 2, bias=qkv_bias)
        else:
            # æ ‡å‡†query
            self.q = nn.Linear(dim, dim, bias=qkv_bias)
        
        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        
        if gate_type != 'none':
            logger.info(f"[CrossAttention] ğŸ”¥ å¯ç”¨Gate-Attentionæœºåˆ¶: {gate_type}ï¼ˆå‚è€ƒQwen3ï¼‰")
    
    def forward(self, x: torch.Tensor, c: torch.Tensor, 
                mask: None) -> torch.Tensor:
        B, N, C = x.shape
        _, L, _ = c.shape
        
        # ğŸ”¥ Query projection with gate extractionï¼ˆå‚è€ƒQwen3å®ç°ï¼‰
        q_output = self.q(x)
        
        if self.gate_type == 'headwise':
            # Headwise gate: æ¯ä¸ªå¤´ä¸€ä¸ªgateå€¼
            # q_output: (B, N, dim + num_heads)
            q_output = q_output.view(B, N, self.num_heads, -1)
            q, gate_score = torch.split(q_output, [self.head_dim, 1], dim=-1)
            # gate_score: (B, N, num_heads, 1)
            q = q.permute(0, 2, 1, 3)  # (B, num_heads, N, head_dim)
            
        elif self.gate_type == 'elementwise':
            # Elementwise gate: æ¯ä¸ªå…ƒç´ ä¸€ä¸ªgateå€¼ï¼ˆä¸Qwen3ä¸€è‡´ï¼‰
            # q_output: (B, N, dim * 2)
            q_output = q_output.view(B, N, self.num_heads, -1)
            q, gate_score = torch.split(q_output, [self.head_dim, self.head_dim], dim=-1)
            # gate_score: (B, N, num_heads, head_dim)
            q = q.permute(0, 2, 1, 3)  # (B, num_heads, N, head_dim)
            
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šæ— gate
            q = q_output.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
            gate_score = None
        
        # Key-Value projection
        kv = self.kv(c).reshape(B, L, 2, self.num_heads, self.head_dim)
        k, v = kv.unbind(2)  # k, v: (B, L, num_heads, head_dim)
        
        # ğŸš€ Flash Attention è·¯å¾„
        if self.use_flash_attn and x.is_cuda and x.dtype in [torch.float16, torch.bfloat16] and mask is None:
            # Flash Attention éœ€è¦ (B, N, num_heads, head_dim) æ ¼å¼
            # q å½“å‰æ˜¯ (B, num_heads, N, head_dim)ï¼Œéœ€è¦è½¬æ¢
            q = q.transpose(1, 2)  # (B, N, num_heads, head_dim)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)
            
            # Flash Attention cross-attention
            dropout_p = self.attn_drop.p if self.training else 0.
            attn_output = flash_attn_func(q, k, v, dropout_p=dropout_p, causal=False)
            # attn_output: (B, N, num_heads, head_dim)
            
        else:
            # PyTorch SDPA åç«¯
            # è½¬æ¢ k, v åˆ° (B, num_heads, L, head_dim)
            k = k.permute(0, 2, 1, 3)
            v = v.permute(0, 2, 1, 3)
            
            # QK Normalization
            q, k = self.q_norm(q), self.k_norm(k)

            # Prepare attn mask (B, L) to mask the condition
            if mask is not None:
                mask = mask.reshape(B, 1, 1, L)
                mask = mask.expand(-1, -1, N, -1)
            
            # Attention computation
            if self.fused_attn:
                attn_output = F.scaled_dot_product_attention(
                    query=q,
                    key=k,
                    value=v,
                    dropout_p=self.attn_drop.p if self.training else 0.,
                    attn_mask=mask
                )
            else:
                q = q * self.scale
                attn = q @ k.transpose(-2, -1)
                if mask is not None:
                    attn = attn.masked_fill_(mask.logical_not(), float('-inf'))
                attn = attn.softmax(dim=-1)
                if self.attn_drop.p > 0:
                    attn = self.attn_drop(attn)
                attn_output = attn @ v
            
            # attn_output: (B, num_heads, N, head_dim) -> (B, N, num_heads, head_dim)
            attn_output = attn_output.transpose(1, 2)
        
        # ğŸ”¥ Gate-Attention: ç”¨sigmoid(gate)è°ƒåˆ¶attentionè¾“å‡ºï¼ˆå‚è€ƒQwen3ï¼‰
        if gate_score is not None:
            attn_output = attn_output * torch.sigmoid(gate_score)
        
        # Reshape and project
        attn_output = attn_output.reshape(B, N, C)
        attn_output = self.proj(attn_output)
        if self.proj_drop.p > 0:
            attn_output = self.proj_drop(attn_output)
        
        return attn_output


class DiTXMoEBlock(nn.Module):
    """
    DiTX Block with Token-level Mixture of Experts (MoE) and Gate-Attention.
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. Tokençº§åˆ«è·¯ç”±ï¼šæ¯ä¸ªtokenç‹¬ç«‹é€‰æ‹©ä¸“å®¶ï¼Œç»†ç²’åº¦çš„ç‰¹å¾å¤„ç†
    2. æ—¶é—´æ¡ä»¶æ„ŸçŸ¥ï¼šMoEé—¨æ§æ„ŸçŸ¥æ‰©æ•£æ—¶é—´æ­¥ï¼Œæ ¹æ®å™ªå£°é˜¶æ®µè°ƒæ•´ä¸“å®¶é€‰æ‹©
    3. ä¸“å®¶è‡ªåŠ¨å­¦ä¹ ï¼šåœ¨ä¸åŒæ—¶é—´æ­¥ä¸‹å…³æ³¨ä¸åŒæ¨¡æ€çš„tokenç‰¹å¾
    4. AdaLNåè°ƒï¼šcontext_cåœ¨è¿›å…¥MoEå‰é€šè¿‡AdaLNæ„ŸçŸ¥æ—¶é—´æ¡ä»¶
    5. Gate-Attentionï¼šCross-attentionè¾“å‡ºé€šè¿‡å¯å­¦ä¹ çš„gateè°ƒåˆ¶ï¼ˆå‚è€ƒQwen3ï¼‰
    
    Args:
        hidden_size: éšè—å±‚ç»´åº¦
        num_heads: æ³¨æ„åŠ›å¤´æ•°
        mlp_ratio: MLPæ‰©å±•æ¯”ä¾‹
        use_token_moe: æ˜¯å¦ä½¿ç”¨tokençº§MoE
        num_experts: MoEä¸“å®¶æ•°é‡
        num_experts_per_tok: æ¯ä¸ªtokenæ¿€æ´»çš„ä¸“å®¶æ•°
        n_shared_experts: å…±äº«ä¸“å®¶æ•°é‡
        moe_aux_loss_alpha: MoEè¾…åŠ©æŸå¤±æƒé‡
        enable_grad_accumulation: æ˜¯å¦å¯ç”¨æ¢¯åº¦ç´¯ç§¯å‹å¥½æ¨¡å¼
        gate_type: Gate-Attentionç±»å‹ ('none', 'headwise', 'elementwise')
        p_drop_attn: Attention dropoutæ¦‚ç‡
        qkv_bias: æ˜¯å¦ä½¿ç”¨QKV bias
        qk_norm: æ˜¯å¦å¯¹Qå’ŒKè¿›è¡Œå½’ä¸€åŒ–
    """
    def __init__(self, 
                hidden_size=768,
                num_heads=12,
                mlp_ratio=4.0,
                
                # MoEé…ç½®
                use_token_moe=True,           # ğŸ”¥ æ”¹åï¼šå¼ºè°ƒtokençº§åˆ«
                num_experts=8,                # Tokençº§MoEå»ºè®®8-16ä¸ªä¸“å®¶ï¼ˆæ¯”æ¨¡æ€çº§æ›´å¤šï¼‰
                num_experts_per_tok=2,
                n_shared_experts=1,
                moe_aux_loss_alpha=0.01,
                enable_grad_accumulation=False,  # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯æ”¯æŒ
                
                # Gate-Attentioné…ç½®
                gate_type='elementwise',      # ğŸ”¥ 'none', 'headwise', 'elementwise'
                
                # å…¶ä»–å‚æ•°
                p_drop_attn=0.1,
                qkv_bias=False,
                qk_norm=False,
                **block_kwargs):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.use_token_moe = use_token_moe
        self.enable_grad_accumulation = enable_grad_accumulation

        # ğŸš€ Self-Attention with Flash Attention support
        self.self_attn = FlashSelfAttention(
            dim=hidden_size,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_norm=qk_norm,
            attn_drop=p_drop_attn,
            proj_drop=0.,
            norm_layer=nn.LayerNorm,
        )
        
        # â­ Tokençº§åˆ«MoEï¼šæ¯ä¸ªtokenç‹¬ç«‹è·¯ç”±ï¼Œä¸“å®¶è‡ªåŠ¨å­¦ä¹ ç‰¹å¾æ¨¡å¼
        if use_token_moe:
            self.token_moe = SparseMoeBlock(
                embed_dim=hidden_size,
                mlp_ratio=mlp_ratio,
                num_experts=num_experts,
                num_experts_per_tok=num_experts_per_tok,
                n_shared_experts=n_shared_experts,
                aux_loss_alpha=moe_aux_loss_alpha,
                use_time_cond=True,  # ğŸ”¥ å¯ç”¨æ—¶é—´æ¡ä»¶æ„ŸçŸ¥
                enable_grad_accumulation=enable_grad_accumulation
            )
            # context_cçš„AdaLNï¼šè®©MoEè¾“å…¥æ„ŸçŸ¥æ—¶é—´æ¡ä»¶
            self.context_adaln = AdaptiveLayerNorm(dim=hidden_size, dim_cond=hidden_size)
            logger.info(f"[DiTXMoEBlock] ğŸ”¥ Initialized Token-level MoE with {num_experts} experts, "
                       f"top-{num_experts_per_tok}, {n_shared_experts} shared, time_cond=True, "
                       f"grad_accum={enable_grad_accumulation}")
        
        # Cross-Attention with Gate-Attention
        self.cross_attn = CrossAttention(
            dim=hidden_size, 
            num_heads=num_heads,
            qkv_bias=qkv_bias, 
            qk_norm=qk_norm,
            norm_layer=nn.LayerNorm,
            gate_type=gate_type,  # ğŸ”¥ ä¼ é€’gate-attentioné…ç½®
            **block_kwargs
        )
       
        # MLP
        mlp_hidden_dim = int(hidden_size * mlp_ratio)
        approx_gelu = lambda: nn.GELU(approximate="tanh")
        self.mlp = Mlp(
            in_features=hidden_size, 
            hidden_features=mlp_hidden_dim, 
            act_layer=approx_gelu, 
            drop=0.0
        )

        # Normalization layers
        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.norm3 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)

        # AdaLN modulation
        modulation_size = 9 * hidden_size
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(hidden_size, modulation_size, bias=True)
        )
    
    def reset_moe_accumulation(self):
        """é‡ç½®MoEçš„ç´¯ç§¯ç»Ÿè®¡ï¼ˆåœ¨optimizer.step()åè°ƒç”¨ï¼‰"""
        if self.use_token_moe and self.enable_grad_accumulation:
            self.token_moe.reset_gate_accumulation()
    
    def get_moe_stats(self):
        """è·å–MoEç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºwandbè®°å½•ï¼‰"""
        if self.use_token_moe and hasattr(self.token_moe, 'moe_stats'):
            return self.token_moe.moe_stats
        return None
        
    def forward(self, x, time_c, context_c, attn_mask=None, modality_lens=None):
        """
        Forward pass of the DiTX-MoE block.
        
        Args:
            x: åŠ¨ä½œåºåˆ— (batch_size, seq_length, hidden_size)
            time_c: æ—¶é—´æ­¥åµŒå…¥ (batch_size, hidden_size)
            context_c: å¤šæ¨¡æ€ç‰¹å¾ (batch_size, L_total, hidden_size)
                      åŒ…å«æ‰€æœ‰æ¨¡æ€çš„token: [head_tokens, wrist_tokens, tactile_tokens, proprio_tokens]
            attn_mask: å¯é€‰çš„æ³¨æ„åŠ›mask
            modality_lens: æ¨¡æ€é•¿åº¦ä¿¡æ¯ï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼Œä½†tokençº§MoEä¸éœ€è¦ï¼‰
        
        Returns:
            x: è¾“å‡ºç‰¹å¾ (batch_size, seq_length, hidden_size)
        """
        # adaLN modulation
        modulation = self.adaLN_modulation(time_c)
        chunks = modulation.chunk(9, dim=-1)
        
        shift_msa, scale_msa, gate_msa = chunks[0], chunks[1], chunks[2]
        shift_cross, scale_cross, gate_cross = chunks[3], chunks[4], chunks[5]
        shift_mlp, scale_mlp, gate_mlp = chunks[6], chunks[7], chunks[8]

        # 1. Self-Attention with adaLN conditioning (ğŸš€ Flash Attention)
        normed_x = modulate(self.norm1(x), shift_msa, scale_msa)
        self_attn_output, _ = self.self_attn(normed_x, attn_mask=attn_mask)
        x = x + gate_msa.unsqueeze(1) * self_attn_output

        # 2. â­ Tokençº§åˆ«MoEå¤„ç†å¤šæ¨¡æ€è¾“å…¥ç‰¹å¾
        # æ¯ä¸ªtokenç‹¬ç«‹è·¯ç”±ï¼Œä¸“å®¶è‡ªåŠ¨å­¦ä¹ åœ¨ä¸åŒæ—¶é—´æ­¥ä¸‹å…³æ³¨ä»€ä¹ˆç‰¹å¾
        if self.use_token_moe:
            # å…ˆé€šè¿‡AdaLNè®©context_cæ„ŸçŸ¥æ—¶é—´æ¡ä»¶
            context_c_normed = self.context_adaln(context_c, time_c)
            # Tokençº§åˆ«MoEå¤„ç†ï¼š(B, L_total, D) -> (B, L_total, D)
            # æ¯ä¸ªtokenç‹¬ç«‹é€‰æ‹©ä¸“å®¶ï¼Œé—¨æ§ç”±æ—¶é—´æ¡ä»¶è°ƒåˆ¶
            context_c_processed = self.token_moe(context_c_normed, time_c)
        else:
            context_c_processed = context_c

        # 3. Cross-Attention with adaLN conditioning
        normed_x_cross = modulate(self.norm2(x), shift_cross, scale_cross)
        cross_attn_output = self.cross_attn(normed_x_cross, context_c_processed, mask=None)
        x = x + gate_cross.unsqueeze(1) * cross_attn_output

        # 4. MLP with adaLN conditioning
        normed_x_mlp = modulate(self.norm3(x), shift_mlp, scale_mlp)
        mlp_output = self.mlp(normed_x_mlp)
        x = x + gate_mlp.unsqueeze(1) * mlp_output

        return x

if __name__ == "__main__":
    """
    æµ‹è¯•DiTX-MoE Blockçš„åŠŸèƒ½
    è¿è¡Œæ–¹å¼: python ditx_moe_block.py
    """
    
    def test_ditx_moe_block():
        """æµ‹è¯•DiTXMoEBlockçš„åŸºæœ¬åŠŸèƒ½"""
        print("=" * 80)
        print("æµ‹è¯• DiTXMoEBlock (Tokençº§åˆ«MoE + æ—¶é—´æ¡ä»¶æ„ŸçŸ¥)")
        print("=" * 80)
        
        # å‚æ•°è®¾ç½®
        batch_size = 4
        seq_len = 50          # åŠ¨ä½œåºåˆ—é•¿åº¦
        hidden_size = 768
        num_heads = 12
        
        # å¤šæ¨¡æ€ç‰¹å¾é•¿åº¦ï¼ˆçœŸå®åœºæ™¯ï¼š1180 tokensï¼‰
        L_head = 392          # å¤´éƒ¨ç›¸æœº: 1ç›¸æœº Ã— 2T Ã— 196patches
        L_wrist = 784         # è…•éƒ¨ç›¸æœº: 2ç›¸æœº Ã— 2T Ã— 196patches  
        L_tactile = 2         # è§¦è§‰ä¼ æ„Ÿå™¨: 2ä¼ æ„Ÿå™¨ Ã— 1patch
        L_proprio = 2         # æœ¬ä½“æ„ŸçŸ¥: 2æ—¶é—´æ­¥
        L_total = L_head + L_wrist + L_tactile + L_proprio  # 1180 tokens
        
        # æ¨¡æ€é•¿åº¦ä¿¡æ¯ï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰
        modality_lens = {
            'head': L_head, 
            'wrist': L_wrist + L_tactile,  # è…•éƒ¨è§†è§‰+è§¦è§‰
            'proprio': L_proprio
        }
        
        # åˆ›å»ºDiTXMoEBlock (Tokençº§MoE + Gate-Attention)
        block_moe = DiTXMoEBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mlp_ratio=4.0,
            use_token_moe=True,
            num_experts=8,        # ğŸ”¥ Tokençº§MoEå»ºè®®æ›´å¤šä¸“å®¶
            num_experts_per_tok=2,
            n_shared_experts=1,
            moe_aux_loss_alpha=0.01,
            gate_type='headwise',  # ğŸ”¥ Gate-Attentionï¼ˆå‚è€ƒQwen3ï¼‰
            p_drop_attn=0.1
        )
        
        # åˆ›å»ºåŸå§‹DiTXBlockä½œä¸ºå¯¹æ¯”
        block_vanilla = DiTXBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            mlp_ratio=4.0,
            p_drop_attn=0.1
        )
        
        # è¾“å…¥æ•°æ®
        x = torch.randn(batch_size, seq_len, hidden_size)
        time_c = torch.randn(batch_size, hidden_size)
        context_c = torch.randn(batch_size, L_total, hidden_size)
        
        print(f"\nè¾“å…¥å½¢çŠ¶:")
        print(f"  x (åŠ¨ä½œåºåˆ—):    {x.shape}")
        print(f"  time_c (æ—¶é—´):   {time_c.shape}")
        print(f"  context_c (å¤šæ¨¡æ€): {context_c.shape}")
        print(f"    â””â”€ å¤´éƒ¨: {L_head}, è…•éƒ¨: {L_wrist}, æœ¬ä½“: {L_proprio}")
        
        # å‰å‘ä¼ æ’­ - Tokençº§MoE + Gate-Attentionç‰ˆæœ¬
        print(f"\n" + "â”€" * 80)
        print("DiTXMoEBlock å‰å‘ä¼ æ’­ (Tokençº§åˆ«è·¯ç”± + Gate-Attention)...")
        print(f"  ğŸ”¥ æ¯ä¸ªtokenç‹¬ç«‹é€‰æ‹©ä¸“å®¶ï¼Œä¸“å®¶è‡ªåŠ¨å­¦ä¹ åœ¨ä¸åŒæ—¶é—´æ­¥ä¸‹å…³æ³¨ä»€ä¹ˆç‰¹å¾")
        print(f"  ğŸ”¥ Gate-Attentionè°ƒåˆ¶cross-attentionè¾“å‡ºï¼ˆå‚è€ƒQwen3ï¼‰")
        block_moe.train()
        output_moe = block_moe(x, time_c, context_c, modality_lens=modality_lens)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output_moe.shape}")
        
        # æ£€æŸ¥MoEç»Ÿè®¡ä¿¡æ¯
        moe_stats = block_moe.get_moe_stats()
        if moe_stats:
            print(f"  MoEç»Ÿè®¡:")
            print(f"    - aux_loss: {moe_stats['aux_loss']:.6f}")
            print(f"    - expert_usage: {moe_stats['expert_usage'].cpu().numpy()}")
            print(f"    - topk_weights_mean: {moe_stats['topk_weights_mean']:.4f}")
            print(f"    - topk_weights_std: {moe_stats['topk_weights_std']:.4f}")
        print(f"  âœ… æˆåŠŸ!")
        
        # å‰å‘ä¼ æ’­ - åŸå§‹ç‰ˆæœ¬
        print(f"\n" + "â”€" * 80)
        print("DiTXBlock (åŸå§‹) å‰å‘ä¼ æ’­...")
        output_vanilla = block_vanilla(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output_vanilla.shape}")
        print(f"  âœ… æˆåŠŸ!")
        
        # å‚æ•°ç»Ÿè®¡
        print(f"\n" + "=" * 80)
        print("å‚æ•°å¯¹æ¯”:")
        print("=" * 80)
        
        params_moe = sum(p.numel() for p in block_moe.parameters())
        params_vanilla = sum(p.numel() for p in block_vanilla.parameters())
        params_diff = params_moe - params_vanilla
        
        print(f"  DiTXMoEBlock:  {params_moe:,} å‚æ•°")
        print(f"  DiTXBlock:     {params_vanilla:,} å‚æ•°")
        print(f"  å¢åŠ :          {params_diff:,} å‚æ•° (+{params_diff/params_vanilla*100:.1f}%)")
        
        # æ£€æŸ¥MoEæ¨¡å—
        if hasattr(block_moe, 'token_moe'):
            moe_params = sum(p.numel() for p in block_moe.token_moe.parameters())
            print(f"\n  Token-level MoEå‚æ•°: {moe_params:,}")
            print(f"    â”œâ”€ ä¸“å®¶æ•°é‡: {block_moe.token_moe.num_experts}")
            print(f"    â”œâ”€ Top-K: {block_moe.token_moe.num_experts_per_tok}")
            print(f"    â”œâ”€ å…±äº«ä¸“å®¶: {block_moe.token_moe.n_shared_experts}")
            print(f"    â”œâ”€ æ—¶é—´æ¡ä»¶: {block_moe.token_moe.use_time_cond}")
            print(f"    â””â”€ æ¢¯åº¦ç´¯ç§¯: {block_moe.token_moe.enable_grad_accumulation}")
        
        # æ£€æŸ¥Gate-Attention
        if hasattr(block_moe, 'cross_attn'):
            cross_attn_params = sum(p.numel() for p in block_moe.cross_attn.parameters())
            print(f"\n  Cross-Attentionå‚æ•°: {cross_attn_params:,}")
            print(f"    â”œâ”€ æ³¨æ„åŠ›å¤´æ•°: {block_moe.cross_attn.num_heads}")
            print(f"    â”œâ”€ Headç»´åº¦: {block_moe.cross_attn.head_dim}")
            print(f"    â””â”€ Gateç±»å‹: {block_moe.cross_attn.gate_type} ğŸ”¥")
        
        print(f"\n" + "=" * 80)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("=" * 80)


    def test_batch_sizes():
        """æµ‹è¯•ä¸åŒbatch size"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•ä¸åŒBatch Size (Tokençº§MoE)")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_token_moe=True,
            num_experts=8,
            num_experts_per_tok=2
        )
        block.eval()
        
        modality_lens = {'head': 128, 'wrist': 112, 'proprio': 16}
        
        for batch_size in [1, 2, 4, 8]:
            x = torch.randn(batch_size, 32, 512)
            time_c = torch.randn(batch_size, 512)
            context_c = torch.randn(batch_size, 256, 512)
            
            with torch.no_grad():
                output = block(x, time_c, context_c, modality_lens=modality_lens)
            
            print(f"  Batch size {batch_size}: {output.shape} âœ…")
        
        print("=" * 80)


    def test_without_moe():
        """æµ‹è¯•å…³é—­MoEçš„æƒ…å†µ"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•å…³é—­MoE (use_token_moe=False)")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_token_moe=False
        )
        
        x = torch.randn(2, 32, 512)
        time_c = torch.randn(2, 512)
        context_c = torch.randn(2, 256, 512)
        
        output = block(x, time_c, context_c)
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  âœ… å…³é—­MoEæ¨¡å¼æ­£å¸¸å·¥ä½œ!")
        print("=" * 80)
    
    
    def test_gradient_flow():
        """æµ‹è¯•æ¢¯åº¦æµåŠ¨å’ŒMoEè¾…åŠ©æŸå¤±"""
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•æ¢¯åº¦æµåŠ¨å’ŒMoEè¾…åŠ©æŸå¤±")
        print("=" * 80)
        
        block = DiTXMoEBlock(
            hidden_size=512,
            num_heads=8,
            use_token_moe=True,
            num_experts=8,
            num_experts_per_tok=2,
            moe_aux_loss_alpha=0.01
        )
        block.train()
        
        x = torch.randn(2, 32, 512, requires_grad=True)
        time_c = torch.randn(2, 512, requires_grad=True)
        context_c = torch.randn(2, 256, 512, requires_grad=True)
        
        modality_lens = {'head': 128, 'wrist': 112, 'proprio': 16}
        output = block(x, time_c, context_c, modality_lens=modality_lens)
        
        # æ£€æŸ¥MoEç»Ÿè®¡
        moe_stats = block.get_moe_stats()
        print(f"  MoE aux_loss: {moe_stats['aux_loss'] if moe_stats else 0.0:.6f}")
        
        loss = output.sum()
        loss.backward()
        
        print(f"  x.grad is not None: {x.grad is not None}")
        print(f"  time_c.grad is not None: {time_c.grad is not None}")
        print(f"  context_c.grad is not None: {context_c.grad is not None}")
        print(f"  âœ… æ¢¯åº¦æµåŠ¨æ­£å¸¸!")
        print("=" * 80)


    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    torch.manual_seed(42)
    
    test_ditx_moe_block()
    test_batch_sizes()
    test_without_moe()
    test_gradient_flow()
    
    print("\n" + "ğŸ‰" * 40)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ‰" * 40)

