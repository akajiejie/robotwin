# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# References:
# DiT: https://github.com/facebookresearch/DiT
# RDT: https://github.com/thu-ml/RoboticsDiffusionTransformer
# --------------------------------------------------------

import re
import logging
from typing import Union, Tuple
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.vision_transformer import Mlp, RmsNorm
from maniflow.model.diffusion.positional_embedding import SinusoidalPosEmb
from maniflow.model.diffusion.ditx_moe_block import DiTXMoEBlock
from maniflow.model.diffusion.ditx_block import AdaptiveLayerNorm
from termcolor import cprint

logger = logging.getLogger(__name__)

class FinalLayer(nn.Module):
    """DiTXæœ€ç»ˆè¾“å‡ºå±‚"""
    def __init__(self, hidden_size, out_channels):
        super().__init__()
        self.norm_final = RmsNorm(hidden_size, eps=1e-6)
        approx_gelu = lambda: nn.GELU(approximate="tanh")
        self.ffn_final = Mlp(in_features=hidden_size,
            hidden_features=hidden_size,
            out_features=out_channels, 
            act_layer=approx_gelu, drop=0)

    def forward(self, x):
        x = self.norm_final(x)
        x = self.ffn_final(x)
        return x

class DiTXMoE(nn.Module):
    """
    DiTXæ¨¡å‹çš„MoEç‰ˆæœ¬ï¼Œä½¿ç”¨DiTXMoEBlockæ›¿ä»£DiTXBlock
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. æ¨¡æ€çº§åˆ«MoEè·¯ç”±ï¼šæŒ‰æ¨¡æ€ç»„åˆè¿›è¡Œè·¯ç”±ï¼Œä¿æŒæ¨¡æ€å†…è¯­ä¹‰ä¸€è‡´æ€§
    2. æ—¶é—´æ¡ä»¶æ„ŸçŸ¥ï¼šMoEé—¨æ§æ„ŸçŸ¥æ‰©æ•£æ—¶é—´æ­¥
    3. æ¨¡æ€é•¿åº¦ä¼ é€’ï¼šå°†æ¨¡æ€é•¿åº¦ä¿¡æ¯ä¼ é€’ç»™Blockç”¨äºæ­£ç¡®åˆ†å‰²
    4. ğŸ†• æ¨¡æ€åµŒå…¥ï¼šä¸ºä¸åŒæ¨¡æ€æ·»åŠ å¯å­¦ä¹ çš„æ ‡è¯†ç¬¦ï¼Œå¸®åŠ©MoE GateåŒºåˆ†æ¨¡æ€
    """
    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        horizon: int,
        n_obs_steps: int = None,
        cond_dim: int = 256,
        visual_cond_len: int = 1024,
        diffusion_timestep_embed_dim: int = 256,
        diffusion_target_t_embed_dim: int = 256,
        block_type: str = "DiTXMoE",
        n_layer: int = 12,
        n_head: int = 12,
        n_emb: int = 768,
        mlp_ratio: float = 4.0,
        p_drop_attn: float = 0.1,
        qkv_bias: bool = False,
        qk_norm: bool = False,
        pre_norm_modality: bool = False,
        language_conditioned: bool = False,
        language_model: str = "t5-small",
        # MoE specific parameters
        use_modality_moe: bool = True,
        num_experts: int = 4,         # æ¨¡æ€çº§MoEå»ºè®®4ä¸ªä¸“å®¶
        num_experts_per_tok: int = 2,
        n_shared_experts: int = 1,
        moe_aux_loss_alpha: float = 0.01,
        # æ¨¡æ€é•¿åº¦é…ç½®ï¼ˆç”¨äºæ­£ç¡®åˆ†å‰²context_cï¼‰
        head_cond_len: int = None,    # å¤´éƒ¨ç›¸æœºç‰¹å¾é•¿åº¦ï¼ŒNoneåˆ™è‡ªåŠ¨è®¡ç®—
        wrist_cond_len: int = None,   # è…•éƒ¨ç›¸æœºç‰¹å¾é•¿åº¦ï¼ŒNoneåˆ™è‡ªåŠ¨è®¡ç®—
        # ğŸ†• æ¨¡æ€åµŒå…¥é…ç½®
        use_modality_embedding: bool = True,  # æ˜¯å¦ä½¿ç”¨æ¨¡æ€åµŒå…¥
    ):
        super().__init__()
        self.n_obs_steps = n_obs_steps
        self.visual_cond_len = visual_cond_len
        self.language_conditioned = language_conditioned
        self.pre_norm_modality = pre_norm_modality
        
        # æ¨¡æ€é•¿åº¦é…ç½®ï¼ˆç”¨äºMoEè·¯ç”±ï¼‰
        # é»˜è®¤å‡è®¾ï¼švisual_cond_len = head + wristï¼Œæœ¬ä½“æ„ŸçŸ¥åœ¨å¤–éƒ¨æ‹¼æ¥
        self.head_cond_len = head_cond_len
        self.wrist_cond_len = wrist_cond_len
        
        # ğŸ†• æ¨¡æ€åµŒå…¥ï¼šä¸ºä¸åŒæ¨¡æ€æ·»åŠ å¯å­¦ä¹ çš„æ ‡è¯†ç¬¦
        self.use_modality_embedding = use_modality_embedding
        if use_modality_embedding:
            # 3ç§æ¨¡æ€ç±»å‹ï¼šhead, wrist, proprio
            self.modality_embeddings = nn.ParameterDict({
                'head': nn.Parameter(torch.zeros(1, 1, n_emb)),    # å¤´éƒ¨ç›¸æœºåµŒå…¥
                'wrist': nn.Parameter(torch.zeros(1, 1, n_emb)),   # è…•éƒ¨ç›¸æœºåµŒå…¥
                'proprio': nn.Parameter(torch.zeros(1, 1, n_emb)), # æœ¬ä½“æ„ŸçŸ¥åµŒå…¥
                'lang': nn.Parameter(torch.zeros(1, 1, n_emb)),    # è¯­è¨€åµŒå…¥
            })
            # åˆå§‹åŒ–æ¨¡æ€åµŒå…¥ï¼ˆä½¿ç”¨è¾ƒå°çš„æ ‡å‡†å·®ï¼‰
            for key in self.modality_embeddings:
                nn.init.normal_(self.modality_embeddings[key], std=0.02)
            cprint(f"[DiTXMoE] åˆå§‹åŒ–æ¨¡æ€åµŒå…¥: head, wrist, proprio, lang", "cyan")
        
        # Constants
        T = horizon
        self.T = T
        self.horizon = horizon

        # è¾“å…¥åµŒå…¥
        self.hidden_dim = n_emb
        self.input_emb = nn.Linear(input_dim, n_emb)
        self.pos_emb = nn.Parameter(torch.zeros(1, T, n_emb))
        self.vis_cond_obs_emb = nn.Linear(cond_dim, n_emb)
        self.vis_cond_pos_embed = nn.Parameter(
            torch.zeros(1, visual_cond_len * n_obs_steps, n_emb)
        )
        
        # æ¨¡æ€é¢„å½’ä¸€åŒ–
        if self.pre_norm_modality:
            self.vis_norm = AdaptiveLayerNorm(dim=n_emb, dim_cond=n_emb)

        # æ—¶é—´æ­¥ç¼–ç å™¨
        flow_timestep_encoder = nn.Sequential(
            SinusoidalPosEmb(diffusion_timestep_embed_dim),
            nn.Linear(diffusion_timestep_embed_dim, diffusion_timestep_embed_dim * 4),
            nn.Mish(),
            nn.Linear(diffusion_timestep_embed_dim * 4, n_emb),
        )
        flow_target_t_encoder = nn.Sequential(
            SinusoidalPosEmb(diffusion_target_t_embed_dim),
            nn.Linear(diffusion_target_t_embed_dim, diffusion_target_t_embed_dim * 4),
            nn.Mish(),
            nn.Linear(diffusion_target_t_embed_dim * 4, self.hidden_dim),
        )
        self.flow_timestep_encoder = flow_timestep_encoder
        self.flow_target_t_encoder = flow_target_t_encoder
        self.timestep_target_t_adaptor = nn.Linear(self.hidden_dim * 2, self.hidden_dim)

        # è¯­è¨€æ¡ä»¶ç¼–ç 
        if self.language_conditioned:
            self.load_T5_encoder(model_name=language_model, freeze=True)
            self.lang_adaptor = self.build_condition_adapter(
                "mlp2x_gelu", 
                in_features=self.language_encoder_out_dim, 
                out_features=n_emb
            )
            if self.pre_norm_modality:
                self.lang_norm = AdaptiveLayerNorm(dim=n_emb, dim_cond=n_emb)
            
        # Transformer blocks (ä½¿ç”¨DiTXMoEBlock)
        self.block_type = block_type
        if block_type == "DiTXMoE":
            self.blocks = nn.ModuleList([
                DiTXMoEBlock(
                    hidden_size=n_emb, 
                    num_heads=n_head, 
                    mlp_ratio=mlp_ratio, 
                    p_drop_attn=p_drop_attn,
                    qkv_bias=qkv_bias, 
                    qk_norm=qk_norm,
                    use_modality_moe=use_modality_moe,
                    num_experts=num_experts,
                    num_experts_per_tok=num_experts_per_tok,
                    n_shared_experts=n_shared_experts,
                    moe_aux_loss_alpha=moe_aux_loss_alpha,
                ) for _ in range(n_layer)
            ])
            cprint(f"[DiTXMoE] åˆå§‹åŒ–{n_layer}ä¸ªDiTXMoEå—: hidden_size={n_emb}, num_heads={n_head}, "
                   f"MoE={use_modality_moe}, experts={num_experts}, top_k={num_experts_per_tok}", "cyan")
        
        # æœ€ç»ˆè¾“å‡ºå±‚
        self.final_layer = FinalLayer(n_emb, output_dim)

        self.initialize_weights()
        cprint(f"[DiTXMoE] æƒé‡åˆå§‹åŒ–å®Œæˆ", "green")
        
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def build_condition_adapter(self, projector_type, in_features, out_features):
        """æ„å»ºæ¡ä»¶é€‚é…å™¨"""
        projector = None
        if projector_type == 'linear':
            projector = nn.Linear(in_features, out_features)
        else:
            mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu$', projector_type)
            if mlp_gelu_match:
                mlp_depth = int(mlp_gelu_match.group(1))
                modules = [nn.Linear(in_features, out_features)]
                for _ in range(1, mlp_depth):
                    modules.append(nn.GELU(approximate="tanh"))
                    modules.append(nn.Linear(out_features, out_features))
                projector = nn.Sequential(*modules)

        if projector is None:
            raise ValueError(f'æœªçŸ¥çš„projectorç±»å‹: {projector_type}')

        return projector
    
    def load_T5_encoder(self, model_name, freeze=True):
        """åŠ è½½T5è¯­è¨€ç¼–ç å™¨"""
        from transformers import T5Config, T5EncoderModel, AutoTokenizer
        T5_model_name = ["t5-small", "t5-base", "t5-large", "t5-3b", "t5-11b"]
        assert model_name in T5_model_name, f"æ¨¡å‹åç§°{model_name}ä¸åœ¨{T5_model_name}ä¸­"
        encoder_name = model_name
        pretrained_model_id = f"google-t5/{encoder_name}"
        encoder_cfg = T5Config()
        self.language_encoder = T5EncoderModel(encoder_cfg).from_pretrained(pretrained_model_id)
        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_id)
        if freeze:
            self.language_encoder.eval()
            for param in self.language_encoder.parameters():
                param.requires_grad = False

        self.language_encoder_out_dim = 512
        cprint(f"åŠ è½½T5ç¼–ç å™¨: {encoder_name}", "green")

    def encode_text_input_T5(self, lang_cond, norm_lang_embedding=False, output_type="sentence",
                             device="cuda" if torch.cuda.is_available() else "cpu"):
        """ç¼–ç æ–‡æœ¬è¾“å…¥"""
        language_inputs = self.tokenizer(lang_cond, return_tensors="pt", padding=True, truncation=True)
        input_ids = language_inputs["input_ids"].to(device)
        attention_mask = language_inputs["attention_mask"].to(device)
        encoder_outputs = self.language_encoder(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = encoder_outputs.last_hidden_state
        if output_type == "token":
            return token_embeddings
        sentence_embedding = torch.mean(token_embeddings, dim=1).squeeze(1)
        if norm_lang_embedding:
            sentence_embedding = F.normalize(sentence_embedding, p=2, dim=-1)
        return sentence_embedding

    def initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for block in self.blocks:
            # åˆå§‹åŒ–self_attn
            nn.init.xavier_uniform_(block.self_attn.in_proj_weight)
            if block.self_attn.in_proj_bias is not None:
                nn.init.zeros_(block.self_attn.in_proj_bias)
            
            nn.init.xavier_uniform_(block.self_attn.out_proj.weight)
            if block.self_attn.out_proj.bias is not None:
                nn.init.zeros_(block.self_attn.out_proj.bias)

        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        self.apply(_basic_init)

        # Zero-out adaLN modulation
        for block in self.blocks:
            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)
            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)

        # åˆå§‹åŒ–input emb
        nn.init.normal_(self.input_emb.weight, std=0.02)
        nn.init.constant_(self.input_emb.bias, 0) if self.input_emb.bias is not None else None

        # åˆå§‹åŒ–pos emb
        nn.init.normal_(self.pos_emb, std=0.02)

        # åˆå§‹åŒ–timestepç¼–ç å™¨
        for layer in self.flow_timestep_encoder:
            if isinstance(layer, nn.Linear):
                nn.init.normal_(layer.weight, std=0.02)
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)
        
        for layer in self.flow_target_t_encoder:
            if isinstance(layer, nn.Linear):
                nn.init.normal_(layer.weight, std=0.02)
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)
        
        # åˆå§‹åŒ–visual condition embedding
        nn.init.normal_(self.vis_cond_obs_emb.weight, std=0.02)
        nn.init.constant_(self.vis_cond_obs_emb.bias, 0) if self.vis_cond_obs_emb.bias is not None else None

        # åˆå§‹åŒ–adaptor
        nn.init.normal_(self.timestep_target_t_adaptor.weight, std=0.02)
        nn.init.constant_(self.timestep_target_t_adaptor.bias, 0)

        if self.language_conditioned:
            nn.init.normal_(self.lang_adaptor[0].weight, std=0.02)
            nn.init.constant_(self.lang_adaptor[0].bias, 0) if self.lang_adaptor[0].bias is not None else None
            nn.init.normal_(self.lang_adaptor[-1].weight, std=0.02)
            nn.init.constant_(self.lang_adaptor[-1].bias, 0) if self.lang_adaptor[-1].bias is not None else None
        
        if self.pre_norm_modality:
            nn.init.zeros_(self.vis_norm.cond_linear.weight)
            nn.init.constant_(self.vis_norm.cond_linear.bias[:self.hidden_dim], 1.)
            nn.init.zeros_(self.vis_norm.cond_linear.bias[self.hidden_dim:])
            if self.language_conditioned:
                nn.init.zeros_(self.lang_norm.cond_linear.weight)
                nn.init.constant_(self.lang_norm.cond_linear.bias[:self.hidden_dim], 1.)
                nn.init.zeros_(self.lang_norm.cond_linear.bias[self.hidden_dim:])

        # Zero-out final layer
        nn.init.constant_(self.final_layer.ffn_final.fc2.weight, 0)
        nn.init.constant_(self.final_layer.ffn_final.fc2.bias, 0)

    def get_optim_groups(self, weight_decay: float=1e-3):
        """è·å–ä¼˜åŒ–å™¨å‚æ•°ç»„"""
        decay = set()
        no_decay = set()
        whitelist_weight_modules = (torch.nn.Linear, torch.nn.MultiheadAttention)
        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding, RmsNorm)
        for mn, m in self.named_modules():
            for pn, p in m.named_parameters():
                fpn = "%s.%s" % (mn, pn) if mn else pn

                if pn.endswith("bias"):
                    no_decay.add(fpn)
                elif pn.startswith("bias"):
                    no_decay.add(fpn)
                elif pn.endswith("weight") and isinstance(m, whitelist_weight_modules):
                    decay.add(fpn)
                elif pn.endswith("weight") and isinstance(m, blacklist_weight_modules):
                    no_decay.add(fpn)

        no_decay.add("pos_emb")
        if self.vis_cond_pos_embed is not None:
            no_decay.add("vis_cond_pos_embed") 

        param_dict = {pn: p for pn, p in self.named_parameters()}
        inter_params = decay & no_decay
        union_params = decay | no_decay
        assert len(inter_params) == 0, f"å‚æ•°{inter_params}åŒæ—¶åœ¨decayå’Œno_decayä¸­"
        assert len(param_dict.keys() - union_params) == 0, \
            f"å‚æ•°{param_dict.keys() - union_params}æœªåˆ†é…åˆ°decayæˆ–no_decay"

        optim_groups = [
            {"params": [param_dict[pn] for pn in sorted(list(decay))], "weight_decay": weight_decay},
            {"params": [param_dict[pn] for pn in sorted(list(no_decay))], "weight_decay": 0.0},
        ]
        return optim_groups

    def configure_optimizers(self, learning_rate: float=1e-4, weight_decay: float=1e-3,
                            betas: Tuple[float, float]=(0.9,0.95)):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optim_groups = self.get_optim_groups(weight_decay=weight_decay)
        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)
        return optimizer

    def forward(self, 
            sample: torch.Tensor, 
            timestep: Union[torch.Tensor, float, int], 
            target_t: Union[torch.Tensor, float, int], 
            vis_cond: torch.Tensor,
            lang_cond: Union[torch.Tensor, list, str] = None,
            modality_lens: dict = None,
            **kwargs):
        """
        å‰å‘ä¼ æ’­
        Input:
            sample: (B,T,input_dim) åŠ¨ä½œåºåˆ—
            timestep: (B,) or int, æ‰©æ•£æ—¶é—´æ­¥t
            target_t: (B,) or float, ç›®æ ‡æ—¶é—´æ­¥
            vis_cond: (B,L,vis_cond_dim) å¤šæ¨¡æ€è§†è§‰æ¡ä»¶
            lang_cond: (B,) or list, è¯­è¨€æ¡ä»¶
            modality_lens: dict, æ¨¡æ€é•¿åº¦ä¿¡æ¯ {'head': L_head, 'wrist': L_wrist, 'proprio': L_proprio}
        Output: 
            action: (B,T,output_dim) é¢„æµ‹åŠ¨ä½œ
        """
        # è¾“å…¥åµŒå…¥
        input_emb = self.input_emb(sample)
        x = input_emb + self.pos_emb

        # 1. æ—¶é—´æ­¥ç¼–ç 
        timesteps = timestep
        if not torch.is_tensor(timesteps):
            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)
        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:
            timesteps = timesteps[None].to(sample.device)
        timesteps = timesteps.expand(sample.shape[0])
        timestep_embed = self.flow_timestep_encoder(timesteps)
        
        # 2. ç›®æ ‡æ—¶é—´æ­¥ç¼–ç 
        target_ts = target_t
        if not torch.is_tensor(target_ts): 
            target_ts = torch.tensor([target_ts], dtype=torch.float32, device=sample.device)    
        elif torch.is_tensor(target_ts) and len(target_ts.shape) == 0:
            target_ts = target_ts[None].to(sample.device)
        target_ts = target_ts.expand(sample.shape[0])
        target_t_embed = self.flow_target_t_encoder(target_ts)
        
        time_c = torch.cat([timestep_embed, target_t_embed], dim=-1)
        time_c = self.timestep_target_t_adaptor(time_c)

        # 3. è§†è§‰æ¡ä»¶ç¼–ç 
        vis_con_obs_emb = self.vis_cond_obs_emb(vis_cond)
        vis_cond_pos_embed = self.vis_cond_pos_embed[:, :vis_cond.shape[1]]
        context_c = vis_con_obs_emb + vis_cond_pos_embed
        if self.pre_norm_modality:
            context_c = self.vis_norm(context_c, time_c)

        # 4. è¯­è¨€æ¡ä»¶ç¼–ç 
        lang_len = 0
        if self.language_conditioned:
            assert lang_cond is not None
            lang_c = self.encode_text_input_T5(lang_cond, output_type="token", device=sample.device)
            lang_c = self.lang_adaptor(lang_c)
            if self.pre_norm_modality:
                lang_c = self.lang_norm(lang_c, time_c)
            lang_len = lang_c.shape[1]
            context_c = torch.cat([context_c, lang_c], dim=1)

        # 5. è®¡ç®—æ¨¡æ€é•¿åº¦ä¿¡æ¯ï¼ˆç”¨äºMoEè·¯ç”±ï¼‰
        if modality_lens is None:
            # è‡ªåŠ¨æ¨æ–­æ¨¡æ€é•¿åº¦
            total_vis_len = vis_cond.shape[1]
            if self.head_cond_len is not None and self.wrist_cond_len is not None:
                # ğŸ”§ ä¿®æ­£: head_cond_lenå’Œwrist_cond_lenå·²ç»æ˜¯æ¯ä¸ªæ—¶é—´æ­¥çš„tokenæ•°
                # vis_condå·²ç»åŒ…å«äº†æ‰€æœ‰æ—¶é—´æ­¥ï¼Œæ‰€ä»¥ä¸éœ€è¦å†ä¹˜ä»¥n_obs_steps
                head_len = self.head_cond_len
                wrist_len = self.wrist_cond_len
            else:
                # é»˜è®¤å‡åˆ†è§†è§‰ç‰¹å¾
                head_len = total_vis_len // 2
                wrist_len = total_vis_len - head_len
            # æœ¬ä½“æ„ŸçŸ¥é•¿åº¦ = total_vis_len - head_len - wrist_len
            proprio_len = total_vis_len - head_len - wrist_len
            modality_lens = {'head': head_len, 'wrist': wrist_len, 'proprio': proprio_len}
        
        # ğŸ†• 6. æ·»åŠ æ¨¡æ€åµŒå…¥ï¼ˆå¸®åŠ©MoE GateåŒºåˆ†ä¸åŒæ¨¡æ€ï¼‰
        if self.use_modality_embedding:
            context_c = self._add_modality_embeddings(context_c, modality_lens, lang_len)

        # 7. Transformer blocks (ä½¿ç”¨MoEå¤„ç†å¤šæ¨¡æ€ç‰¹å¾ï¼Œä¼ å…¥æ¨¡æ€é•¿åº¦)
        for block in self.blocks:
            x = block(x, time_c, context_c, modality_lens=modality_lens)

        # 8. è¾“å‡ºå±‚
        x = self.final_layer(x)
        x = x[:, -self.horizon:]
        
        return x
    
    def _add_modality_embeddings(self, context_c, modality_lens, lang_len):
        """
        ğŸ†• ä¸ºcontext_cçš„ä¸åŒæ¨¡æ€åŒºåŸŸæ·»åŠ æ¨¡æ€åµŒå…¥
        
        Args:
            context_c: (B, L_total, D) å¤šæ¨¡æ€ç‰¹å¾
            modality_lens: dict æ¨¡æ€é•¿åº¦ä¿¡æ¯
            lang_len: int è¯­è¨€tokené•¿åº¦
        Returns:
            context_c: (B, L_total, D) æ·»åŠ æ¨¡æ€åµŒå…¥åçš„ç‰¹å¾
        """
        B, L, D = context_c.shape
        head_len = modality_lens.get('head', 0)
        wrist_len = modality_lens.get('wrist', 0)
        proprio_len = modality_lens.get('proprio', 0)
        
        # åˆ›å»ºæ¨¡æ€åµŒå…¥æ©ç 
        # context_cç»“æ„: [head_tokens, wrist_tokens, proprio_tokens, lang_tokens]
        if head_len > 0:
            head_emb = self.modality_embeddings['head'].expand(B, head_len, D)
            context_c[:, :head_len] = context_c[:, :head_len] + head_emb
        
        if wrist_len > 0:
            wrist_start = head_len
            wrist_emb = self.modality_embeddings['wrist'].expand(B, wrist_len, D)
            context_c[:, wrist_start:wrist_start+wrist_len] = context_c[:, wrist_start:wrist_start+wrist_len] + wrist_emb
        
        if proprio_len > 0:
            proprio_start = head_len + wrist_len
            proprio_emb = self.modality_embeddings['proprio'].expand(B, proprio_len, D)
            context_c[:, proprio_start:proprio_start+proprio_len] = context_c[:, proprio_start:proprio_start+proprio_len] + proprio_emb
        
        if lang_len > 0:
            lang_start = head_len + wrist_len + proprio_len
            lang_emb = self.modality_embeddings['lang'].expand(B, lang_len, D)
            context_c[:, lang_start:lang_start+lang_len] = context_c[:, lang_start:lang_start+lang_len] + lang_emb
        
        return context_c


if __name__ == "__main__":
    """æµ‹è¯•DiTXMoEæ¨¡å‹"""
    print("="*80)
    print("æµ‹è¯•DiTXMoEæ¨¡å‹ (æ¨¡æ€çº§åˆ«MoE + æ—¶é—´æ¡ä»¶æ„ŸçŸ¥)")
    print("="*80)
    
    torch.manual_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # å‚æ•°è®¾ç½®
    batch_size = 2
    horizon = 10
    input_dim = 16
    output_dim = 16
    cond_dim = 256
    visual_cond_len = 128
    n_obs_steps = 2
    
    # æ¨¡æ€é•¿åº¦é…ç½®
    head_cond_len = 64   # å¤´éƒ¨ç›¸æœºç‰¹å¾é•¿åº¦
    wrist_cond_len = 64  # è…•éƒ¨ç›¸æœºç‰¹å¾é•¿åº¦
    
    # åˆ›å»ºDiTXMoEæ¨¡å‹
    model_moe = DiTXMoE(
        input_dim=input_dim,
        output_dim=output_dim,
        horizon=horizon,
        n_obs_steps=n_obs_steps,
        cond_dim=cond_dim,
        visual_cond_len=visual_cond_len,
        diffusion_timestep_embed_dim=256,
        diffusion_target_t_embed_dim=256,
        block_type="DiTXMoE",
        n_layer=2,
        n_head=8,
        n_emb=768,
        mlp_ratio=4.0,
        p_drop_attn=0.1,
        language_conditioned=False,
        pre_norm_modality=False,
        # MoEé…ç½®
        use_modality_moe=True,
        num_experts=4,        # æ¨¡æ€çº§MoEå»ºè®®4ä¸ªä¸“å®¶
        num_experts_per_tok=2,
        n_shared_experts=1,
        moe_aux_loss_alpha=0.01,
        # æ¨¡æ€é•¿åº¦é…ç½®
        head_cond_len=head_cond_len,
        wrist_cond_len=wrist_cond_len,
    ).to(device)
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    sample = torch.randn(batch_size, horizon, input_dim).to(device)
    timestep = torch.tensor([1, 2]).to(device)
    target_t = torch.tensor([0.1, 0.2]).to(device)
    vis_cond = torch.randn(batch_size, visual_cond_len * n_obs_steps, cond_dim).to(device)
    
    print(f"\nè¾“å…¥å½¢çŠ¶:")
    print(f"  sample: {sample.shape}")
    print(f"  timestep: {timestep.shape}")
    print(f"  target_t: {target_t.shape}")
    print(f"  vis_cond: {vis_cond.shape}")
    print(f"  æ¨¡æ€é•¿åº¦: head={head_cond_len*n_obs_steps}, wrist={wrist_cond_len*n_obs_steps}")
    
    # å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
    print(f"\nå‰å‘ä¼ æ’­ (è®­ç»ƒæ¨¡å¼)...")
    model_moe.train()
    output = model_moe(sample, timestep, target_t, vis_cond)
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert output.shape == (batch_size, horizon, output_dim), "è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"
    
    # æ£€æŸ¥MoEç»Ÿè®¡ä¿¡æ¯
    print(f"\nMoEç»Ÿè®¡ä¿¡æ¯:")
    for i, block in enumerate(model_moe.blocks):
        if hasattr(block, 'modality_moe') and block.modality_moe.moe_stats:
            stats = block.modality_moe.moe_stats
            print(f"  Block {i}:")
            print(f"    - aux_loss: {stats['aux_loss']:.6f}")
            print(f"    - expert_usage: {[f'{u:.3f}' for u in stats['expert_usage'].tolist()]}")
    
    # å‚æ•°ç»Ÿè®¡
    params_moe = sum(p.numel() for p in model_moe.parameters())
    print(f"\næ¨¡å‹ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {params_moe:,}")
    print(f"  MoEæ¨¡å—æ•°: {len(model_moe.blocks)}")
    
    # æ¢¯åº¦æµ‹è¯•
    print(f"\næ¢¯åº¦æµ‹è¯•...")
    loss = output.sum()
    loss.backward()
    print(f"  âœ… åå‘ä¼ æ’­æˆåŠŸ")
    
    # æ¨ç†æ¨¡å¼æµ‹è¯•
    print(f"\næ¨ç†æ¨¡å¼æµ‹è¯•...")
    model_moe.eval()
    with torch.no_grad():
        output_eval = model_moe(sample, timestep, target_t, vis_cond)
    print(f"  è¾“å‡ºå½¢çŠ¶: {output_eval.shape}")
    print(f"  âœ… æ¨ç†æˆåŠŸ")
    
    print(f"\n" + "="*80)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("="*80)
