import pickle, os
import numpy as np
import pdb
from copy import deepcopy
import zarr
import shutil
import argparse
import yaml
import cv2
import h5py

def depth_decoding_single(encoded_depth):
    """
    Decode a single RGB-encoded depth image back to millimeter array
    
    Args:
        encoded_depth: Single PNG compressed RGB depth image bytes
    
    Returns:
        depth_mm: Depth array in millimeters (original format)
    """
    DEFAULT_RGB_SCALE_FACTOR = 256000.0
    
    # Remove padding
    png_data = encoded_depth.rstrip(b"\0")
    
    # Decode PNG (this gives BGR array from OpenCV)
    bgr_array = cv2.imdecode(np.frombuffer(png_data, np.uint8), cv2.IMREAD_COLOR)
    
    if bgr_array is None:
        raise ValueError("Failed to decode depth image")
    
    # Convert BGR back to RGB (since we stored as RGB originally)
    image_array = cv2.cvtColor(bgr_array, cv2.COLOR_BGR2RGB)
    
    # EXACT COPY of ImageToFloatArray algorithm for RGB
    image_shape = image_array.shape
    channels = image_shape[2] if len(image_shape) > 2 else 1
    
    if channels == 3:
        # RGB image needs to be converted to 24 bit integer
        # This is the EXACT line from the original
        float_array = np.sum(image_array * [65536, 256, 1], axis=2)
        scale_factor = DEFAULT_RGB_SCALE_FACTOR
    else:
        raise ValueError("Expected RGB image with 3 channels")
    
    # Convert back to original units  
    scaled_array = float_array / scale_factor  # This gives meters
    depth_mm = scaled_array * 1000.0  # Convert back to millimeters
    
    return depth_mm

def load_hdf5(dataset_path):
    if not os.path.isfile(dataset_path):
        print(f"Dataset does not exist at \n{dataset_path}\n")
        exit()

    with h5py.File(dataset_path, "r") as root:
        left_gripper, left_arm = (
            root["/joint_action/left_gripper"][()],
            root["/joint_action/left_arm"][()],
        )
        right_gripper, right_arm = (
            root["/joint_action/right_gripper"][()],
            root["/joint_action/right_arm"][()],
        )
        vector = root["/joint_action/vector"][()]
        image_dict = dict()
        depth_dict = dict()
        for cam_name in root[f"/observation/"].keys():
            image_dict[cam_name] = root[f"/observation/{cam_name}/rgb"][()]
            depth_dict[cam_name] = root[f"/observation/{cam_name}/depth"][()]
        pointcloud = root["/pointcloud"][()]

    return left_gripper, left_arm, right_gripper, right_arm, vector, pointcloud, image_dict, depth_dict


def main():
    parser = argparse.ArgumentParser(description="Process some episodes.")
    parser.add_argument(
        "task_name",
        type=str,
        help="The name of the task (e.g., beat_block_hammer)",
    )
    parser.add_argument("task_config", type=str)
    parser.add_argument(
        "expert_data_num",
        type=int,
        help="Number of episodes to process (e.g., 50)",
    )
    parser.add_argument(
        "--decode-depth",
        action="store_true",
        default=True,
        help="Decode depth images to float arrays (default: store compressed)",
    )
    args = parser.parse_args()

    task_name = args.task_name
    num = args.expert_data_num
    task_config = args.task_config
    decode_depth = args.decode_depth
    print(f"Processing {num} episodes for task {task_name} with config {task_config}, decode_depth={decode_depth}") 

    load_dir = "../../data/" + str(task_name) + "/" + str(task_config)

    total_count = 0

    save_dir = f"./data/{task_name}-{task_config}-{num}.zarr"

    if os.path.exists(save_dir):
        shutil.rmtree(save_dir)

    current_ep = 0

    zarr_root = zarr.group(save_dir)
    zarr_data = zarr_root.create_group("data")
    zarr_meta = zarr_root.create_group("meta")

    point_cloud_arrays = []
    head_camera_arrays, front_camera_arrays, left_camera_arrays, right_camera_arrays = (
        [],
        [],
        [],
        [],
    )
    head_depth_arrays, left_depth_arrays, right_depth_arrays = (
        [],
        [],
        [],
    )
    episode_ends_arrays, action_arrays, state_arrays, joint_action_arrays = (
        [],
        [],
        [],
        [],
    )

    while current_ep < num:
        print(f"processing episode: {current_ep + 1} / {num}", end="\r")

        load_path = os.path.join(load_dir, f"data/episode{current_ep}.hdf5")
        (
            left_gripper_all,
            left_arm_all,
            right_gripper_all,
            right_arm_all,
            vector_all,
            pointcloud_all,
            image_dict_all,
            depth_dict_all,
        ) = load_hdf5(load_path)

        for j in range(0, left_gripper_all.shape[0]):
            
            head_img_bit = image_dict_all["head_camera"][j]
            left_img_bit = image_dict_all["left_camera"][j]
            right_img_bit = image_dict_all["right_camera"][j]
            head_depth_bit = depth_dict_all["head_camera"][j]
            # left_depth_bit = depth_dict_all["left_camera"][j]
            # right_depth_bit = depth_dict_all["right_camera"][j]
            pointcloud = pointcloud_all[j]
            joint_state = vector_all[j]

            if j != left_gripper_all.shape[0] - 1:
                head_img = cv2.imdecode(np.frombuffer(head_img_bit, np.uint8), cv2.IMREAD_COLOR)
                left_img = cv2.imdecode(np.frombuffer(left_img_bit, np.uint8), cv2.IMREAD_COLOR)
                right_img = cv2.imdecode(np.frombuffer(right_img_bit, np.uint8), cv2.IMREAD_COLOR)

                if decode_depth:
                    head_depth = depth_decoding_single(head_depth_bit) / 1000.0  # Convert to meters
                    # left_depth = depth_decoding_single(left_depth_bit) / 1000.0  # Convert to meters
                    # right_depth = depth_decoding_single(right_depth_bit) / 1000.0  # Convert to meters
                else:
                    raise ValueError("Depth decoding is required but not implemented in this script.")

                head_camera_arrays.append(head_img)
                left_camera_arrays.append(left_img)
                right_camera_arrays.append(right_img)
                head_depth_arrays.append(head_depth)
                # left_depth_arrays.append(left_depth)
                # right_depth_arrays.append(right_depth)
                point_cloud_arrays.append(pointcloud)
                state_arrays.append(joint_state)
            if j != 0:
                joint_action_arrays.append(joint_state)

        current_ep += 1
        total_count += left_gripper_all.shape[0] - 1
        episode_ends_arrays.append(total_count)

    print()
    try:
        episode_ends_arrays = np.array(episode_ends_arrays)
        state_arrays = np.array(state_arrays)
        head_camera_arrays = np.array(head_camera_arrays)
        left_camera_arrays = np.array(left_camera_arrays)
        right_camera_arrays = np.array(right_camera_arrays)
        head_depth_arrays = np.array(head_depth_arrays)
        # left_depth_arrays = np.array(left_depth_arrays)
        # right_depth_arrays = np.array(right_depth_arrays)
        point_cloud_arrays = np.array(point_cloud_arrays)
        joint_action_arrays = np.array(joint_action_arrays)

        head_camera_arrays = np.moveaxis(head_camera_arrays, -1, 1)  # NHWC -> NCHW
        left_camera_arrays = np.moveaxis(left_camera_arrays, -1, 1)  # NHWC -> NCHW
        right_camera_arrays = np.moveaxis(right_camera_arrays, -1, 1)  # NHWC -> NCHW

        compressor = zarr.Blosc(cname="zstd", clevel=3, shuffle=1)
        state_chunk_size = (100, state_arrays.shape[1])
        joint_chunk_size = (100, joint_action_arrays.shape[1])
        head_camera_chunk_size = (100, *head_camera_arrays.shape[1:])
        left_camera_chunk_size = (100, *left_camera_arrays.shape[1:])
        right_camera_chunk_size = (100, *right_camera_arrays.shape[1:])
        head_depth_chunk_size = (100, *head_depth_arrays.shape[1:])
        point_cloud_chunk_size = (100, point_cloud_arrays.shape[1])
        zarr_data.create_dataset(
            "head_camera",
            data=head_camera_arrays,
            chunks=head_camera_chunk_size,
            overwrite=True,
            compressor=compressor,
        )
        zarr_data.create_dataset(
            "left_camera",
            data=left_camera_arrays,
            chunks=left_camera_chunk_size,
            overwrite=True,
            compressor=compressor,
        )
        zarr_data.create_dataset(
            "right_camera",
            data=right_camera_arrays,
            chunks=right_camera_chunk_size,
            overwrite=True,
            compressor=compressor,
        )
        # zarr_data.create_dataset(
        #     "head_depth",
        #     data=head_depth_arrays,
        #     chunks=head_depth_chunk_size,
        #     overwrite=True,
        #     compressor=compressor,
        # )
        zarr_data.create_dataset(
            "point_cloud",
            data=point_cloud_arrays,
            chunks=point_cloud_chunk_size,
            overwrite=True,
            compressor=compressor,
        )
        zarr_data.create_dataset(
            "state",
            data=state_arrays,
            chunks=state_chunk_size,
            dtype="float32",
            overwrite=True,
            compressor=compressor,
        )
        zarr_data.create_dataset(
            "action",
            data=joint_action_arrays,
            chunks=joint_chunk_size,
            dtype="float32",
            overwrite=True,
            compressor=compressor,
        )
        zarr_meta.create_dataset(
            "episode_ends",
            data=episode_ends_arrays,
            dtype="int64",
            overwrite=True,
            compressor=compressor,
        )
    except ZeroDivisionError as e:
        print("If you get a `ZeroDivisionError: division by zero`, check that `data/pointcloud` in the task config is set to true.")
        raise 
    except Exception as e:
        print(f"An unexpected error occurred ({type(e).__name__}): {e}")
        raise

if __name__ == "__main__":
    main()
